{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://838581f9-41df-47dc-a452-47ea8f1e1edc.us-east4-0.gcp.cloud.qdrant.io:6333\", \n",
    "    api_key=\"7QBIL3BHx33BOqbHlsSIrn5in7kTX4OLzn5FAXQYPFP7Y1KFYbagpg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=\"my_collection\",\n",
    "    vectors_config=models.VectorParams(size=100, distance=models.Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samar\\Desktop\\Langchat\\lang\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\samar\\Desktop\\Langchat\\lang\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\samar\\Desktop\\Langchat\\lang\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 0.3.0. An updated version of the class exists in the langchain-qdrant package and should be used instead. To use it run `pip install -U langchain-qdrant` and import as `from langchain_qdrant import Qdrant`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "doc_store = Qdrant(\n",
    "    client=qdrant_client, collection_name=\"my_collection\", \n",
    "    embeddings=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.qdrant.Qdrant at 0x1d9c6c19190>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(directory):\n",
    "    file_loader = PyPDFDirectoryLoader(directory)\n",
    "    documents = file_loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = read_doc(\"C:/Users/samar/Desktop/Langchat/data/Langsmith/Pdf_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(docs,chunk_size=800,chunk_overlap=20):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    doc= text_splitter.split_documents(docs)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Tutorials | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsAdd observability to\\nyour LLM applicationEvaluate your LLM applicationOptimize a classifierHow-to\\nguidesConceptsReferencePricingSelf-hostingTutorialsTutorialsNew to LangSmith? This is the place\\nto start. Here, you'll find a hands-on introduction to key LangSmith workflows.Add observability to\\nyour LLM applicationEvaluate your LLM applicationOptimize a classifierWas this page\\nhelpful?PreviousQuick startNextAdd observability to your LLM\\napplicationCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\1.pdf', 'page': 0}),\n",
       " Document(page_content=\"Set up a workspace | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupCreate an account and API keyCreate an organizationSet up access controlSet up\\nbilling for your LangSmith accountSet up a workspaceTracingDatasetsEvaluationHuman\\nfeedbackMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesSetupSet up a workspaceSet up a workspaceinfoWorkspaces are not launched yet but are\\nprojected to launch by June 2024.When you log in for the first time, a default workspace will be\\ncreated for you automatically in your personal organization.\\nWorkspaces are often used to separate resources between different teams, business units, or\\ndeployment environments. Most LangSmith activity happens in the context of a workspace, each of\\nwhich has its own settings that are distinct from an organization's.To create a new workspace, head\\nto the Settings page Workspaces tab and click Create Workspace.\\nOnce your workspace has been created, you can manage its members and other configuration by\\nselecting it on this page.noteDifferent plans have different limits placed on the number of\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\10.pdf', 'page': 0}),\n",
       " Document(page_content='workspaces that can be used in an organization.\\nPlease see the pricing page for more information.Was this page helpful?PreviousSet up billing for\\nyour LangSmith accountNextAnnotate code for tracingCommunityDiscordTwitterGitHubDocs\\nCodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS\\nDocsCopyright © 2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\10.pdf', 'page': 1}),\n",
       " Document(page_content=\"Annotate code for tracing | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingAnnotate code for tracingToggle tracing on and offLog traces to specific\\nprojectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed\\ntracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever\\ntracesLog custom LLM tracesPrevent logging of inputs and outputs in tracesExport tracesShare or\\nunshare a trace publiclyTrace generator functionsTrace with LangChain (Python and JS/TS)Trace\\nwith Instructor (Python only)DatasetsEvaluationHuman feedbackMonitoring and\\nautomationsPromptsConceptsReferencePricingSelf-hostingHow-to guidesTracingAnnotate code for\\ntracingOn this pageAnnotate code for tracingThere are several ways to log traces to LangSmith.Use\\n@traceable / traceable?LangSmith makes it easy to log traces with minimal changes to your existing\\ncode with the @traceable decorator in Python and traceable function in TypeScript.noteThe\\nLANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be\\nlogged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\11.pdf', 'page': 0}),\n",
       " Document(page_content='and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY\\nenvironment variable to your API key (see Setup for more information).By default, the traces will be\\nlogged to a project named default.\\nTo log traces to a different project, see this section.PythonTypeScriptThe @traceable decorator is a\\nsimple way to log traces from the LangSmith Python SDK. Simply decorate any function with\\n@traceable.\\nfrom langsmith import traceablefrom openai import Clientopenai = Client()@traceabledef\\nformat_prompt(subject):    return [        {            \"role\": \"system\",            \"content\": \"You are a helpful\\nassistant.\",        },        {            \"role\": \"user\",            \"content\": f\"What\\'s a good name for a store that\\nsells {subject}?\"        }    ]@traceable(run_type=\"llm\")def invoke_llm(messages):    return\\nopenai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0   \\n)@traceabledef parse_output(response):    return\\nresponse.choices[0].message.content@traceabledef run_pipeline():    messages =\\nformat_prompt(\"colorful socks\")    response = invoke_llm(messages)    return\\nparse_output(response)run_pipeline()The traceable function is a simple way to log traces from the\\nLangSmith TypeScript SDK. Simply wrap any function with traceable.\\nNote that when wrapping a sync function with traceable, (e.g. formatPrompt in the example below),\\nyou should use the await keyword when calling it to ensure the trace is logged correctly.\\nimport { traceable } from \"langsmith/traceable\";import OpenAI from \"openai\";const openai = new\\nOpenAI();const formatPrompt = traceable(  (subject: string) => {    return [      {        role: \"system\" as\\nconst,        content: \"You are a helpful assistant.\",      },      {        role: \"user\" as const,        content:\\n`What\\'s a good name for a store that sells ${subject}?`,    },];},{ name: \"formatPrompt\" });const\\ninvokeLLM = traceable(    async ({ messages }: { messages: { role: string; content: string }[] }) => {    \\n   return openai.chat.completions.create({            model: \"gpt-3.5-turbo\",            messages:\\nmessages,            temperature: 0,        });    },    { run_type: \"llm\", name: \"invokeLLM\" });const\\nparseOutput = traceable(    (response: any) => {        return response.choices[0].message.content;   ', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\11.pdf', 'page': 1}),\n",
       " Document(page_content='},    { name: \"parseOutput\" });const runPipeline = traceable(    async () => {        const messages =\\nawait formatPrompt(\"colorful socks\");        const response = await invokeLLM({ messages });       \\nreturn parseOutput(response);    },    { name: \"runPipeline\" });await runPipeline();Wrap the OpenAI\\nclient?The wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI\\nclient in order to automatically log traces -- no decorator or function wrapping required!\\nThe wrapper works seamlessly with the @traceable decorator or traceable function and you can use\\nboth in the same application.Tool calls are automatically renderednoteThe\\nLANGCHAIN_TRACING_V2 environment variable must be set to \\'true\\' in order for traces to be\\nlogged to LangSmith, even when using wrap_openai or wrapOpenAI. This allows you to toggle\\ntracing on and off without changing your code.Additionally, you will need to set the\\nLANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By\\ndefault, the traces will be logged to a project named default.\\nTo log traces to a different project, see this section.PythonTypeScriptimport openaifrom langsmith\\nimport traceablefrom langsmith.wrappers import wrap_openaiclient =\\nwrap_openai(openai.Client())@traceable(run_type=\"tool\", name=\"Retrieve Context\")def\\nmy_tool(question: str) -> str:    return \"During this morning\\'s meeting, we solved all world\\nconflict.\"@traceable(name=\"Chat Pipeline\")def chat_pipeline(question: str):    context =\\nmy_tool(question)    messages = [        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\\nPlease respond to the user\\'s request only based on the given context.\" },        { \"role\": \"user\",\\n\"content\": f\"Question: {question}\\\\nContext: {context}\"}    ]    chat_completion =\\nclient.chat.completions.create(        model=\"gpt-3.5-turbo\", messages=messages    )    return\\nchat_completion.choices[0].message.contentchat_pipeline(\"Can you summarize this morning\\'s\\nmeetings?\")import OpenAI from \"openai\";import { traceable } from \"langsmith/traceable\";import {\\nwrapOpenAI } from \"langsmith/wrappers\";const client = wrapOpenAI(new OpenAI());const myTool =\\ntraceable(async (question: string) => {    return \"During this morning\\'s meeting, we solved all world\\nconflict.\";}, { name: \"Retrieve Context\", run_type: \"tool\" });const chatPipeline = traceable(async', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\11.pdf', 'page': 2}),\n",
       " Document(page_content='(question: string) => {    const context = await myTool(question);    const messages = [        {           \\nrole: \"system\",            content:                \"You are a helpful assistant. Please respond to the user\\'s\\nrequest only based on the given context.\",        },        { role: \"user\", content: `Question: ${question}\\nContext: ${context}` },    ];    const chatCompletion = await client.chat.completions.create({       \\nmodel: \"gpt-3.5-turbo\",        messages: messages,    });    return\\nchatCompletion.choices[0].message.content;}, { name: \"Chat Pipeline\" });await chatPipeline(\"Can\\nyou summarize this morning\\'s meetings?\");Use the RunTree API?Another, more explicit way to log\\ntraces to LangSmith is via the RunTree API. This API allows you more control over your tracing -\\nyou can manually\\ncreate runs and children runs to assemble your trace. You still need to set your\\nLANGCHAIN_API_KEY, but LANGCHAIN_TRACING_V2 is not\\nnecessary for this method.PythonTypeScriptimport openaifrom langsmith.run_trees import\\nRunTree# This can be a user input to your appquestion = \"Can you summarize this morning\\'s\\nmeetings?\"# Create a top-level runpipeline = RunTree(    name=\"Chat Pipeline\",    run_type=\"chain\",\\n   inputs={\"question\": question})# This can be retrieved in a retrieval stepcontext = \"During this\\nmorning\\'s meeting, we solved all world conflict.\"messages = [    { \"role\": \"system\", \"content\": \"You\\nare a helpful assistant. Please respond to the user\\'s request only based on the given context.\" },    {\\n\"role\": \"user\", \"content\": f\"Question: {question}\\\\nContext: {context}\"}]# Create a child\\nrunchild_llm_run = pipeline.create_child(    name=\"OpenAI Call\",    run_type=\"llm\",   \\ninputs={\"messages\": messages},)# Generate a completionclient = openai.Client()chat_completion =\\nclient.chat.completions.create(    model=\"gpt-3.5-turbo\", messages=messages)# End the runs and\\nlog\\nthemchild_llm_run.end(outputs=chat_completion)child_llm_run.post()pipeline.end(outputs={\"answer\\n\": chat_completion.choices[0].message.content})pipeline.post()import OpenAI from \"openai\";import {\\nRunTree } from \"langsmith\";// This can be a user input to your appconst question = \"Can you\\nsummarize this morning\\'s meetings?\";const pipeline = new RunTree({    name: \"Chat Pipeline\",   ', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\11.pdf', 'page': 3}),\n",
       " Document(page_content='run_type: \"chain\",    inputs: { question }});// This can be retrieved in a retrieval stepconst context =\\n\"During this morning\\'s meeting, we solved all world conflict.\";const messages = [    { role: \"system\",\\ncontent: \"You are a helpful assistant. Please respond to the user\\'s request only based on the given\\ncontext.\" },    { role: \"user\", content: `Question: ${question}Context: ${context}` }];// Create a child\\nrunconst childRun = await pipeline.createChild({    name: \"OpenAI Call\",    run_type: \"llm\",    inputs: {\\nmessages },});// Generate a completionconst client = new OpenAI();const chatCompletion = await\\nclient.chat.completions.create({    model: \"gpt-3.5-turbo\",    messages: messages,});// End the runs\\nand log themchildRun.end(chatCompletion);await childRun.postRun();pipeline.end({ outputs: {\\nanswer: chatCompletion.choices[0].message.content } });await pipeline.postRun();Was this page\\nhelpful?PreviousSet up a workspaceNextToggle tracing on and offUse @traceable / traceableWrap\\nthe OpenAI clientUse the RunTree APICommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\11.pdf', 'page': 4}),\n",
       " Document(page_content=\"Toggle tracing on and off | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingAnnotate code for tracingToggle tracing on and offLog traces to specific\\nprojectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed\\ntracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever\\ntracesLog custom LLM tracesPrevent logging of inputs and outputs in tracesExport tracesShare or\\nunshare a trace publiclyTrace generator functionsTrace with LangChain (Python and JS/TS)Trace\\nwith Instructor (Python only)DatasetsEvaluationHuman feedbackMonitoring and\\nautomationsPromptsConceptsReferencePricingSelf-hostingHow-to guidesTracingToggle tracing on\\nand offToggle tracing on and offnoteThis section is only relevant for users who areUsing\\n@traceable/traceableWrapping the OpenAI clientUsing LangChainIf you've decided you no longer\\nwant to trace your runs, you can unset the LANGCHAIN_TRACING_V2 environment variable.\\nTraces will no longer be logged to LangSmith.\\nNote that this currently does not affect the RunTree objects or API users.Was this page\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\12.pdf', 'page': 0}),\n",
       " Document(page_content='helpful?PreviousAnnotate code for tracingNextLog traces to specific\\nprojectCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\12.pdf', 'page': 1}),\n",
       " Document(page_content='Log traces to specific project | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingAnnotate code for tracingToggle tracing on and offLog traces to specific\\nprojectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed\\ntracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever\\ntracesLog custom LLM tracesPrevent logging of inputs and outputs in tracesExport tracesShare or\\nunshare a trace publiclyTrace generator functionsTrace with LangChain (Python and JS/TS)Trace\\nwith Instructor (Python only)DatasetsEvaluationHuman feedbackMonitoring and\\nautomationsPromptsConceptsReferencePricingSelf-hostingHow-to guidesTracingLog traces to\\nspecific projectOn this pageLog traces to specific projectYou can change the destination project of\\nyour traces both statically through environment variables and dynamically at runtime.Set the\\ndestination project statically?As mentioned in the Tracing Concepts section, LangSmith uses the\\nconcept of a Project to group traces. If left unspecified, the project is set to default. You can set the\\nLANGCHAIN_PROJECT environment variable to configure a custom project name for an entire', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\13.pdf', 'page': 0}),\n",
       " Document(page_content='application run. This should be done before executing your application.export\\nLANGCHAIN_PROJECT=my-custom-projectIf the project specified does not exist, it will be created\\nautomatically when the first trace is ingested.Set the destination project dynamically?You can also\\nset the project name at program runtime in various ways, depending on how you are annotating\\nyour code for tracing. This is useful when you want to log traces to different projects within the same\\napplication.noteSetting the project name dynamically using one of the below methods overrides the\\nproject name set by the LANGCHAIN_PROJECT environment variable.PythonTypeScriptimport\\nopenaifrom langsmith import traceablefrom langsmith.run_trees import RunTreeclient =\\nopenai.Client()messages = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\":\\n\"user\", \"content\": \"Hello!\"}]# Use the @traceable decorator with the \\'project_name\\' parameter to log\\ntraces to LangSmith# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for\\n@traceable to work@traceable(    run_type=\"llm\",    name=\"OpenAI Call Decorator\",   \\nproject_name=\"My Project\")def call_openai(    messages: list[dict], model: str = \"gpt-3.5-turbo\") ->\\nstr:    return client.chat.completions.create(        model=model,        messages=messages,   \\n).choices[0].message.content# Call the decorated functioncall_openai(messages)# You can also\\nspecify the Project via the project_name parameter# This will override the project_name specified in\\nthe @traceable decoratorcall_openai(    messages,    langsmith_extra={\"project_name\": \"My\\nOverriden Project\"},)# The wrapped OpenAI client accepts all the same langsmith_extra\\nparameters# as @traceable decorated functions, and logs traces to LangSmith automatically.#\\nEnsure that the LANGCHAIN_TRACING_V2 environment variables is set for the wrapper to\\nwork.from langsmith import wrapperswrapped_client =\\nwrappers.wrap_openai(client)wrapped_client.chat.completions.create(    model=\"gpt-3.5-turbo\",   \\nmessages=messages,    langsmith_extra={\"project_name\": \"My Project\"},)# Alternatively, create a\\nRunTree object# You can set the project name using the project_name parameterrt = RunTree(   \\nrun_type=\"llm\",    name=\"OpenAI Call RunTree\",    inputs={\"messages\": messages},   \\nproject_name=\"My Project\")chat_completion = client.chat.completions.create(   ', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\13.pdf', 'page': 1}),\n",
       " Document(page_content='model=\"gpt-3.5-turbo\",    messages=messages,)# End and submit the\\nrunrt.end(outputs=chat_completion)rt.post()import OpenAI from \"openai\";import { traceable } from\\n\"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";import { RunTree} from\\n\"langsmith\";const client = new OpenAI();const messages = [    {role: \"system\", content: \"You are a\\nhelpful assistant.\"},    {role: \"user\", content: \"Hello!\"}];const traceableCallOpenAI = traceable(async\\n(messages: {role: string, content: string}[]) => {    const completion = await\\nclient.chat.completions.create({        model: \"gpt-3.5-turbo\",        messages: messages,    });    return\\ncompletion.choices[0].message.content;},{    run_type: \"llm\",    name: \"OpenAI Call Traceable\",   \\nproject_name: \"My Project\"});// Call the traceable functionawait traceableCallOpenAI(messages,\\n\"gpt-3.5-turbo\");// Create and use a RunTree objectconst rt = new RunTree({    runType: \"llm\",   \\nname: \"OpenAI Call RunTree\",    inputs: { messages },    project_name: \"My Project\"});// Execute a\\nchat completion and handle it within RunTreert.end({outputs: chatCompletion});await\\nrt.postRun();Was this page helpful?PreviousToggle tracing on and offNextSet a sampling rate for\\ntracesSet the destination project staticallySet the destination project\\ndynamicallyCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\13.pdf', 'page': 2}),\n",
       " Document(page_content='Set a sampling rate for traces | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingAnnotate code for tracingToggle tracing on and offLog traces to specific\\nprojectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed\\ntracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever\\ntracesLog custom LLM tracesPrevent logging of inputs and outputs in tracesExport tracesShare or\\nunshare a trace publiclyTrace generator functionsTrace with LangChain (Python and JS/TS)Trace\\nwith Instructor (Python only)DatasetsEvaluationHuman feedbackMonitoring and\\nautomationsPromptsConceptsReferencePricingSelf-hostingHow-to guidesTracingSet a sampling\\nrate for tracesSet a sampling rate for tracesnoteThis section is relevant for those using the\\nLangSmith SDK or LangChain, not for those logging directly with the LangSmith API.By default, all\\ntraces are logged to LangSmith.\\nTo down-sample the number of traces logged to LangSmith, set the\\nLANGCHAIN_TRACING_SAMPLING_RATE environment variable to', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\14.pdf', 'page': 0}),\n",
       " Document(page_content='any float between 0 (no traces) and 1 (all traces).\\nFor instance, setting the following environment variable will log 75% of the traces.export\\nLANGCHAIN_TRACING_SAMPLING_RATE=0.75This works for the traceable decorator and\\nRunTree objects.Was this page helpful?PreviousLog traces to specific projectNextAdd metadata\\nand tags to tracesCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\14.pdf', 'page': 1}),\n",
       " Document(page_content='Add metadata and tags to traces | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingAnnotate code for tracingToggle tracing on and offLog traces to specific\\nprojectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed\\ntracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever\\ntracesLog custom LLM tracesPrevent logging of inputs and outputs in tracesExport tracesShare or\\nunshare a trace publiclyTrace generator functionsTrace with LangChain (Python and JS/TS)Trace\\nwith Instructor (Python only)DatasetsEvaluationHuman feedbackMonitoring and\\nautomationsPromptsConceptsReferencePricingSelf-hostingHow-to guidesTracingAdd metadata and\\ntags to tracesAdd metadata and tags to tracesLangSmith supports sending arbitrary metadata and\\ntags along with traces.Tags are strings that can be used to categorize or label a trace. Metadata is a\\ndictionary of key-value pairs that can be used to store additional information about a trace.Both are\\nuseful for associating additional information with a trace, such as the environment in which it was\\nexecuted, the user who initiated it, or an internal correlation ID.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\15.pdf', 'page': 0}),\n",
       " Document(page_content='For more information on tags and metadata, see the Concepts page. For information on how to\\nquery traces and runs by metadata and tags, see the Filter traces in the application\\npage.PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.run_trees\\nimport RunTreeclient = openai.Client()messages = [    {\"role\": \"system\", \"content\": \"You are a helpful\\nassistant.\"},    {\"role\": \"user\", \"content\": \"Hello!\"}]# Use the @traceable decorator with tags and\\nmetadata# Ensure that the LANGCHAIN_TRACING_V2 environment variables are set for\\n@traceable to work@traceable(    run_type=\"llm\",    name=\"OpenAI Call Decorator\",   \\ntags=[\"my-tag\"],    metadata={\"my-key\": \"my-value\"})def call_openai(    messages: list[dict], model:\\nstr = \"gpt-3.5-turbo\") -> str:    return client.chat.completions.create(        model=model,       \\nmessages=messages,    ).choices[0].message.contentcall_openai(    messages,    # You can also\\nprovide tags and metadata at invocation time    # via the langsmith_extra parameter   \\nlangsmith_extra={\"tags\": [\"my-other-tag\"], \"metadata\": {\"my-other-key\": \"my-value\"}})# Alternatively,\\nyou can create a RunTree object with tags and metadatart = RunTree(    run_type=\"llm\",   \\nname=\"OpenAI Call RunTree\",    inputs={\"messages\": messages},    tags=[\"my-tag\"],   \\nextra={\"metadata\": {\"my-key\": \"my-value\"}})chat_completion = client.chat.completions.create(   \\nmodel=\"gpt-3.5-turbo\",    messages=messages,)# End and submit the\\nrunrt.end(outputs=chat_completion)rt.post()import OpenAI from \"openai\";import { traceable } from\\n\"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";import { RunTree} from\\n\"langsmith\";const client = new OpenAI();const messages = [    {role: \"system\", content: \"You are a\\nhelpful assistant.\"},    {role: \"user\", content: \"Hello!\"}];const traceableCallOpenAI = traceable(async\\n(messages: {role: string, content: string}[]) => {    const completion = await\\nclient.chat.completions.create({        model: \"gpt-3.5-turbo\",        messages: messages,    });    return\\ncompletion.choices[0].message.content;},{    run_type: \"llm\",    name: \"OpenAI Call Traceable\",   \\ntags: [\"my-tag\"],    metadata: { \"my-key\": \"my-value\" }});// Call the traceable functionawait\\ntraceableCallOpenAI(messages, \"gpt-3.5-turbo\");// Create a RunTree objectconst rt = new\\nRunTree({  run_type: \"llm\",  name: \"OpenAI Call RunTree\",  inputs: { messages },  tags: [\"my-tag\"], ', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\15.pdf', 'page': 1}),\n",
       " Document(page_content='extra: { metadata: { \"my-key\": \"my-value\" } },});const chatCompletion = await\\nclient.chat.completions.create({  model: \"gpt-3.5-turbo\",  messages: messages,});// End and submit\\nthe runrt.end(chatCompletion);await rt.postRun();Was this page helpful?PreviousSet a sampling rate\\nfor tracesNextImplement distributed tracingCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\15.pdf', 'page': 2}),\n",
       " Document(page_content='Implement distributed tracing | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingAnnotate code for tracingToggle tracing on and offLog traces to specific\\nprojectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed\\ntracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever\\ntracesLog custom LLM tracesPrevent logging of inputs and outputs in tracesExport tracesShare or\\nunshare a trace publiclyTrace generator functionsTrace with LangChain (Python and JS/TS)Trace\\nwith Instructor (Python only)DatasetsEvaluationHuman feedbackMonitoring and\\nautomationsPromptsConceptsReferencePricingSelf-hostingHow-to guidesTracingImplement\\ndistributed tracingImplement distributed tracingnoteThis feature is currently only available in the\\nPython SDK. Support for TypeScript is coming soon.Sometimes, you need to trace a request across\\nmultiple services.LangSmith supports distributed tracing out of the box, linking runs within a trace\\nacross services using context propagation headers (langsmith-trace and optional baggage for\\nmetadata/tags).Example client-server setup:Trace starts on clientContinues on server# client.pyfrom', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\16.pdf', 'page': 0}),\n",
       " Document(page_content='langsmith.run_helpers import get_current_run_tree, traceableimport httpx@traceableasync def\\nmy_client_function():    headers = {}    async with httpx.AsyncClient(base_url=\"...\") as client:        if\\nrun_tree := get_current_run_tree():            # add langsmith-id to headers           \\nheaders.update(run_tree.to_headers())        return await client.post(\"/my-route\",\\nheaders=headers)Then the server (or other service) can continue the trace by passing the headers\\nin as langsmith_extra:# server.pyfrom langsmith import traceablefrom langsmith.run_helpers import\\ntracing_contextfrom fastapi import FastAPI, Request@traceableasync def my_application():    ...app\\n= FastAPI()  # Or Flask, Django, or any other framework@app.post(\"/my-route\")async def\\nfake_route(request: Request):    # request.headers:  {\"langsmith-trace\": \"...\"}    # as well as optional\\nmetadata/tags in `baggage`    with tracing_context(parent=request.headers):        return await\\nmy_application()The example above uses the tracing_context context manager. You can also\\ndirectly specify the parent run context in the langsmith_extra parameter of a method wrapped with\\n@traceable.from langsmith.run_helpers import traceable, trace# ... same as\\nabove@app.post(\"/my-route\")async def fake_route(request: Request):    # request.headers: \\n{\"langsmith-trace\": \"...\"}    my_application(langsmith_extra={\"parent\": request.headers})Was this\\npage helpful?PreviousAdd metadata and tags to tracesNextAccess the current run (span) within a\\ntraced functionCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\16.pdf', 'page': 1}),\n",
       " Document(page_content='Access the current run (span) within a traced function | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingAnnotate code for tracingToggle tracing on and offLog traces to specific\\nprojectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed\\ntracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever\\ntracesLog custom LLM tracesPrevent logging of inputs and outputs in tracesExport tracesShare or\\nunshare a trace publiclyTrace generator functionsTrace with LangChain (Python and JS/TS)Trace\\nwith Instructor (Python only)DatasetsEvaluationHuman feedbackMonitoring and\\nautomationsPromptsConceptsReferencePricingSelf-hostingHow-to guidesTracingAccess the current\\nrun (span) within a traced functionAccess the current run (span) within a traced functionIn some\\ncases you will want to access the current run (span) within a traced function. This can be useful for\\nextracting UUIDs, tags, or other information from the current run.You can access the current run by\\ncalling the get_current_run_tree/getCurrentRunTree function in the Python or TypeScript SDK,\\nrespectively.For a full list of available properties on the RunTree object, see this', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\17.pdf', 'page': 0}),\n",
       " Document(page_content='reference.PythonTypeScriptfrom langsmith import traceablefrom langsmith.run_helpers import\\nget_current_run_treefrom openai import Clientopenai = Client()@traceabledef\\nformat_prompt(subject):    run = get_current_run_tree()    print(f\"format_prompt Run Id: {run.id}\")   \\nprint(f\"format_prompt Trace Id: {run.trace_id}\")    print(f\"format_prompt Parent Run Id:\\n{run.parent_run.id}\")    return [        {            \"role\": \"system\",            \"content\": \"You are a helpful\\nassistant.\",        },        {            \"role\": \"user\",            \"content\": f\"What\\'s a good name for a store that\\nsells {subject}?\"        }    ]@traceable(run_type=\"llm\")def invoke_llm(messages):    run =\\nget_current_run_tree()    print(f\"invoke_llm Run Id: {run.id}\")    print(f\"invoke_llm Trace Id:\\n{run.trace_id}\")    print(f\"invoke_llm Parent Run Id: {run.parent_run.id}\")    return\\nopenai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0   \\n)@traceabledef parse_output(response):    run = get_current_run_tree()    print(f\"parse_output Run\\nId: {run.id}\")    print(f\"parse_output Trace Id: {run.trace_id}\")    print(f\"parse_output Parent Run Id:\\n{run.parent_run.id}\")    return response.choices[0].message.content@traceabledef run_pipeline():   \\nrun = get_current_run_tree()    print(f\"run_pipeline Run Id: {run.id}\")    print(f\"run_pipeline Trace Id:\\n{run.trace_id}\")    messages = format_prompt(\"colorful socks\")    response = invoke_llm(messages)  \\n return parse_output(response)run_pipeline()import { traceable, getCurrentRunTree } from\\n\"langsmith/traceable\";import OpenAI from \"openai\";const openai = new OpenAI();const\\nformatPrompt = traceable(  (subject: string) => {    const run = getCurrentRunTree();   \\nconsole.log(\"formatPrompt Run ID\", run.id)    console.log(\"formatPrompt Trace ID\", run.trace_id)   \\nconsole.log(\"formatPrompt Parent Run ID\", run.parent_run.id)    return [      {        role: \"system\" as\\nconst,        content: \"You are a helpful assistant.\",      },      {        role: \"user\" as const,        content:\\n`What\\'s a good name for a store that sells ${subject}?`,      },    ];  },  { name: \"formatPrompt\" });const\\ninvokeLLM = traceable(    async (messages: { role: string; content: string }[]) => {        const run =\\ngetCurrentRunTree();        console.log(\"invokeLLM Run ID\", run.id)        console.log(\"invokeLLM\\nTrace ID\", run.trace_id)        console.log(\"invokeLLM Parent Run ID\", run.parent_run.id)        return\\nopenai.chat.completions.create({            model: \"gpt-3.5-turbo\",            messages: messages,           ', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\17.pdf', 'page': 1}),\n",
       " Document(page_content='temperature: 0,        });    },    { run_type: \"llm\", name: \"invokeLLM\" });const parseOutput = traceable( \\n  (response: any) => {        const run = getCurrentRunTree();        console.log(\"parseOutput Run ID\",\\nrun.id)        console.log(\"parseOutput Trace ID\", run.trace_id)        console.log(\"parseOutput Parent\\nRun ID\", run.parent_run.id)        return response.choices[0].message.content;    },    { name:\\n\"parseOutput\" });const runPipeline = traceable(    async () => {        const run =\\ngetCurrentRunTree();        console.log(\"runPipline Run ID\", run.id)        console.log(\"runPipline Trace\\nID\", run.trace_id)        console.log(\"runPipline Parent Run ID\", run.parent_run?.id)        const\\nmessages = await formatPrompt(\"colorful socks\");        const response = await\\ninvokeLLM(messages);        return parseOutput(response);    },    { name: \"runPipeline\" });await\\nrunPipeline();Was this page helpful?PreviousImplement distributed tracingNextLog multimodal\\ntracesCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\17.pdf', 'page': 2}),\n",
       " Document(page_content='Log multimodal traces | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingAnnotate code for tracingToggle tracing on and offLog traces to specific\\nprojectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed\\ntracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever\\ntracesLog custom LLM tracesPrevent logging of inputs and outputs in tracesExport tracesShare or\\nunshare a trace publiclyTrace generator functionsTrace with LangChain (Python and JS/TS)Trace\\nwith Instructor (Python only)DatasetsEvaluationHuman feedbackMonitoring and\\nautomationsPromptsConceptsReferencePricingSelf-hostingHow-to guidesTracingLog multimodal\\ntracesLog multimodal tracesLangSmith supports logging and rendering images as part of traces.\\nThis is currently supported for multimodal LLM runs.In order to log images, use wrap_openai/\\nwrapOpenAI in Python or TypeScript respectively and pass an image URL or base64 encoded\\nimage as part of the input.PythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers\\nimport wrap_openaiclient = wrap_openai(OpenAI())response = client.chat.completions.create( ', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\18.pdf', 'page': 0}),\n",
       " Document(page_content='model=\"gpt-4-turbo\",  messages=[    {      \"role\": \"user\",      \"content\": [        {\"type\": \"text\", \"text\":\\n\"What?s in this image?\"},        {          \"type\": \"image_url\",          \"image_url\": {            \"url\":\\n\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-bo\\nardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",          },        },      ],    } \\n],)print(response.choices[0])import OpenAI from \"openai\";import { wrapOpenAI } from\\n\"langsmith/wrappers\";// Wrap the OpenAI client to automatically log tracesconst wrappedClient =\\nwrapOpenAI(new OpenAI());const response = await wrappedClient.chat.completions.create({   \\nmodel: \"gpt-4-turbo\",    messages: [      {        role: \"user\",        content: [          { type: \"text\", text:\\n\"What?s in this image?\" },          {            type: \"image_url\",            image_url: {              \"url\":\\n\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-bo\\nardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",            },          },        ],      },  \\n ],});console.log(response.choices[0]);The image will be rendered as part of the trace in the\\nLangSmith UI.Was this page helpful?PreviousAccess the current run (span) within a traced\\nfunctionNextLog retriever tracesCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\18.pdf', 'page': 1}),\n",
       " Document(page_content=\"Log retriever traces | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingAnnotate code for tracingToggle tracing on and offLog traces to specific\\nprojectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed\\ntracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever\\ntracesLog custom LLM tracesPrevent logging of inputs and outputs in tracesExport tracesShare or\\nunshare a trace publiclyTrace generator functionsTrace with LangChain (Python and JS/TS)Trace\\nwith Instructor (Python only)DatasetsEvaluationHuman feedbackMonitoring and\\nautomationsPromptsConceptsReferencePricingSelf-hostingHow-to guidesTracingLog retriever\\ntracesLog retriever tracesnoteNothing will break if you don't log retriever traces in the correct format\\nand data will still be logged. However, the data will not be rendered in a way that is specific to\\nretriever steps.Many LLM applications require looking up documents from vector databases,\\nknowledge graphs, or other types of indexes. Retriever traces are a way to log the documents that\\nare retrieved by the retriever.\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\19.pdf', 'page': 0}),\n",
       " Document(page_content='LangSmith provides special rendering for retrieval steps in traces to make it easier to understand\\nand diagnose retrieval issues. In order for retrieval steps to be rendered correctly, a few small steps\\nneed to be taken.Annotate the retriever step with run_type=\"retriever\".Return a list of Python\\ndictionaries or TypeScript objects from the retriever step. Each dictionary should contain the\\nfollowing keys:page_content: The text of the document.type: This should always be\\n\"Document\".metadata: A python dictionary or TypeScript object containing metadata about the\\ndocument. This metadata will be displayed in the trace.The following code snippets show how to log\\na retrieval steps in Python and TypeScript.PythonTypeScriptfrom langsmith import traceabledef\\n_convert_docs(results):    return [        {            \"page_content\": r,            \"type\": \"Document\",           \\n\"metadata\": {\"foo\": \"bar\"}        }        for r in results    ]@traceable(run_type=\"retriever\")def\\nretrieve_docs(query):    # Foo retriever returning hardcoded dummy documents.    # In production,\\nthis could be a real vector datatabase or other document index.    contents = [\"Document contents\\n1\", \"Document contents 2\", \"Document contents 3\"]    return\\n_convert_docs(contents)retrieve_docs(\"User query\")import { traceable } from\\n\"langsmith/traceable\";interface Document {  page_content: string;  type: string;  metadata: { foo:\\nstring };}function convertDocs(results: string[]): Document[] {  return results.map((r) => ({   \\npage_content: r,    type: \"Document\",    metadata: { foo: \"bar\" }  }));}const retrieveDocs = traceable( \\n(query: string): Document[] => {    // Foo retriever returning hardcoded dummy documents.    // In\\nproduction, this could be a real vector database or other document index.    const contents =\\n[\"Document contents 1\", \"Document contents 2\", \"Document contents 3\"];    return\\nconvertDocs(contents);  },  { name: \"retrieveDocs\", run_type: \"retriever\" } // Configuration for\\ntraceable);await retrieveDocs(\"User query\");The following image shows how a retriever step is\\nrendered in a trace. The contents along with the metadata are displayed with each document.Was\\nthis page helpful?PreviousLog multimodal tracesNextLog custom LLM\\ntracesCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\19.pdf', 'page': 1}),\n",
       " Document(page_content='2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\19.pdf', 'page': 2}),\n",
       " Document(page_content=\"Add observability to your LLM application | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsAdd observability to\\nyour LLM applicationEvaluate your LLM applicationOptimize a classifierHow-to\\nguidesConceptsReferencePricingSelf-hostingTutorialsAdd observability to your LLM applicationOn\\nthis pageAdd observability to your LLM applicationObservability is important for any software\\napplication, but especially so for LLM applications.\\nLLMs are non-deterministic by nature, meaning they can produce unexpected results.\\nThis makes them trickier than normal to debug.Luckily, this is where LangSmith can help!\\nLangSmith has LLM-native observability, allowing you to get meaningful insights into your\\napplication.Note that observability is important throughout all stages of application development -\\nfrom prototyping, to beta testing, to production.\\nThere are different considerations at all stages, but they are all intricately tied together.\\nIn this tutorial we walk through the natural progression.Let's assume that we're building a simple\\nRAG application using the OpenAI SDK.\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\2.pdf', 'page': 0}),\n",
       " Document(page_content='The simple application we\\'re adding observability to looks like:PythonTypeScriptfrom openai import\\nOpenAIopenai_client = OpenAI()# This is the retriever we will use in RAG# This is mocked out, but it\\ncould be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]   \\nreturn results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef\\nrag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using\\nonly the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return\\nopenai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\":\\nsystem_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-3.5-turbo\",   \\n)import { OpenAI } from \"openai\";const openAIClient = new OpenAI();// This is the retriever we will\\nuse in RAG// This is mocked out, but it could be anything we wantasync function retriever(query:\\nstring) {  return [\"This is a document\"];}// This is the end-to-end RAG chain.// It does a retrieval step\\nthen calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);   \\nconst systemMessage =    \"Answer the users question using only the provided information\\nbelow:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [\\n     { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model:\\n\"gpt-3.5-turbo\",  });}Prototyping?Having observability set up from the start can you help iterate much\\nmore quickly than you would otherwise be able to.\\nIt allows you to have great visibility into your application as you are rapidly iterating on the prompt, or\\nchanging the data and models you are using.\\nIn this section we\\'ll walk through how to set up observability so you can have maximal observability\\nas you are prototyping.Set up your environment?First, create an API key by navigating to the\\nsettings page.Next, install the LangSmith SDK:Python SDKTypeScript SDKpip install langsmithnpm\\ninstall langsmithFinally, set up the appropriate environment variables. This will log traces to the\\ndefault project (though you can easily change that).export LANGCHAIN_TRACING_V2=trueexport\\nLANGCHAIN_API_KEY=<your-api-key>export LANGCHAIN_PROJECT=defaultTrace your LLM\\ncalls?The first thing you might want to trace is all your OpenAI calls.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\2.pdf', 'page': 1}),\n",
       " Document(page_content='After all, this is where the LLM is actually being called, so it is the most important part!\\nWe\\'ve tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI\\nwrapper.\\nAll you have to do is modify your code to look something like:PythonTypeScriptfrom openai import\\nOpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This\\nis the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef\\nretriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the\\nend-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs =\\nretriever(question)    system_message = \"\"\"Answer the users question using only the provided\\ninformation below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return\\nopenai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\":\\nsystem_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-3.5-turbo\",   \\n)import { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";const\\nopenAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is\\nmocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is\\na document\"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync\\nfunction rag(question: string) {  const docs = await retriever(question);    const systemMessage =   \\n\"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");     \\nreturn await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content:\\nsystemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-3.5-turbo\",  });}Notice how\\nwe import from langsmith.wrappers import wrap_openai and use it to wrap the OpenAI client\\n(openai_client = wrap_openai(OpenAI())).What happens if you call it in the following\\nway?rag(\"where did harrison work\")This will produce a trace of just the OpenAI call - it should look\\nsomething like thisTrace the whole chain?Great - we\\'ve traced the LLM call. But it\\'s often very\\ninformative to trace more than that.\\nLangSmith is built for tracing the entire LLM pipeline - so let\\'s do that!', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\2.pdf', 'page': 2}),\n",
       " Document(page_content='We can do this by modifying the code to now look something like this:PythonTypeScriptfrom openai\\nimport OpenAIfrom langsmith import traceablefrom langsmith.wrappers import\\nwrap_openaiopenai_client = wrap_openai(OpenAI())def retriever(query: str):    results = [\"Harrison\\nworked at Kensho\"]    return results@traceabledef rag(question):    docs = retriever(question)   \\nsystem_message = \"\"\"Answer the users question using only the provided information below:       \\n{docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(       \\nmessages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\":\\nquestion},        ],        model=\"gpt-3.5-turbo\",    )import { OpenAI } from \"openai\";import { traceable }\\nfrom \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient =\\nwrapOpenAI(new OpenAI());async function retriever(query: string) {  return [\"This is a\\ndocument\"];}const rag = traceable(async function rag(question: string) {  const docs = await\\nretriever(question);    const systemMessage =    \"Answer the users question using only the provided\\ninformation below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({   \\nmessages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],\\n   model: \"gpt-3.5-turbo\",  });});Notice how we import from langsmith import traceable and use it\\ndecorate the overall function (@traceable).What happens if you call it in the following\\nway?rag(\"where did harrison work\")This will produce a trace of just the entire pipeline (with the\\nOpenAI call as a child run) - it should look something like thisTrace the retrieval step?There\\'s one\\nlast part of the application we haven\\'t traced - the retrieval step!\\nRetrieval is a key part of LLM applications, and we\\'ve made it easy to log retrieval steps as well.\\nAll we have to do is modify our code to look like:PythonTypeScriptfrom openai import OpenAIfrom\\nlangsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client =\\nwrap_openai(OpenAI())@traceable(run_type=\"retriever\")def retriever(query: str):    results =\\n[\"Harrison worked at Kensho\"]    return results@traceabledef rag(question):    docs =\\nretriever(question)    system_message = \"\"\"Answer the users question using only the provided\\ninformation below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\2.pdf', 'page': 3}),\n",
       " Document(page_content='openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\":\\nsystem_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-3.5-turbo\",   \\n)import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI\\n} from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());const retriever =\\ntraceable(  async function retriever(query: string) {    return [\"This is a document\"];  },  { run_type:\\n\"retriever\" })const rag = traceable(async function rag(question: string) {  const docs = await\\nretriever(question);    const systemMessage =    \"Answer the users question using only the provided\\ninformation below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({   \\nmessages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],\\n   model: \"gpt-3.5-turbo\",  });});Notice how we import from langsmith import traceable and use it\\ndecorate the overall function (@traceable(run_type=\"retriever\")).What happens if you call it in the\\nfollowing way?rag(\"where did harrison work\")This will produce a trace of the whole chain including\\nthe retrieval step - it should look something like thisBeta Testing?The next stage of LLM application\\ndevelopment is beta testing your application.\\nThis is when you release it to a few initial users.\\nHaving good observability set up here is crucial as often you don\\'t know exactly how users will\\nactually use your application, so this allows you get insights into how they do so.\\nThis also means that you probably want to make some changes to your tracing set up to better allow\\nfor that.\\nThis extends the observability you set up in the previous sectionCollecting Feedback?A huge part of\\nhaving good observability during beta testing is collecting feedback.\\nWhat feedback you collect is often application specific - but at the very least a simple thumbs\\nup/down is a good start.\\nAfter logging that feedback, you need to be able to easily associate it with the run that caused that.\\nLuckily LangSmith makes it easy to do that.First, you need to log the feedback from your app.\\nAn easy way to do this is to keep track of a run ID for each run, and then use that to log feedback.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\2.pdf', 'page': 4}),\n",
       " Document(page_content='Keeping track of the run ID would look something like:import uuidrun_id = str(uuid.uuid4())rag(   \\n\"where did harrison work\",    langsmith_extra={\"run_id\": run_id})Associating feedback with that run\\nwould look something like:from langsmith import Clientls_client = Client()ls_client.create_feedback(  \\n run_id,    key=\"user-score\",    score=1.0,)Once the feedback is logged, you can then see it\\nassociated with each run by clicking into the Metadata tab when inspecting the run.\\nIt should look something like thisYou can also query for all runs with positive (or negative) feedback\\nby using the filtering logic in the runs table.\\nYou can do this by creating a filter like the following:Logging Metadata?It is also a good idea to start\\nlogging metadata.\\nThis allows you to start keep track of different attributes of your app.\\nThis is important in allowing you to know what version or variant of your app was used to produce a\\ngiven result.For this example, we will log the LLM used.\\nOftentimes you may be experimenting with different LLMs, so having that information as metadata\\ncan be useful for filtering.\\nIn order to do that, we can add it as such:from openai import OpenAIfrom langsmith import\\ntraceablefrom langsmith.wrappers import wrap_openaiopenai_client =\\nwrap_openai(OpenAI())@traceable(run_type=\"retriever\")def retriever(query: str):    results =\\n[\"Harrison worked at Kensho\"]    return results@traceable(metadata={\"llm\": \"gpt-3.5-turbo\"})def\\nrag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using\\nonly the provided information below:    {docs}\"\"\".format(docs=\\'\\\\n\\'.join(docs))    return\\nopenai_client.chat.completions.create(messages = [        {\"role\": \"system\", \"content\":\\nsystem_message},        {\"role\": \"user\", \"content\": question},    ], model=\"gpt-3.5-turbo\")Notice we\\nadded @traceable(metadata={\"llm\": \"gpt-3.5-turbo\"}) to the rag function.Keeping track of metadata\\nin this way assumes that it is known ahead of time.\\nThis is fine for LLM types, but less desirable for other types of information - like a User ID.\\nIn order to log information that, we can pass it in at run time with the run ID.import uuidrun_id =', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\2.pdf', 'page': 5}),\n",
       " Document(page_content='str(uuid.uuid4())rag(    \"where did harrison work\",    langsmith_extra={\"run_id\": run_id, \"metadata\":\\n{\"user_id\": \"harrison\"}})Now that we\\'ve logged these two pieces of metadata, we should be able to\\nsee them both show up in the UI here.We can filter for these pieces of information by constructing a\\nfilter like the following:Production?Great - you\\'ve used this newfound observability to iterate quickly\\nand gain confidence that your app is performing well.\\nTime to ship it to production!\\nWhat new observability do you need to add?First of all, let\\'s note that the same observability you\\'ve\\nalready added will keep on providing value in production.\\nYou will continue to be able to drill down into particular runs.In production you likely have a LOT\\nmore traffic. So you don\\'t really want to be stuck looking at datapoints one at a time.\\nLuckily, LangSmith has a set of tools to help with observability in production.Monitoring?If you click\\non the Monitor tab in a project, you will see a series of monitoring charts.\\nHere we track lots of LLM specific statistics - number of traces, feedback, time-to-first-token, etc.\\nYou can view these over time across a few different time bins.A/B Testing?noteGroup-by\\nfunctionality for A/B testing requires at least 2 different values to exist for a given metadata key.You\\ncan also use this tab to perform a version of A/B Testing.\\nIn the previous tutorial we starting tracking a few different metadata attributes - one of which was\\nllm.\\nWe can group the monitoring charts by ANY metadata attribute, and instantly get grouped charts\\nover time.\\nThis allows us to experiment with different LLMs (or prompts, or other) and track their performance\\nover time.In order to do this, we just need to click on the Metadata button at the top.\\nThis will give us a drop down of options to choose from to group by:Once we select this, we will start\\nto see charts grouped by this attribute:Drilldown?One of the awesome abilities that LangSmith\\nprovides is the ability to easily drilldown into datapoints that you identify\\nas problematic while looking at monitoring charts.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\2.pdf', 'page': 6}),\n",
       " Document(page_content='In order to do this, you can simply hover over a datapoint in the monitoring chart.\\nWhen you do this, you will be able to click the datapoint.\\nThis will lead you back to the runs table with a filtered view:Conclusion?In this tutorial you saw how\\nto set up your LLM application with best-in-class observability.\\nNo matter what stage your application is in, you will still benefit from observability.If you have more\\nin-depth questions about observability, check out the how-to section for guides on topics like testing,\\nprompt management, and more.Observability is not the only thing LangSmith can help with!\\nIt can also help with evaluation, optimization, and more!\\nCheck out the other tutorials to see how to get started with those.Was this page\\nhelpful?PreviousTutorialsNextEvaluate your LLM applicationPrototypingSet up your\\nenvironmentTrace your LLM callsTrace the whole chainTrace the retrieval stepBeta\\nTestingCollecting FeedbackLogging MetadataProductionMonitoringA/B\\nTestingDrilldownConclusionCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\2.pdf', 'page': 7}),\n",
       " Document(page_content=\"Log custom LLM traces | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingAnnotate code for tracingToggle tracing on and offLog traces to specific\\nprojectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed\\ntracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever\\ntracesLog custom LLM tracesPrevent logging of inputs and outputs in tracesExport tracesShare or\\nunshare a trace publiclyTrace generator functionsTrace with LangChain (Python and JS/TS)Trace\\nwith Instructor (Python only)DatasetsEvaluationHuman feedbackMonitoring and\\nautomationsPromptsConceptsReferencePricingSelf-hostingHow-to guidesTracingLog custom LLM\\ntracesOn this pageLog custom LLM tracesnoteNothing will break if you don't log LLM traces in the\\ncorrect format and data will still be logged. However, the data will not be processed or rendered in a\\nway that is specific to LLMs.The best way to logs traces from OpenAI models is to use the wrapper\\navailable in the langsmith SDK for Python and TypeScript. However, you can also log traces from\\ncustom models by following the guidelines below.LangSmith provides special rendering and\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\20.pdf', 'page': 0}),\n",
       " Document(page_content='processing for LLM traces, including token counting (assuming token counts are not available from\\nthe model provider).\\nIn order to make the most of this feature, you must log your LLM traces in a specific,\\nOpenAI-compatible format.Chat-style models?For chat-style models, inputs must be a list of\\nmessages, represented as Python dictionaries or TypeScript object. Each message must contain\\nthe key role and content.The output must return a dictionary/object that contains the key choices\\nwith a value that is a list of dictionaries/objects. Each dictionary/object must contain the key\\nmessage, which maps to a message object with the keys role and content.The input to your function\\nshould be named messages.PythonTypeScriptfrom langsmith import traceableinputs = [    {\"role\":\\n\"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"I\\'d like to book a\\ntable for two.\"},]output = {    \"choices\": [        {            \"message\": {                \"role\": \"assistant\",           \\n    \"content\": \"Sure, what time would you like to book the table for?\"            }        }   \\n]}@traceable(run_type=\"llm\")def chat_model(messages: list):    return\\noutputchat_model(inputs)import { traceable } from \"langsmith/traceable\";const messages = [  { role:\\n\"system\", content: \"You are a helpful assistant.\" },  { role: \"user\", content: \"I\\'d like to book a table for\\ntwo.\" }];const output = {  choices: [    {      message: {        role: \"assistant\",        content: \"Sure, what\\ntime would you like to book the table for?\"      }    }  ]};const chatModel = traceable(  async ({\\nmessages }: { messages: { role: string; content: string }[] }) => {    return output;  },  { run_type: \"llm\",\\nname: \"chat_model\" });await chatModel({ messages });The above code will log the following\\ntrace:Specify model name?You can configure the model name by passing the model parameter to\\nthe function you are wrapping/decorating with traceable. The model parameter is used to match cost\\ninformation.PythonTypeScriptfrom langsmith import traceableinputs = [    {\"role\": \"system\", \"content\":\\n\"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"I\\'d like to book a table for two.\"},]output =\\n{    \"choices\": [        {            \"message\": {                \"role\": \"assistant\",                \"content\": \"Sure, what\\ntime would you like to book the table for?\"            }        }    ]}@traceable(run_type=\"llm\")def\\nchat_model(messages: list, model: str):    return outputchat_model(inputs, \"gpt-3.5-turbo\")import {', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\20.pdf', 'page': 1}),\n",
       " Document(page_content='traceable } from \"langsmith/traceable\";const messages = [  { role: \"system\", content: \"You are a\\nhelpful assistant.\" },  { role: \"user\", content: \"I\\'d like to book a table for two.\" }];const output = { \\nchoices: [    {      message: {        role: \"assistant\",        content: \"Sure, what time would you like to\\nbook the table for?\"      }    }  ]};const chatModel = traceable(  async ({ messages, model }: {\\nmessages: { role: string; content: string }[]; model: string }) => {    return output;  },  { run_type: \"llm\",\\nname: \"chat_model\" });await chatModel({ messages, model: \"gpt-3.5-turbo\" });Stream outputs?For\\nstreaming, you can \"reduce\" the outputs into the same format as the non-streaming version:def\\n_reduce_chunks(chunks: list):    all_text = \"\".join([chunk[\"choices\"][0][\"message\"][\"content\"] for\\nchunk in chunks])    return {\"choices\": [{\"message\": {\"content\": all_text, \"role\":\\n\"assistant\"}}]}@traceable(run_type=\"llm\", reduce_fn=_reduce_chunks)def\\nmy_streaming_chat_model(messages: list, model: str):    for chunk in [\"Hello, \" +\\nmessages[1][\"content\"]]:        yield {            \"choices\": [                {                    \"message\": {                \\n       \"content\": chunk,                        \"role\": \"assistant\",                    }                }            ]        }list(   \\nmy_streaming_chat_model(        [            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\\nPlease greet the user.\"},            {\"role\": \"user\", \"content\": \"polly the parrot\"},        ],       \\nmodel=\"gpt-3.5-turbo\",    ))Manually provide token counts?By default, LangSmith uses tiktoken to\\ncount tokens, utilizing a best guess at the model\\'s tokenizer based on the model parameter you\\nprovide. To manually provide token counts, you can add a usage key to the function\\'s response,\\ncontaining a dictionary with the keys prompt_tokens and completion_tokens.PythonTypeScriptfrom\\nlangsmith import traceablefrom langsmith.run_helpers import get_current_run_treeinputs = [   \\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"I\\'d like to\\nbook a table for two.\"},]output = {    \"choices\": [        {            \"message\": {                \"role\":\\n\"assistant\",                \"content\": \"Sure, what time would you like to book the table for?\"            }        } \\n  ],    \"usage\": {        \"prompt_tokens\": 27,        \"completion_tokens\": 13,   \\n},}@traceable(run_type=\"llm\")def chat_model(messages: list, model: str):    run_tree =\\nget_current_run_tree()    run_tree.extra[\"batch_size\"] = 1    return outputchat_model(inputs,', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\20.pdf', 'page': 2}),\n",
       " Document(page_content='\"gpt-3.5-turbo\")import { traceable, getCurrentRunTree } from \"langsmith/traceable\";const messages\\n= [  { role: \"system\", content: \"You are a helpful assistant.\" },  { role: \"user\", content: \"I\\'d like to book\\na table for two.\" },];const output = {  choices: [    {      message: {        role: \"assistant\",        content:\\n\"Sure, what time would you like to book the table for?\",      },    },  ],  usage: {    \"prompt_tokens\": 27,  \\n \"completion_tokens\": 13,  },};const chatModel = traceable(  async ({    messages,    model,  }: {   \\nmessages: { role: string; content: string }[];    model: string;  }) => {    const run =\\ngetCurrentRunTree();    run.extra.batch_size = 1    return output;  },  { run_type: \"llm\", name:\\n\"chat_model\" });await chatModel({ messages, model: \"gpt-3.5-turbo\" });Instruct-style models?For\\ninstruct-style models (string in, string out), your inputs must contain a key prompt with a string value.\\nOther inputs are also permitted. The output must return an object that, when serialized, contains the\\nkey choices with a list of dictionaries/objects. Each must contain the key text with a string\\nvalue.PythonTypeScript@traceable(run_type=\"llm\")def hello_llm(prompt: str):    return {       \\n\"choices\": [            {\"text\": \"Hello, \" + prompt}        ],    }hello_llm(\"polly the parrot\\\\n\")import { traceable\\n} from \"langsmith/traceable\";const helloLLM = traceable(  ({ prompt }: { prompt: string }) => {    return\\n{      choices: [        { text: \"Hello, \" + prompt }      ]    };  },  { run_type: \"llm\", name: \"hello_llm\" });await\\nhelloLLM({ prompt: \"polly the parrot\\\\n\" });The above code will log the following trace:Was this page\\nhelpful?PreviousLog retriever tracesNextPrevent logging of inputs and outputs in tracesChat-style\\nmodelsSpecify model nameStream outputsManually provide token countsInstruct-style\\nmodelsCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\20.pdf', 'page': 3}),\n",
       " Document(page_content='Prevent logging of inputs and outputs in traces | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingAnnotate code for tracingToggle tracing on and offLog traces to specific\\nprojectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed\\ntracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever\\ntracesLog custom LLM tracesPrevent logging of inputs and outputs in tracesExport tracesShare or\\nunshare a trace publiclyTrace generator functionsTrace with LangChain (Python and JS/TS)Trace\\nwith Instructor (Python only)DatasetsEvaluationHuman feedbackMonitoring and\\nautomationsPromptsConceptsReferencePricingSelf-hostingHow-to guidesTracingPrevent logging of\\ninputs and outputs in tracesPrevent logging of inputs and outputs in tracesIn some situations, you\\nmay need to prevent the inputs and outputs of your traces from being logged for privacy or security\\nreasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are\\nsent to the LangSmith backend.If you want to completely hide the inputs and outputs of your traces,\\nyou can set the following environment variables when running your', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\21.pdf', 'page': 0}),\n",
       " Document(page_content='application:LANGCHAIN_HIDE_INPUTS=trueLANGCHAIN_HIDE_OUTPUTS=trueThis works for\\nboth the LangSmith SDK (Python and TypeScript) and LangChain.You can also customize and\\noverride this behavior for a given Client instance. This can be done by setting the hide_inputs and\\nhide_outputs parameters on the Client object.\\nFor the example below, we will simply return an empty object for both hide_inputs and hide_outputs,\\nbut you can customize this to your needs.PythonTypeScriptimport openaifrom langsmith import\\nClientfrom langsmith.wrappers import wrap_openaiopenai_client =\\nwrap_openai(openai.Client())langsmith_client = Client(    hide_inputs=lambda inputs: {},\\nhide_outputs=lambda outputs: {})# The trace produced will have its metadata present, but the inputs\\nwill be hiddenopenai_client.chat.completions.create(    model=\"gpt-3.5-turbo\",    messages=[       \\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"Hello!\"},   \\n],    langsmith_extra={\"client\": langsmith_client},)# The trace produced will not have hidden inputs\\nand outputsopenai_client.chat.completions.create(    model=\"gpt-3.5-turbo\",    messages=[       \\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"Hello!\"},   \\n],)import OpenAI from \"openai\";import { Client } from \"langsmith\";import { wrapOpenAI } from\\n\"langsmith/wrappers\";const langsmithClient = new Client({  hideInputs: (inputs) => ({}),  hideOutputs:\\n(outputs) => ({}),});// The trace produced will have its metadata present, but the inputs will be\\nhiddenconst filteredOAIClient = wrapOpenAI(new OpenAI(), {    client: langsmithClient,});await\\nfilteredOAIClient.chat.completions.create({    model: \"gpt-3.5-turbo\",    messages: [      { role:\\n\"system\", content: \"You are a helpful assistant.\" },      { role: \"user\", content: \"Hello!\" },    ],});const\\nopenaiClient = wrapOpenAI(new OpenAI());// The trace produced will not have hidden inputs and\\noutputsawait openaiClient.chat.completions.create({    model: \"gpt-3.5-turbo\",    messages: [      {\\nrole: \"system\", content: \"You are a helpful assistant.\" },      { role: \"user\", content: \"Hello!\" },   \\n],});Was this page helpful?PreviousLog custom LLM tracesNextExport\\ntracesCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\21.pdf', 'page': 1}),\n",
       " Document(page_content='2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\21.pdf', 'page': 2}),\n",
       " Document(page_content='Export traces | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingAnnotate code for tracingToggle tracing on and offLog traces to specific\\nprojectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed\\ntracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever\\ntracesLog custom LLM tracesPrevent logging of inputs and outputs in tracesExport tracesShare or\\nunshare a trace publiclyTrace generator functionsTrace with LangChain (Python and JS/TS)Trace\\nwith Instructor (Python only)DatasetsEvaluationHuman feedbackMonitoring and\\nautomationsPromptsConceptsReferencePricingSelf-hostingHow-to guidesTracingExport tracesOn\\nthis pageExport tracesRecommended ReadingBefore diving into this content, it might be helpful to\\nread the following:Run (span) data formatLangSmith API ReferenceLangSmith trace query\\nsyntaxThe recommended way to export runs (the span data in LangSmith traces) is to use the\\nlist_runs method in the SDK or /runs/query endpoint in the API.LangSmith stores traces in a simple\\nformat that is specified in the Run (span) data format.Use filter arguments?For simple queries, you', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\22.pdf', 'page': 0}),\n",
       " Document(page_content='don\\'t have to rely on our query syntax. You can use the filter arguments specified in the filter\\narguments reference.PrerequisitesInitialize the client before running the below code\\nsnippets.PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client, Run } from\\n\"langsmith\";const client = new Client();Below are some examples of ways to list runs using keyword\\narguments:List all runs in a project?PythonTypeScriptproject_runs =\\nclient.list_runs(project_name=\"<your_project>\")// Download runs in a projectconst projectRuns:\\nRun[] = [];for await (const run of client.listRuns({  projectName: \"<your_project>\",})) { \\nprojectRuns.push(run);};List LLM and Chat runs in the last 24\\nhours?PythonTypeScripttodays_llm_runs = client.list_runs(    project_name=\"<your_project>\",   \\nstart_time=datetime.now() - timedelta(days=1),    run_type=\"llm\",)const todaysLlmRuns: Run[] =\\n[];for await (const run of client.listRuns({  projectName: \"<your_project>\",  startTime: new\\nDate(Date.now() - 1000 * 60 * 60 * 24),  runType: \"llm\",})) {  todaysLlmRuns.push(run);};List root\\nruns in a project?Root runs are runs that have no parents. These are assigned a value of True for\\nis_root. You can use this to filter for root runs.PythonTypeScriptroot_runs = client.list_runs(   \\nproject_name=\"<your_project>\",    is_root=True)const rootRuns: Run[] = [];for await (const run of\\nclient.listRuns({  projectName: \"<your_project>\",  isRoot: 1,})) {  rootRuns.push(run);};List runs\\nwithout errors?PythonTypeScriptcorrect_runs = client.list_runs(project_name=\"<your_project>\",\\nerror=False)const correctRuns: Run[] = [];for await (const run of client.listRuns({  projectName:\\n\"<your_project>\",  error: false,})) {  correctRuns.push(run);};List runs by run ID?Ignores Other\\nArgumentsIf you provide a list of run IDs in the way described above, it will ignore all other filtering\\narguments like project_name, run_type, etc. and directly return the runs matching the given IDs.If\\nyou have a list of run IDs, you can list them directly:PythonTypeScriptrun_ids =\\n[\\'a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836\\',\\'9398e6be-964f-4aa4-8ae9-ad78cd4b7074\\']selected_ru\\nns = client.list_runs(id=run_ids)const runIds = [  \"a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836\", \\n\"9398e6be-964f-4aa4-8ae9-ad78cd4b7074\",];const selectedRuns: Run[] = [];for await (const run of\\nclient.listRuns({  id: runIds,})) {  selectedRuns.push(run);};Use filter query language?For more', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\22.pdf', 'page': 1}),\n",
       " Document(page_content='complex queries, you can use the query language described in the filter query language\\nrefernece.List all runs called \"extractor\" whose root of the trace was assigned feedback \"user_score\"\\nscore of 1?PythonTypeScriptclient.list_runs(    project_name=\"<your_project>\",    filter=\\'eq(name,\\n\"extractor\")\\',    trace_filter=\\'and(eq(feedback_key, \"user_score\"), eq(feedback_score,\\n1))\\')client.listRuns({  projectName: \"<your_project>\",  filter: \\'eq(name, \"extractor\")\\',  traceFilter:\\n\\'and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))\\'})List runs with \"star_rating\" key\\nwhose score is greater than 4?PythonTypeScriptclient.list_runs(    project_name=\"<your_project>\",  \\n filter=\\'and(eq(feedback_key, \"star_rating\"), gt(feedback_score, 4))\\')client.listRuns({  projectName:\\n\"<your_project>\",  filter: \\'and(eq(feedback_key, \"star_rating\"), gt(feedback_score, 4))\\'})List runs that\\ntook longer than 5 seconds to\\ncomplete?PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter=\\'gt(latency,\\n\"5s\")\\')client.listRuns({projectName: \"<your_project>\", filter: \\'gt(latency, \"5s\")\\'})List all runs where\\ntotal_tokens is greater than 5000?PythonTypeScriptclient.list_runs(project_name=\"<your_project>\",\\nfilter=\\'gt(total_tokens, 5000)\\')client.listRuns({projectName: \"<your_project>\", filter: \\'gt(total_tokens,\\n5000)\\'})List all runs that have \"error\" not equal to\\nnull?PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter=\\'neq(error,\\nnull)\\')client.listRuns({projectName: \"<your_project>\", filter: \\'neq(error, null)\\'})List all runs where\\nstart_time is greater than a specific\\ntimestamp?PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter=\\'gt(start_time,\\n\"2023-07-15T12:34:56Z\")\\')client.listRuns({projectName: \"<your_project>\", filter: \\'gt(start_time,\\n\"2023-07-15T12:34:56Z\")\\'})List all runs that contain the string\\n\"substring\"?PythonTypeScriptclient.list_runs(project_name=\"<your_project>\",\\nfilter=\\'search(\"substring\")\\')client.listRuns({projectName: \"<your_project>\", filter:\\n\\'search(\"substring\")\\'})List all runs that are tagged with the git hash\\n\"2aa1cf4\"?PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter=\\'has(tags,\\n\"2aa1cf4\")\\')client.listRuns({projectName: \"<your_project>\", filter: \\'has(tags, \"2aa1cf4\")\\'})List all', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\22.pdf', 'page': 2}),\n",
       " Document(page_content='\"chain\" type runs that took more than 10 seconds and?had total_tokens greater than\\n5000PythonTypeScriptclient.list_runs(  project_name=\"<your_project>\",  filter=\\'and(eq(run_type,\\n\"chain\"), gt(latency, 10), gt(total_tokens, 5000))\\')client.listRuns({  projectName: \"<your_project>\", \\nfilter: \\'and(eq(run_type, \"chain\"), gt(latency, 10), gt(total_tokens, 5000))\\'})List all runs that started\\nafter a specific timestamp and either?have \"error\" not equal to null or a \"Correctness\" feedback\\nscore equal to 0PythonTypeScriptclient.list_runs(  project_name=\"<your_project>\", \\nfilter=\\'and(gt(start_time, \"2023-07-15T12:34:56Z\"), or(neq(error, null), and(eq(feedback_key,\\n\"Correctness\"), eq(feedback_score, 0.0))))\\')client.listRuns({  projectName: \"<your_project>\",  filter:\\n\\'and(gt(start_time, \"2023-07-15T12:34:56Z\"), or(neq(error, null), and(eq(feedback_key,\\n\"Correctness\"), eq(feedback_score, 0.0))))\\'})Complex query: List all runs where tags include\\n\"experimental\" or \"beta\" and?latency is greater than 2 secondsPythonTypeScriptclient.list_runs( \\nproject_name=\"<your_project>\",  filter=\\'and(or(has(tags, \"experimental\"), has(tags, \"beta\")),\\ngt(latency, 2))\\')client.listRuns({  projectName: \"<your_project>\",  filter: \\'and(or(has(tags,\\n\"experimental\"), has(tags, \"beta\")), gt(latency, 2))\\'})Search trace trees by full text You can use the\\nsearch() function without?any specific field to do a full text search across all string fields in a run.\\nThis\\nallows you to quickly find traces that match a search term.PythonTypeScriptclient.list_runs( \\nproject_name=\"<your_project>\",  filter=\\'search(\"image classification\")\\')client.listRuns({ \\nprojectName: \"<your_project>\",  filter: \\'search(\"image classification\")\\'})Check for presence of\\nmetadata?If you want to check for the presence of metadata, you can use the eq operator,\\noptionally with an and statement to match by value. This is useful if you want to log more structured\\ninformation\\nabout your runs.PythonTypeScriptto_search = {    \"user_id\": \"\"}# Check for any run with the \"user_id\"\\nmetadata keyclient.list_runs(  project_name=\"default\",  filter=\"eq(metadata_key, \\'user_id\\')\")# Check\\nfor runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3client.list_runs( \\nproject_name=\"default\",  filter=\"and(eq(metadata_key, \\'user_id\\'), eq(metadata_value,', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\22.pdf', 'page': 3}),\n",
       " Document(page_content='\\'4070f233-f61e-44eb-bff1-da3c163895a3\\'))\")// Check for any run with the \"user_id\" metadata\\nkeyclient.listRuns({  projectName: \\'default\\',  filter: `eq(metadata_key, \\'user_id\\')`});// Check for runs\\nwith user_id=4070f233-f61e-44eb-bff1-da3c163895a3client.listRuns({  projectName: \\'default\\',  filter:\\n`and(eq(metadata_key, \\'user_id\\'), eq(metadata_value,\\n\\'4070f233-f61e-44eb-bff1-da3c163895a3\\'))`});Check for environment details in metadata.?A\\ncommon pattern is to add environment\\ninformation to your traces via metadata. If you want to filter for runs containing\\nenvironment metadata, you can use the same pattern as above:PythonTypeScriptclient.list_runs( \\nproject_name=\"default\",  filter=\"and(eq(metadata_key, \\'environment\\'), eq(metadata_value,\\n\\'production\\'))\")client.listRuns({  projectName: \\'default\\',  filter: `and(eq(metadata_key, \\'environment\\'),\\neq(metadata_value, \\'production\\'))`});Check for conversation ID in metadata?Another common way\\nto associate traces\\nin the same conversation is by using a shared conversation ID. If you want to filter\\nruns based on a conversation ID in this way, you can search for that ID in the\\nmetadata.PythonTypeScriptclient.list_runs(  project_name=\"default\",  filter=\"and(eq(metadata_key,\\n\\'conversation_id\\'), eq(metadata_value, \\'a1b2c3d4-e5f6-7890\\'))\")client.listRuns({  projectName:\\n\\'default\\',  filter: `and(eq(metadata_key, \\'conversation_id\\'), eq(metadata_value,\\n\\'a1b2c3d4-e5f6-7890\\'))`});Combine multiple filters?If you want to combine multiple conditions to\\nrefine your search, you can use the and operator along with other\\nfiltering functions. Here\\'s how you can search for runs named \"ChatOpenAI\" that also\\nhave a specific conversation_id in their metadata:PythonTypeScriptclient.list_runs( \\nproject_name=\"default\",  filter=\"and(eq(name, \\'ChatOpenAI\\'), eq(metadata_key, \\'conversation_id\\'),\\neq(metadata_value, \\'69b12c91-b1e2-46ce-91de-794c077e8151\\'))\")client.listRuns({  projectName:\\n\\'default\\',  filter: `and(eq(name, \\'ChatOpenAI\\'), eq(metadata_key, \\'conversation_id\\'),\\neq(metadata_value, \\'69b12c91-b1e2-46ce-91de-794c077e8151\\'))`});Tree Filter?List all runs named\\n\"RetrieveDocs\" whose root run has a \"user_score\" feedback of 1 and any run in the full trace is', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\22.pdf', 'page': 4}),\n",
       " Document(page_content='named \"ExpandQuery\".This type of query is useful if you want to extract a specific run conditional on\\nvarious states or steps being reached within the trace.PythonTypeScriptclient.list_runs(   \\nproject_name=\"<your_project>\",    filter=\\'eq(name, \"RetrieveDocs\")\\',   \\ntrace_filter=\\'and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))\\',    tree_filter=\\'eq(name,\\n\"ExpandQuery\")\\')client.listRuns({  projectName: \"<your_project>\",  filter: \\'eq(name, \"RetrieveDocs\")\\',\\n traceFilter: \\'and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))\\',  treeFilter: \\'eq(name,\\n\"ExpandQuery\")\\'})Advanced: export flattened trace view with child tool usage?The following Python\\nexample demonstrates how to export a flattened view of traces, including information on the tools\\n(from nested runs) used by the agent within each trace.\\nThis can be used to analyze the behavior of your agents across multiple traces.This example\\nqueries all tool runs within a specified number of days and groups them by their parent (root) run ID.\\nIt then fetches the relevant information for each root run, such as the run name, inputs, outputs, and\\ncombines that information with the child run information.To optimize the query, the example:Selects\\nonly the necessary fields when querying tool runs to reduce query time.Fetches root runs in batches\\nwhile processing tool runs concurrently.Pythonfrom collections import defaultdictfrom\\nconcurrent.futures import Future, ThreadPoolExecutorfrom datetime import datetime, timedeltafrom\\nlangsmith import Clientfrom tqdm.auto import tqdmclient = Client()project_name =\\n\"my-project\"num_days = 30# List all tool runstool_runs = client.list_runs(   \\nproject_name=project_name,    start_time=datetime.now() - timedelta(days=num_days),   \\nrun_type=\"tool\",    # We don\\'t need to fetch inputs, outputs, and other values that # may increase the\\nquery time    select=[\"trace_id\", \"name\", \"run_type\"],)data = []futures: list[Future] = []trace_cursor =\\n0trace_batch_size = 50tool_runs_by_parent = defaultdict(lambda: defaultdict(set))# Do not exceed\\nrate limitwith ThreadPoolExecutor(max_workers=2) as executor:    # Group tool runs by parent run\\nID    for run in tqdm(tool_runs):        # Collect all tools invoked within a given trace       \\ntool_runs_by_parent[run.trace_id][\"tools_involved\"].add(run.name)        # maybe send a batch of\\nparent run IDs to the server        # this lets us query for the root runs in batches        # while still', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\22.pdf', 'page': 5}),\n",
       " Document(page_content='processing the tool runs        if len(tool_runs_by_parent) % trace_batch_size == 0:            if\\nthis_batch := list(tool_runs_by_parent.keys())[                trace_cursor : trace_cursor +\\ntrace_batch_size            ]:                trace_cursor += trace_batch_size                futures.append(       \\n            executor.submit(                        client.list_runs,                        project_name=project_name,  \\n                     run_ids=this_batch,                        select=[\"name\", \"inputs\", \"outputs\", \"run_type\"],        \\n           )                )    if this_batch := list(tool_runs_by_parent.keys())[trace_cursor:]:       \\nfutures.append(            executor.submit(                client.list_runs,               \\nproject_name=project_name,                run_ids=this_batch,                select=[\"name\", \"inputs\",\\n\"outputs\", \"run_type\"],            )        )for future in tqdm(futures):    root_runs = future.result()    for\\nroot_run in root_runs:        root_data = tool_runs_by_parent[root_run.id]        data.append(            {   \\n            \"run_id\": root_run.id,                \"run_name\": root_run.name,                \"run_type\":\\nroot_run.run_type,                \"inputs\": root_run.inputs,                \"outputs\": root_run.outputs,               \\n\"tools_involved\": list(root_data[\"tools_involved\"]),            }        )# (Optional): Convert to a pandas\\nDataFrameimport pandas as pddf = pd.DataFrame(data)df.head()Advanced: export retriever IO for\\ntraces with feedback?This query is useful if you want to fine-tune embeddings or diagnose\\nend-to-end system performance issues based on retriever behavior.\\nThe following Python example demonstrates how to export retriever inputs and outputs within traces\\nthat have a specific feedback score.Pythonfrom collections import defaultdictfrom concurrent.futures\\nimport Future, ThreadPoolExecutorfrom datetime import datetime, timedeltaimport pandas as\\npdfrom langsmith import Clientfrom tqdm.auto import tqdmclient = Client()project_name =\\n\"your-project-name\"num_days = 1# List all tool runsretriever_runs = client.list_runs(   \\nproject_name=project_name,    start_time=datetime.now() - timedelta(days=num_days),   \\nrun_type=\"retriever\",    # This time we do want to fetch the inputs and outputs, since they    # may\\nbe adjusted by query expansion steps.    select=[\"trace_id\", \"name\", \"run_type\", \"inputs\", \"outputs\"], \\n  trace_filter=\\'eq(feedback_key, \"user_score\")\\',)data = []futures: list[Future] = []trace_cursor =\\n0trace_batch_size = 50retriever_runs_by_parent = defaultdict(lambda: defaultdict(list))# Do not', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\22.pdf', 'page': 6}),\n",
       " Document(page_content='exceed rate limitwith ThreadPoolExecutor(max_workers=2) as executor:    # Group retriever runs by\\nparent run ID    for run in tqdm(retriever_runs):        # Collect all retriever calls invoked within a given\\ntrace        for k, v in run.inputs.items():           \\nretriever_runs_by_parent[run.trace_id][f\"retriever.inputs.{k}\"].append(v)        for k, v in (run.outputs\\nor {}).items():            # Extend the docs           \\nretriever_runs_by_parent[run.trace_id][f\"retriever.outputs.{k}\"].extend(v)        # maybe send a batch\\nof parent run IDs to the server        # this lets us query for the root runs in batches        # while still\\nprocessing the retriever runs        if len(retriever_runs_by_parent) % trace_batch_size == 0:            if\\nthis_batch := list(retriever_runs_by_parent.keys())[                trace_cursor : trace_cursor +\\ntrace_batch_size            ]:                trace_cursor += trace_batch_size                futures.append(       \\n            executor.submit(                        client.list_runs,                        project_name=project_name,  \\n                     run_ids=this_batch,                        select=[                            \"name\",                           \\n\"inputs\",                            \"outputs\",                            \"run_type\",                            \"feedback_stats\", \\n                      ],                    )                )    if this_batch :=\\nlist(retriever_runs_by_parent.keys())[trace_cursor:]:        futures.append(            executor.submit(      \\n         client.list_runs,                project_name=project_name,                run_ids=this_batch,               \\nselect=[\"name\", \"inputs\", \"outputs\", \"run_type\"],            )        )for future in tqdm(futures):    root_runs\\n= future.result()    for root_run in root_runs:        root_data = retriever_runs_by_parent[root_run.id]    \\n   feedback = {            f\"feedback.{k}\": v.get(\"avg\")            for k, v in (root_run.feedback_stats or\\n{}).items()        }        inputs = {f\"inputs.{k}\": v for k, v in root_run.inputs.items()}        outputs =\\n{f\"outputs.{k}\": v for k, v in (root_run.outputs or {}).items()}        data.append(            {               \\n\"run_id\": root_run.id,                \"run_name\": root_run.name,                **inputs,                **outputs,    \\n           **feedback,                **root_data,            }        )# (Optional): Convert to a pandas\\nDataFrameimport pandas as pddf = pd.DataFrame(data)df.head()Was this page\\nhelpful?PreviousPrevent logging of inputs and outputs in tracesNextShare or unshare a trace\\npubliclyUse filter argumentsList all runs in a projectList LLM and Chat runs in the last 24 hoursList', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\22.pdf', 'page': 7}),\n",
       " Document(page_content='root runs in a projectList runs without errorsList runs by run IDUse filter query languageList all runs\\ncalled \"extractor\" whose root of the trace was assigned feedback \"user_score\" score of 1List runs\\nwith \"star_rating\" key whose score is greater than 4List runs that took longer than 5 seconds to\\ncompleteList all runs where total_tokens is greater than 5000List all runs that have \"error\" not equal\\nto nullList all runs where start_time is greater than a specific timestampList all runs that contain the\\nstring \"substring\"List all runs that are tagged with the git hash \"2aa1cf4\"List all \"chain\" type runs that\\ntook more than 10 seconds andList all runs that started after a specific timestamp and\\neitherComplex query: List all runs where tags include \"experimental\" or \"beta\" andSearch trace trees\\nby full text You can use the search() function withoutCheck for presence of metadataCheck for\\nenvironment details in metadata.Check for conversation ID in metadataCombine multiple filtersTree\\nFilterAdvanced: export flattened trace view with child tool usageAdvanced: export retriever IO for\\ntraces with feedbackCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\22.pdf', 'page': 8}),\n",
       " Document(page_content=\"Share or unshare a trace publicly | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingAnnotate code for tracingToggle tracing on and offLog traces to specific\\nprojectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed\\ntracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever\\ntracesLog custom LLM tracesPrevent logging of inputs and outputs in tracesExport tracesShare or\\nunshare a trace publiclyTrace generator functionsTrace with LangChain (Python and JS/TS)Trace\\nwith Instructor (Python only)DatasetsEvaluationHuman feedbackMonitoring and\\nautomationsPromptsConceptsReferencePricingSelf-hostingHow-to guidesTracingShare or unshare\\na trace publiclyShare or unshare a trace publiclycautionSharing a trace publicly will make it\\naccessible to anyone with the link. Make sure you're not sharing sensitive information.\\nThis feature is only available in the cloud-hosted version of LangSmith.To share a trace publicly,\\nsimply click on the Share button in the upper right hand side of any trace view.\\nThis will open a dialog where you can copy the link to the trace.Shared traces will be accessible to\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\23.pdf', 'page': 0}),\n",
       " Document(page_content='anyone with the link, even if they don\\'t have a LangSmith account. They will be able to view the\\ntrace, but not edit it.To \"unshare\" a trace, eitherClick on Unshare by click on Public in the upper right\\nhand corner of any publicly shared trace, then Unshare in the dialog.\\nNavigate to your organization\\'s list of publicly shared traces, either by clicking on Settings -> Shared\\nURLs or this link, then click on Unshare next to the trace you want to unshare.\\nWas this page helpful?PreviousExport tracesNextTrace generator\\nfunctionsCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\23.pdf', 'page': 1}),\n",
       " Document(page_content='Trace generator functions | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingAnnotate code for tracingToggle tracing on and offLog traces to specific\\nprojectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed\\ntracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever\\ntracesLog custom LLM tracesPrevent logging of inputs and outputs in tracesExport tracesShare or\\nunshare a trace publiclyTrace generator functionsTrace with LangChain (Python and JS/TS)Trace\\nwith Instructor (Python only)DatasetsEvaluationHuman feedbackMonitoring and\\nautomationsPromptsConceptsReferencePricingSelf-hostingHow-to guidesTracingTrace generator\\nfunctionsOn this pageTrace generator functionsIn most LLM applications, you will want to stream\\noutputs to minimize the time to the first token seen by the user.LangSmith\\'s tracing functionality\\nnatively supports streamed outputs via generator functions. Below is an\\nexample.PythonTypeScriptfrom langsmith import traceable@traceabledef my_generator():    for\\nchunk in [\"Hello\", \"World\", \"!\"]:        yield chunk# Stream to the userfor output in my_generator():   ', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\24.pdf', 'page': 0}),\n",
       " Document(page_content='print(output)# It also works with async functionsimport asyncio@traceableasync def\\nmy_async_generator():    hunk in [\"Hello\", \"World\", \"!\"]:        yield chunk# Stream to the userasync\\ndef main():    async for output in my_async_generator():        print(output)asyncio.run(main())import {\\ntraceable } from \"langsmith/traceable\";const myGenerator = traceable(function* () {    for (const\\nchunk of [\"Hello\", \"World\", \"!\"]) {        yield chunk;    }});for (const output of myGenerator()) {   \\nconsole.log(output);}Aggregate Results?By default, the outputs of the traced function are\\naggregated into a single array in LangSmith.\\nIf you want to customize how it is stored (for instance, concatenating the outputs into a single\\nstring), you can use the aggregate option (reduce_fn in python).\\nThis is especially useful for aggregating streamed LLM outputs.noteAggregating outputs only\\nimpacts the traced representation of the outputs. It doesn not alter the values returned by your\\nfunction.PythonTypeScriptfrom langsmith import traceabledef concatenate_strings(outputs: list):   \\nreturn \"\".join(outputs)@traceable(reduce_fn=concatenate_strings)def my_generator():    for chunk in\\n[\"Hello\", \"World\", \"!\"]:        yield chunk# Stream to the userfor output in my_generator():   \\nprint(output)# It also works with async functionsimport\\nasyncio@traceable(reduce_fn=concatenate_strings)async def my_async_generator():    for chunk in\\n[\"Hello\", \"World\", \"!\"]:        yield chunk# Stream to the userasync def main():    async for output in\\nmy_async_generator():        print(output)asyncio.run(main())import { traceable } from\\n\"langsmith/traceable\";const concatenateStrings = (outputs: string[]) => outputs.join(\"\");const\\nmyGenerator = traceable(function* () {    for (const chunk of [\"Hello\", \"World\", \"!\"]) {        yield chunk; \\n  }}, { aggregator: concatenateStrings });for (const output of await myGenerator()) {   \\nconsole.log(output);}Was this page helpful?PreviousShare or unshare a trace publiclyNextTrace\\nwith LangChain (Python and JS/TS)Aggregate ResultsCommunityDiscordTwitterGitHubDocs\\nCodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS\\nDocsCopyright © 2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\24.pdf', 'page': 1}),\n",
       " Document(page_content='', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\24.pdf', 'page': 2}),\n",
       " Document(page_content='Trace with LangChain (Python and JS/TS) | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingAnnotate code for tracingToggle tracing on and offLog traces to specific\\nprojectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed\\ntracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever\\ntracesLog custom LLM tracesPrevent logging of inputs and outputs in tracesExport tracesShare or\\nunshare a trace publiclyTrace generator functionsTrace with LangChain (Python and JS/TS)Trace\\nwith Instructor (Python only)DatasetsEvaluationHuman feedbackMonitoring and\\nautomationsPromptsConceptsReferencePricingSelf-hostingHow-to guidesTracingTrace with\\nLangChain (Python and JS/TS)On this pageTrace with LangChain (Python and JS/TS)LangSmith\\nintegrates seamlessly with LangChain (Python and JS), the popular open-source framework for\\nbuilding LLM applications.Installation?Install the core library and the OpenAI integration for Python\\nand JS (we use the OpenAI integration for the code snippets below).For a full list of packages\\navailable, see the LangChain Python docs and LangChain JS docs.pipyarnnpmpnpmpip install', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\25.pdf', 'page': 0}),\n",
       " Document(page_content='langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install\\n@langchain/openai @langchain/corepnpm add @langchain/openai @langchain/coreQuick start?1.\\nConfigure your environment?export LANGCHAIN_TRACING_V2=trueexport\\nLANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, so you will\\nneedexport OPENAI_API_KEY=<your-openai-api-key>2. Log a trace?No extra code is needed to\\nlog a trace to LangSmith. Just run your LangChain code as you normally\\nwould.PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts\\nimport ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt =\\nChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to\\nthe user\\'s request only based on the given context.\"),    (\"user\", \"Question: {question}\\\\nContext:\\n{context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain =\\nprompt | model | output_parserquestion = \"Can you summarize this morning\\'s meetings?\"context =\\n\"During this morning\\'s meeting, we solved all world conflict.\"chain.invoke({\"question\": question,\\n\"context\": context})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate }\\nfrom \"@langchain/core/prompts\";import { StringOutputParser } from\\n\"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\",\\n\"You are a helpful assistant. Please respond to the user\\'s request only based on the given\\ncontext.\"],  [\"user\", \"Question: {question}\\\\nContext: {context}\"],]);const model = new ChatOpenAI({\\nmodelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain =\\nprompt.pipe(model).pipe(outputParser);const question = \"Can you summarize this morning\\'s\\nmeetings?\"const context = \"During this morning\\'s meeting, we solved all world conflict.\"await\\nchain.invoke({ question: question, context: context });3. View your trace?By default, the trace will be\\nlogged to the project with the name default. An example of a trace logged using the above code is\\nmade public and can be viewed here.Trace selectively?The previous section showed how to trace\\nall invocations of a LangChain runnables within your applications by setting a single environment\\nvariable. While this is a convenient way to get started, you may want to trace only specific', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\25.pdf', 'page': 1}),\n",
       " Document(page_content='invocations or parts of your application.There are two ways to do this in Python: by manually\\npassing in a LangChainTracer (reference docs) instance as a callback, or by using the\\ntracing_v2_enabled context manager (reference docs).In JS/TS, you can pass a LangChainTracer\\n(reference docs) instance as a callback.PythonTypeScript# You can configure a LangChainTracer\\ninstance to trace a specific invocation.from langchain.callbacks.tracers import\\nLangChainTracertracer = LangChainTracer()chain.invoke({\"question\": \"Am I using a callback?\",\\n\"context\": \"I\\'m using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a\\ncontext manager for tracing a specific block of code.from langchain_core.tracers.context import\\ntracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({\"question\": \"Am I using a context\\nmanager?\", \"context\": \"I\\'m using a context manager\"})# This will NOT be traced (assuming\\nLANGCHAIN_TRACING_V2 is not set)chain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I\\'m\\nnot being traced\"})// You can configure a LangChainTracer instance to trace a specific\\ninvocation.import { LangChainTracer } from \"langchain/callbacks\";const tracer = new\\nLangChainTracer();await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I\\'m using\\na callback\"  },  { callbacks: [tracer] });Log to a specific project?Statically?As mentioned in the tracing\\nconceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the\\ntracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to\\nconfigure a custom project name for an entire application run. This should be done before executing\\nyour application.export LANGCHAIN_PROJECT=my-projectDynamically?This largely builds off of\\nthe previous section and allows you to set the project name for a specific LangChainTracer instance\\nor as parameters to the tracing_v2_enabled context manager in Python.PythonTypeScript# You can\\nset the project name for a specific tracer instance:from langchain.callbacks.tracers import\\nLangChainTracertracer = LangChainTracer(project_name=\"My Project\")chain.invoke({\"question\":\\n\"Am I using a callback?\", \"context\": \"I\\'m using a callback\"}, config={\"callbacks\": [tracer]})# You can\\nset the project name using the project_name parameter.from langchain_core.tracers.context import\\ntracing_v2_enabledwith tracing_v2_enabled(project_name=\"My Project\"):   ', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\25.pdf', 'page': 2}),\n",
       " Document(page_content='chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I\\'m using a context\\nmanager\"})// You can set the project name for a specific tracer instance:import { LangChainTracer }\\nfrom \"langchain/callbacks\";const tracer = new LangChainTracer({ projectName: \"My Project\" });await\\nchain.invoke(  {    question: \"Am I using a callback?\",    context: \"I\\'m using a callback\"  },  { callbacks:\\n[tracer] });Add metadata and tags to traces?LangSmith supports sending arbitrary metadata and\\ntags along with traces.\\nThis is useful for associating additional information with a trace, such as the environment in which it\\nwas executed, or the user who initiated it.\\nFor information on how to query traces and runs by metadata and tags, see this\\nguidePythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts\\nimport ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt =\\nChatPromptTemplate.from_messages([  (\"system\", \"You are a helpful AI.\"),  (\"user\",\\n\"{input}\")])chat_model = ChatOpenAI()output_parser = StrOutputParser()# Tags and metadata can\\nbe configured with RunnableConfigchain = (prompt | chat_model |\\noutput_parser).with_config({\"tags\": [\"top-level-tag\"], \"metadata\": {\"top-level-key\": \"top-level-value\"}})#\\nTags and metadata can also be passed at runtimechain.invoke({\"input\": \"What is the meaning of\\nlife?\"}, {\"tags\": [\"shared-tags\"], \"metadata\": {\"shared-key\": \"shared-value\"}})import { ChatOpenAI }\\nfrom \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import {\\nStringOutputParser } from \"@langchain/core/output_parsers\";const prompt =\\nChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful AI.\"],  [\"user\", \"{input}\"]])const\\nmodel = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new\\nStringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain =\\n(prompt.pipe(model).pipe(outputParser)).withConfig({\"tags\": [\"top-level-tag\"], \"metadata\":\\n{\"top-level-key\": \"top-level-value\"}});// Tags and metadata can also be passed at runtimeawait\\nchain.invoke({input: \"What is the meaning of life?\"}, {tags: [\"shared-tags\"], metadata: {\"shared-key\":\\n\"shared-value\"}})Customize run name?When you create a run, you can specify a name for the run.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\25.pdf', 'page': 3}),\n",
       " Document(page_content='This name is used to identify the run in LangSmith and can be used to filter and group runs. The\\nname is also used as the title of the run in the LangSmith UI.noteThis feature is not currently\\nsupported directly for LLM objects.PythonTypeScript# When tracing within LangChain, run names\\ndefault to the class name of the traced object (e.g., \\'ChatOpenAI\\').configured_chain =\\nchain.with_config({\"run_name\": \"MyCustomChain\"})configured_chain.invoke({\"query\": \"What is the\\nmeaning of life?\"})// When tracing within LangChain, run names default to the class name of the\\ntraced object (e.g., \\'ChatOpenAI\\').// (Note: this is not currently supported directly on LLM\\nobjects.)...const configuredChain = chain.withConfig({ runName: \"MyCustomChain\" });await\\nconfiguredChain.invoke({query: \"What is the meaning of life?\"});Access run (span) ID for LangChain\\ninvocations?When you invoke a LangChain object, you can access the run ID of the invocation. This\\nrun ID can be used to query the run in LangSmith.In Python, you can use the collect_runs context\\nmanager to access the run ID.In JS/TS, you can use a RunCollectorCallbackHandler instance to\\naccess the run ID.PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom\\nlangchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import\\nStrOutputParserfrom langchain_core.tracers.context import collect_runsprompt =\\nChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to\\nthe user\\'s request only based on the given context.\"),    (\"user\", \"Question: {question}Context:\\n{context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain =\\nprompt | model | output_parserquestion = \"Can you summarize this morning\\'s meetings?\"context =\\n\"During this morning\\'s meeting, we solved all world conflict.\"with collect_runs() as cb:  result =\\nchain.invoke({\"question\": question, \"context\": context})  # Get the root run id  run_id =\\ncb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from \"npm:@langchain/openai\";import {\\nChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from\\n\"@langchain/core/output_parsers\";import { RunCollectorCallbackHandler } from\\n\"@langchain/core/tracers/run_collector\";const prompt = ChatPromptTemplate.fromMessages([ \\n[\"system\", \"You are a helpful assistant. Please respond to the user\\'s request only based on the', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\25.pdf', 'page': 4}),\n",
       " Document(page_content='given context.\"],  [\"user\", \"Question: {question}Context: {context}\"],]);const model = new\\nChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const\\nchain = prompt.pipe(model).pipe(outputParser);const runCollector = new\\nRunCollectorCallbackHandler();const question = \"Can you summarize this morning\\'s\\nmeetings?\"const context = \"During this morning\\'s meeting, we solved all world conflict.\"await\\nchain.invoke(    { question: question, context: context },    {        callbacks: [runCollector],    });const\\nrunId = runCollector.tracedRuns[0].id;console.log(runId);Was this page helpful?PreviousTrace\\ngenerator functionsNextTrace with Instructor (Python only)InstallationQuick start1. Configure your\\nenvironment2. Log a trace3. View your traceTrace selectivelyLog to a specific\\nprojectStaticallyDynamicallyAdd metadata and tags to tracesCustomize run nameAccess run (span)\\nID for LangChain invocationsCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\25.pdf', 'page': 5}),\n",
       " Document(page_content='Trace with Instructor (Python only) | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingAnnotate code for tracingToggle tracing on and offLog traces to specific\\nprojectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed\\ntracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever\\ntracesLog custom LLM tracesPrevent logging of inputs and outputs in tracesExport tracesShare or\\nunshare a trace publiclyTrace generator functionsTrace with LangChain (Python and JS/TS)Trace\\nwith Instructor (Python only)DatasetsEvaluationHuman feedbackMonitoring and\\nautomationsPromptsConceptsReferencePricingSelf-hostingHow-to guidesTracingTrace with\\nInstructor (Python only)Trace with Instructor (Python only)We provide a convenient integration with\\nInstructor, a popular open-source library for generating structured outputs with LLMs.In order to use,\\nyou first need to set your LangSmith API key.export LANGCHAIN_API_KEY=<your-api-key>Next,\\nyou will need to install the LangSmith SDK:pip install -U langsmithWrap your OpenAI client with\\nlangsmith.wrappers.wrap_openaifrom openai import OpenAIfrom langsmith import wrappersclient =', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\26.pdf', 'page': 0}),\n",
       " Document(page_content='wrappers.wrap_openai(OpenAI())After this, you can patch the wrapped OpenAI client using\\ninstructor:import instructorclient = instructor.patch(client)Now, you can use instructor as you\\nnormally would, but now everything is logged to LangSmith!from pydantic import BaseModelclass\\nUserDetail(BaseModel):    name: str    age: intuser = client.chat.completions.create(   \\nmodel=\"gpt-3.5-turbo\",    response_model=UserDetail,    messages=[        {\"role\": \"user\", \"content\":\\n\"Extract Jason is 25 years old\"},    ])Oftentimes, you use instructor inside of other functions.\\nYou can get nested traces by using this wrapped client and decorating those functions with\\n@traceable.\\nPlease see this guide for more information on how to annotate your code for tracing with the\\n@traceable decorator.# You can customize the run name with the `name` keyword\\nargument@traceable(name=\"Extract User Details\")def my_function(text: str) -> UserDetail:    return\\nclient.chat.completions.create(        model=\"gpt-3.5-turbo\",        response_model=UserDetail,       \\nmessages=[            {\"role\": \"user\", \"content\": f\"Extract {text}\"},        ]    )my_function(\"Jason is 25\\nyears old\")Was this page helpful?PreviousTrace with LangChain (Python and JS/TS)NextManage\\ndatasets in the applicationCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\26.pdf', 'page': 1}),\n",
       " Document(page_content='Manage datasets in the application | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsManage datasets in the applicationManage datasets\\nprogrammaticallyVersion datasetsShare or unshare a dataset publiclyEvaluationHuman\\nfeedbackMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesDatasetsManage datasets in the applicationOn this pageManage datasets in the\\napplicationRecommended ReadingBefore diving into this content, it might be helpful to read the\\nfollowing:Concepts guide on evaluation and datasetsThe easiest way to interact with datasets is\\ndirectly in the LangSmith app. Here, you can create and edit datasets and example.Create a new\\ndataset and add examples manually?To get started, you can create a new datasets by heading to\\nthe \"Datasets and Testing\" section of the application and clicking on \"+ New Dataset\".Then, enter\\nthe relevant dataset details, including a name, optional description, and dataset type. Please see the\\nconcepts for more information on dataset types. For most flexibility, the key-value dataset type is\\nrecommended.You can then add examples to the dataset by clicking on \"Add Example\". Here, you', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\27.pdf', 'page': 0}),\n",
       " Document(page_content='can enter the input and output as JSON objects.Add inputs and outputs from traces to datasets?We\\ntypically construct datasets over time by collecting representative examples from debugging or other\\nruns. To do this, we first filter the traces to find the ones we want to add to the dataset. Then we add\\nthe inputs and outputs from these traces to the dataset.You can do this from any \\'run\\' details page\\nby clicking the \\'Add to Dataset\\' button in the top right-hand corner.tipAn extremely powerful\\ntechnique to build datasets is to drill-down into the most interesting traces, such as traces that were\\ntagged with poor user feedback, and add them to a dataset.\\nFor tips on how to filter traces, see the [filtering traces] guide.automationsYou can use automations\\nto automatically add traces to a dataset based on certain conditions. For example, you could add all\\ntraces that have a certain tag to a dataset.From there, we select the dataset to organize it in and\\nupdate the ground truth output values if necessary.Upload a CSV file to create a dataset?The\\neasiest way to create a dataset from your own data is by clicking the \\'upload a CSV dataset\\' button\\non the home page or in the top right-hand corner of the \\'Datasets & Testing\\' page.Select a name\\nand description for the dataset, and then confirm that the inferred input and output columns are\\ncorrect.Export a dataset?You can export your LangSmith dataset to CSV or OpenAI evals format\\ndirectly from the web application.To do so, click \"Export Dataset\" from the homepage.\\nTo do so, select a dataset, click on \"Examples\", and then click the \"Export Dataset\" button at the top\\nof the examples table.This will open a modal where you can select the format you want to export\\nto.Was this page helpful?PreviousTrace with Instructor (Python only)NextManage datasets\\nprogrammaticallyCreate a new dataset and add examples manuallyAdd inputs and outputs from\\ntraces to datasetsUpload a CSV file to create a datasetExport a\\ndatasetCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\27.pdf', 'page': 1}),\n",
       " Document(page_content='', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\27.pdf', 'page': 2}),\n",
       " Document(page_content='Manage datasets programmatically | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsManage datasets in the applicationManage datasets\\nprogrammaticallyVersion datasetsShare or unshare a dataset publiclyEvaluationHuman\\nfeedbackMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesDatasetsManage datasets programmaticallyOn this pageManage datasets\\nprogrammaticallyYou can use the Python and TypeScript SDK to manage datasets\\nprogrammatically. This includes creating, updating, and deleting datasets, as well as adding\\nexamples to them.Create a dataset from list of values?The most flexible way to make a dataset\\nusing the client is by creating examples from a list of inputs and optional outputs. Below is an\\nexample.Note that you can add arbitrary metadata to each example, such as a note or a source.\\nThe metadata is stored as a dictionary.PythonTypeScriptfrom langsmith import\\nClientexample_inputs = [  (\"What is the largest mammal?\", \"The blue whale\"),  (\"What do mammals\\nand birds have in common?\", \"They are both warm-blooded\"),  (\"What are reptiles known for?\",', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\28.pdf', 'page': 0}),\n",
       " Document(page_content='\"Having scales\"),  (\"What\\'s the main characteristic of amphibians?\", \"They live both in water and on\\nland\"),]client = Client()dataset_name = \"Elementary Animal Questions\"# Storing inputs in a dataset\\nlets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(   \\ndataset_name=dataset_name, description=\"Questions and answers about animal\\nphylogenetics.\",)for input_prompt, output_answer in example_inputs:    client.create_example(       \\ninputs={\"question\": input_prompt},        outputs={\"answer\": output_answer},       \\nmetadata={\"source\": \"Wikipedia\"},        dataset_id=dataset.id,    )import { Client } from\\n\"langsmith\";const client = new Client();const exampleInputs: [string, string][] = [  [\"What is the largest\\nmammal?\", \"The blue whale\"],  [\"What do mammals and birds have in common?\", \"They are both\\nwarm-blooded\"],  [\"What are reptiles known for?\", \"Having scales\"],  [\"What\\'s the main characteristic\\nof amphibians?\", \"They live both in water and on land\"],];const datasetName = \"Elementary Animal\\nQuestions\";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of\\nexamples.const dataset = await client.createDataset(datasetName, {  description: \"Questions and\\nanswers about animal phylogenetics\",});for (const [inputPrompt, outputAnswer] of exampleInputs) { \\nawait client.createExample(    { question: inputPrompt },    { answer: outputAnswer },    {     \\ndatasetId: dataset.id,      metadata: { source: \"Wikipedia\" },    }  );}Create a dataset from traces?To\\ncreate datasets from the runs (spans) of your traces, you can use the same approach.\\nFor many more examples of how to fetch and filter runs, see the export traces guide.\\nBelow is an example:PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name =\\n\"Example Dataset\"# Filter runs to add to the datasetruns = client.list_runs(   \\nproject_name=\"my_project\",    is_root=True,    error=False,)dataset =\\nclient.create_dataset(dataset_name, description=\"An example dataset\")for run in runs:   \\nclient.create_example(        inputs=run.inputs,        outputs=run.outputs,        dataset_id=dataset.id,  \\n )import { Client, Run } from \"langsmith\";const client = new Client();const datasetName = \"Example\\nDataset\";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of\\nclient.listRuns({  projectName: \"my_project\",  isRoot: 1,  error: false,})) {  runs.push(run);}const', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\28.pdf', 'page': 1}),\n",
       " Document(page_content='dataset = await client.createDataset(datasetName, {  description: \"An example dataset\",  dataType:\\n\"kv\",});for (const run of runs) {  await client.createExample(run.inputs, run.outputs ?? {}, {   \\ndatasetId: dataset.id,  });}Create a dataset from a CSV file?In this section, we will demonstrate how\\nyou can create a dataset by uploading a CSV file.First, ensure your CSV file is properly formatted\\nwith columns that represent your input and output keys. These keys will be utilized to map your data\\nproperly during the upload. You can specify an optional name and description for your dataset.\\nOtherwise, the file name will be used as the dataset name and no description will be\\nprovided.PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file =\\n\\'path/to/your/csvfile.csv\\'input_keys = [\\'column1\\', \\'column2\\'] # replace with your input column\\nnamesoutput_keys = [\\'output1\\', \\'output2\\'] # replace with your output column namesdataset =\\nclient.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,   \\nname=\"My CSV Dataset\",    description=\"Dataset created from a CSV file\"    data_type=\"kv\")import {\\nClient } from \"langsmith\";const client = new Client();const csvFile = \\'path/to/your/csvfile.csv\\';const\\ninputKeys = [\\'column1\\', \\'column2\\']; // replace with your input column namesconst outputKeys =\\n[\\'output1\\', \\'output2\\']; // replace with your output column namesconst dataset = await\\nclient.uploadCsv({    csvFile: csvFile,    fileName: \"My CSV Dataset\",    inputKeys: inputKeys,   \\noutputKeys: outputKeys,    description: \"Dataset created from a CSV file\",    dataType: \"kv\"});Create\\na dataset from pandas DataFrame (Python only)?The python client offers an additional convenience\\nmethod to upload a dataset from a pandas dataframe.from langsmith import Clientimport osimport\\npandas as pdclient = Client()df = pd.read_parquet(\\'path/to/your/myfile.parquet\\')input_keys =\\n[\\'column1\\', \\'column2\\'] # replace with your input column namesoutput_keys = [\\'output1\\', \\'output2\\'] #\\nreplace with your output column namesdataset = client.upload_dataframe(    df=df,   \\ninput_keys=input_keys,    output_keys=output_keys,    name=\"My Parquet Dataset\",   \\ndescription=\"Dataset created from a parquet file\",    data_type=\"kv\" # The default)Fetch\\ndatasets?You can programmatically fetch datasets from LangSmith using the\\nlist_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\28.pdf', 'page': 2}),\n",
       " Document(page_content='calls.PrerequisitesInitialize the client before running the below code snippets.PythonTypeScriptfrom\\nlangsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new\\nClient();Query all datasets?PythonTypeScriptdatasets = client.list_datasets()const datasets = await\\nclient.listDatasets();List datasets by name?If you want to search by the exact name, you can do the\\nfollowing:PythonTypeScriptdatasets = client.list_datasets(dataset_name=\"My Test Dataset 1\")const\\ndatasets = await client.listDatasets({datasetName: \"My Test Dataset 1\"});If you want to do a\\ncase-invariant substring search, try the following:PythonTypeScriptdatasets =\\nclient.list_datasets(dataset_name_contains=\"some substring\")const datasets = await\\nclient.listDatasets({datasetNameContains: \"some substring\"});List datasets by type?You can filter\\ndatasets by type. Below is an example querying for chat datasets.PythonTypeScriptdatasets =\\nclient.list_datasets(data_type=\"chat\")const datasets = await client.listDatasets({dataType:\\n\"chat\"});Fetch examples?You can programmatically fetch examples from LangSmith using the\\nlist_examples/listExamples method in the Python and TypeScript SDKs. Below are some common\\ncalls.PrerequisitesInitialize the client before running the below code snippets.PythonTypeScriptfrom\\nlangsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new\\nClient();List all examples for a dataset?You can filter by dataset ID:PythonTypeScriptexamples =\\nclient.list_examples(dataset_id=\"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\")const examples =\\nawait client.listExamples({datasetId: \"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\"});Or you can filter\\nby dataset name (this must exactly match the dataset name you want to\\nquery)PythonTypeScriptexamples = client.list_examples(dataset_name=\"My Test Dataset\")const\\nexamples = await client.listExamples({datasetName: \"My test Dataset\"});List examples by id?You\\ncan also list multiple examples all by ID.PythonTypeScriptexample_ids = [\\n\\'734fc6a0-c187-4266-9721-90b7a025751a\\', \\'d6b4c1b9-6160-4d63-9b61-b034c585074f\\',\\n\\'4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\\',]examples =\\nclient.list_examples(example_ids=example_ids)const exampleIds = [ \\n\"734fc6a0-c187-4266-9721-90b7a025751a\",  \"d6b4c1b9-6160-4d63-9b61-b034c585074f\", ', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\28.pdf', 'page': 3}),\n",
       " Document(page_content='\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await\\nclient.listExamples({exampleIds: exampleIds});List examples by metadata?You can also filter\\nexamples by metadata. Below is an example querying for examples with a specific metadata\\nkey-value pair.\\nUnder the hood, we check to see if the example\\'s metadata contains the key-value pair(s) you\\nspecify.For example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo:\\nbar} and {baz: qux} would match, as would {foo: bar, baz: qux}.PythonTypeScriptexamples =\\nclient.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await\\nclient.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});Was this page\\nhelpful?PreviousManage datasets in the applicationNextVersion datasetsCreate a dataset from list\\nof valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas\\nDataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by\\ntypeFetch examplesList all examples for a datasetList examples by idList examples by\\nmetadataCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\28.pdf', 'page': 4}),\n",
       " Document(page_content='Version datasets | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsManage datasets in the applicationManage datasets\\nprogrammaticallyVersion datasetsShare or unshare a dataset publiclyEvaluationHuman\\nfeedbackMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesDatasetsVersion datasetsOn this pageVersion datasetsIn LangSmith, datasets are versioned.\\nThis means that every time you add, update, or delete examples in your dataset, a new version of\\nthe dataset is created.Create a new version of a dataset?Any time you add, update, or delete\\nexamples in your dataset, a new version of your dataset is created. This allows you to track changes\\nto your dataset over time and to understand how your dataset has evolved.By default, the version is\\ndefined by the timestamp of the change. When you click on a particular version of a dataset (by\\ntimestamp) in the \"Examples\" tab, you can see the state of the dataset at that point in time.Note that\\nexamples are read-only when viewing a past version of the dataset. You will also see the operations\\nthat were between this version of the dataset and the \"latest\" version of the dataset. Also, by default', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\29.pdf', 'page': 0}),\n",
       " Document(page_content='the latest version of the dataset is shown in the \"Examples\" tab and experiments from all versions\\nare shown in the \"Tests\" tab.In the \"Tests\" tab, you can see the results of tests run on the dataset at\\ndifferent versions.Tag a version?You can also tag versions of your dataset to give them a more\\nhuman-readable name. This can be useful for marking important milestones in your dataset\\'s\\nhistory.For example, you might tag a version of your dataset as \"prod\" and use it to run tests against\\nyour LLM pipeline.Tagging can be done in the UI by clicking on \"+ Tag this version\" in the\\n\"Examples\" tab.You can also tag versions of your dataset using the SDK. Here\\'s an example of how\\nto tag a version of a dataset using the python SDK:from langsmith import Clientfromt datetime\\nimport datetimeclient = Client()initial_time = datetime(2024, 1, 1, 0, 0, 0) # The timestamp of the\\nversion you want to tag# You can tag a specific dataset version with a semantic name, like\\n\"prod\"client.update_dataset_tag(    dataset_name=toxic_dataset_name, as_of=initial_time,\\ntag=\"prod\")To run an evaluation on a particular tagged version of a dataset, you can follow this\\nguide.Was this page helpful?PreviousManage datasets programmaticallyNextShare or unshare a\\ndataset publiclyCreate a new version of a datasetTag a\\nversionCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\29.pdf', 'page': 1}),\n",
       " Document(page_content='Evaluate your LLM application | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsAdd observability to\\nyour LLM applicationEvaluate your LLM applicationOptimize a classifierHow-to\\nguidesConceptsReferencePricingSelf-hostingTutorialsEvaluate your LLM applicationOn this\\npageEvaluate your LLM applicationIt can be hard to measure the performance of your application\\nwith respect to criteria important you or your users.\\nHowever, doing so is crucial, especially as you iterate on your application.\\nIn this guide we will go over how to test and evaluate your application.\\nThis allows you to measure how well your application is performing over a fixed set of data.\\nBeing able to get this insight quickly and reliably will allow you to iterate with confidence.At a high\\nlevel, in this tutorial we will go over how to:Create an initial golden dataset to measure\\nperformanceDefine metrics to use to measure performanceRun evaluations on a few different\\nprompts or modelsCompare results manuallyTrack results over timeSet up automated testing to run\\nin CI/CDFor more information on the evaluation workflows LangSmith supports, check out the', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\3.pdf', 'page': 0}),\n",
       " Document(page_content=\"how-to guides.Lots to cover, let's dive in!Create a dataset?The first step when getting ready to test\\nand evaluate your application is to define the datapoints you want to evaluate.\\nThere are a few aspects to consider here:What should the schema of each datapoint be?How many\\ndatapoints should I gather?How should I gather those datapoints?Schema: Each datapoint should\\nconsist of, at the very least, the inputs to the application.\\nIf you are able, it is also very helpful to define the expected outputs - these represent what you\\nwould expect a properly functioning application to output.\\nOften times you cannot define the perfect output - that's okay! Evaluation is an iterative process.\\nSometimes you may also want to define more information for each example - like the expected\\ndocuments to fetch in RAG, or the expected steps to take as an agent.\\nLangSmith datasets are very flexible and allow you to define arbitrary schemas.How many: There's\\nno hard and fast rule for how many you should gather.\\nThe main thing is to make sure you have proper coverage of edge cases you may want to guard\\nagainst.\\nEven 10-50 examples can provide a lot of value!\\nDon't worry about getting a large number to start - you can (and should) always add over time!How\\nto get: This is maybe the trickiest part.\\nOnce you know you want to gather a dataset... how do you actually go about it?\\nFor most teams that are starting a new project, we generally see them start by collecting the first\\n10-20 datapoints by hand.\\nAfter starting with these datapoints, these datasets are generally living constructs and grow over\\ntime.\\nThey generally grow after seeing how real users will use your application, seeing the pain points that\\nexist, and then moving a few of those datapoints into this set.\\nThere are also methods like synthetically generating data that can be used to augment your dataset.\\nTo start, we recommend not worrying about those and just hand labeling ~10-20 examples.Once\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\3.pdf', 'page': 1}),\n",
       " Document(page_content='you\\'ve got your dataset, there are a few different ways to upload them to LangSmith.\\nFor this tutorial, we will use the client, but you can also upload via the UI (or even create them in the\\nUI).For this tutorial, we will create 5 datapoints to evaluate on.\\nWe will be evaluating a question-answering application.\\nThe input will be a question, and the output will be an answer.\\nSince this is a question-answering application, we can define the expected answer.\\nLet\\'s show how to create and upload this dataset to LangSmith!from langsmith import Clientclient =\\nClient()# Define dataset: these are your test casesdataset_name = \"QA Example Dataset\"dataset =\\nclient.create_dataset(dataset_name)client.create_examples(    inputs=[        {\"question\": \"What is\\nLangChain?\"},        {\"question\": \"What is LangSmith?\"},        {\"question\": \"What is OpenAI?\"},       \\n{\"question\": \"What is Google?\"},        {\"question\": \"What is Mistral?\"},    ],    outputs=[        {\"answer\":\\n\"A framework for building LLM applications\"},        {\"answer\": \"A platform for observing and\\nevaluating LLM applications\"},        {\"answer\": \"A company that creates Large Language Models\"},    \\n   {\"answer\": \"A technology company known for search\"},        {\"answer\": \"A company that creates\\nLarge Language Models\"},    ],    dataset_id=dataset.id,)Now, if we go the LangSmith UI and look for\\nQA Example Dataset in the Datasets & Testing page,\\nwhen we click into it we should see that we have five new examples.Define metrics?After creating\\nour dataset, we can now define some metrics to evaluate our responses on.\\nSince we have an expected answer, we can compare to that as part of our evaluation.\\nHowever, we do not expect our application to output those exact answers, but rather something that\\nis similar.\\nThis makes our evaluation a little trickier.In addition to evaluating correctness, let\\'s also make sure\\nour answers are short and concise.\\nThis will be a little easier - we can define a simple Python function to measure the length of the\\nresponse.Let\\'s go ahead and define these two metrics.For the first, we will use an LLM to judge\\nwhether the output is correct (with respect to the expected output).', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\3.pdf', 'page': 2}),\n",
       " Document(page_content='This LLM-as-a-judge is relatively common for cases that are too complex to measure with a simple\\nfunction.\\nWe can define our own prompt and LLM to use for evaluation here:from langchain_anthropic import\\nChatAnthropicfrom langchain_core.prompts.prompt import PromptTemplatefrom\\nlangsmith.evaluation import LangChainStringEvaluator_PROMPT_TEMPLATE = \"\"\"You are an\\nexpert professor specialized in grading students\\' answers to questions.You are grading the following\\nquestion:{query}Here is the real answer:{answer}You are grading the following predicted\\nanswer:{result}Respond with CORRECT or INCORRECT:Grade:\"\"\"PROMPT = PromptTemplate(   \\ninput_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE)eval_llm =\\nChatAnthropic(temperature=0.0)qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\":\\neval_llm, \"prompt\": PROMPT})noteThis example assumes you have the ANTHROPIC_API_KEY\\nenvironment variable set. You can just as easily run this example with OpenAI by replacing\\nChatAnthropic with ChatOpenAI from langchain_openai.For evaluating the length of the response,\\nthis is a lot easier!\\nWe can just define a simple function that checks whether the actual output is less than 2x the length\\nof the expected result.from langsmith.schemas import Run, Exampledef evaluate_length(run: Run,\\nexample: Example) -> dict:    prediction = run.outputs.get(\"output\") or \"\"    required =\\nexample.outputs.get(\"answer\") or \"\"    score = int(len(prediction) < 2 * len(required))    return\\n{\"key\":\"length\", \"score\": score}Run Evaluations?Great! So now how do we run evaluations?\\nNow that we have a dataset and evaluators, all that we need is our application!\\nWe will build a simple application that just has a system message with instructions on how to\\nrespond and then passes it to the LLM.\\nWe will build this using the OpenAI SDK directly:import openaiopenai_client = openai.Client()def\\nmy_app(question):    return openai_client.chat.completions.create(        model=\"gpt-3.5-turbo\",       \\ntemperature=0,        messages=[            {                \"role\": \"system\",                \"content\": \"Respond to\\nthe users question in a short, concise manner (one short sentence).\"            },            {               ', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\3.pdf', 'page': 3}),\n",
       " Document(page_content='\"role\": \"user\",                \"content\": question,            }        ],    ).choices[0].message.contentBefore\\nrunning this through LangSmith evaluations, we need to define a simple wrapper that maps the input\\nkeys from our dataset to the function we want to call,\\nand then also maps the output of the function to the output key we expect.def\\nlangsmith_app(inputs):    output = my_app(inputs[\"question\"])    return {\"output\": output}Great!\\nNow we\\'re ready to run evaluation.\\nLet\\'s do it!from langsmith.evaluation import evaluateexperiment_results = evaluate(   \\nlangsmith_app, # Your AI system    data=dataset_name, # The data to predict and grade over   \\nevaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results   \\nexperiment_prefix=\"openai-3.5\", # A prefix for your experiment names to easily identify them)This\\nwill output a URL. If we click on it, we should see results of our evaluation!If we go back to the\\ndataset page and select the Experiments tab, we can now see a summary of our one run!Let\\'s now\\ntry it out with a different model! Let\\'s try gpt-4-turboimport openaiopenai_client = openai.Client()def\\nmy_app_1(question):    return openai_client.chat.completions.create(        model=\"gpt-4-turbo\",       \\ntemperature=0,        messages=[            {                \"role\": \"system\",                \"content\": \"Respond to\\nthe users question in a short, concise manner (one short sentence).\"            },            {               \\n\"role\": \"user\",                \"content\": question,            }        ],    ).choices[0].message.contentdef\\nlangsmith_app_1(inputs):    output = my_app_1(inputs[\"question\"])    return {\"output\": output}from\\nlangsmith.evaluation import evaluateexperiment_results = evaluate(    langsmith_app_1, # Your AI\\nsystem    data=dataset_name, # The data to predict and grade over    evaluators=[evaluate_length,\\nqa_evaluator], # The evaluators to score the results    experiment_prefix=\"openai-4\", # A prefix for\\nyour experiment names to easily identify them)And now let\\'s use GPT-4 but also update the prompt\\nto be a bit more strict in requiring the answer to be short.import openaiopenai_client =\\nopenai.Client()def my_app_2(question):    return openai_client.chat.completions.create(       \\nmodel=\"gpt-4-turbo\",        temperature=0,        messages=[            {                \"role\": \"system\",            \\n   \"content\": \"Respond to the users question in a short, concise manner (one short sentence). Do', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\3.pdf', 'page': 4}),\n",
       " Document(page_content='NOT use more than ten words.\"            },            {                \"role\": \"user\",                \"content\":\\nquestion,            }        ],    ).choices[0].message.contentdef langsmith_app_2(inputs):    output =\\nmy_app_2(inputs[\"question\"])    return {\"output\": output}from langsmith.evaluation import\\nevaluateexperiment_results = evaluate(    langsmith_app_2, # Your AI system   \\ndata=dataset_name, # The data to predict and grade over    evaluators=[evaluate_length,\\nqa_evaluator], # The evaluators to score the results    experiment_prefix=\"strict-openai-4\", # A prefix\\nfor your experiment names to easily identify them)If we go back to the Experiments tab on the\\ndatasets page, we should see that all three runs now show up!Comparing results?Awesome, we\\'ve\\nevaluated three different runs. But how can we compare results?\\nThe first way we can do this is just by looking at the runs in the Experiments tab.\\nIf we do that, we can see a high level view of the metrics for each run:Great! So we can tell that\\nGPT-4 is better than GPT-3.5 at knowing who companies are, and we can see that the strict prompt\\nhelped a lot with the length.\\nBut what if we want to explore in more detail?In order to do that, we can select all the runs we want\\nto compare (in this case all three) and open them up in a comparison view:We immediately see all\\nthree tests side by side.\\nSome of the cells are color coded - this is showing a regression of a certain metric compared to a\\ncertain baseline.\\nWe automatically choose defaults for the baseline and metric, but you can change those yourself\\n(outlined in blue below).\\nYou can also choose which columns and which metrics you see by using the Display control\\n(outlined in yellow below).\\nYou can also automatically filter to only see the runs that have improvements/regressions by clicking\\non the icons at the top (outlined in red below).If we want to see more information, we can also select\\nthe Expand button that appears when hovering over a row to open up a side panel with more\\ndetailed information:Set up automated testing to run in CI/CD?Now that we\\'ve run this in a one-off', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\3.pdf', 'page': 5}),\n",
       " Document(page_content='manner, we can set it to run in an automated fashion.\\nWe can do this pretty easily by just including it as a pytest file that we run in CI/CD.\\nAs part of this, we can either just log the results OR set up some criteria to determine if it passes or\\nnot.\\nFor example, if I wanted to ensure that we always got at least 80% of generated responses passing\\nthe length check,\\nwe could set that up with a test like:def test_length_score() -> None:    \"\"\"Test that the length score\\nis at least 80%.\"\"\"    experiment_results = evaluate(        langsmith_app, # Your AI system       \\ndata=dataset_name, # The data to predict and grade over        evaluators=[evaluate_length,\\nqa_evaluator], # The evaluators to score the results    )    # This will be cleaned up in the next\\nrelease:    feedback = client.list_feedback(        run_ids=[r.id for r in\\nclient.list_runs(project_name=experiment_results.experiment_name)],        feedback_key=\"length\"   \\n)    scores = [f.score for f in feedback]    assert sum(scores) / len(scores) >= 0.8, \"Aggregate score\\nshould be at least .8\"Track results over time?Now that we\\'ve got these experiments running in an\\nautomated fashion, we want to track these results over time.\\nWe can do this from the overall Experiments tab in the datasets page.\\nBy default, we show evaluation metrics over time (highlighted in red).\\nWe also automatically track git metrics, to easily associate it with the branch of your code\\n(highlighted in yellow).Conclusion?That\\'s it for this tutorial!We\\'ve gone over how to create an initial\\ntest set, define some evaluation metrics, run experiments, compare them manually, set up CI/CD,\\nand track results over time.\\nHopefully this can help you iterate with confidence.This is just the start. As mentioned earlier,\\nevaluation is an ongoing process.\\nFor example - the datapoints you will want to evaluate on will likely continue to change over time.\\nThere are many types of evaluators you may wish to explore.\\nFor information on this, check out the how-to guides.Additionally, there are other ways to evaluate', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\3.pdf', 'page': 6}),\n",
       " Document(page_content='data besides in this \"offline\" manner (e.g. you can evaluate production data).\\nFor more information on online evaluation, check out this guide.Was this page helpful?PreviousAdd\\nobservability to your LLM applicationNextOptimize a classifierCreate a datasetDefine metricsRun\\nEvaluationsComparing resultsSet up automated testing to run in CI/CDTrack results over\\ntimeConclusionCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\3.pdf', 'page': 7}),\n",
       " Document(page_content=\"Share or unshare a dataset publicly | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsManage datasets in the applicationManage datasets\\nprogrammaticallyVersion datasetsShare or unshare a dataset publiclyEvaluationHuman\\nfeedbackMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesDatasetsShare or unshare a dataset publiclyShare or unshare a dataset\\npubliclycautionSharing a dataset publicly will make it accessible to anyone with the link. Make sure\\nyou're not sharing sensitive information.\\nThis link gives viewers access to all example rows, experiments, and associated runs and feedback\\non this dataset.\\nThis feature is only available in the cloud-hosted version of LangSmith.To share a dataset publicly,\\nsimply click on the Share button in the upper right hand side of any dataset details page.\\nThis will open a dialog where you can copy the link to the dataset.Shared datasets will be accessible\\nto anyone with the link, even if they don't have a LangSmith account. They will be able to view the\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\30.pdf', 'page': 0}),\n",
       " Document(page_content='dataset, experiments and examples, but not edit any of this information.To \"unshare\" a dataset,\\neitherClick on Unshare by click on Public in the upper right hand corner of any publicly shared\\ndataset, then Unshare in the dialog.\\nNavigate to your organization\\'s list of publicly shared dataset, either by clicking on Settings ->\\nShared URLs or this link, then click on Unshare next to the dataset you want to unshare.\\nWas this page helpful?PreviousVersion datasetsNextEvaluate an LLM\\nApplicationCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\30.pdf', 'page': 1}),\n",
       " Document(page_content='Evaluate an LLM Application | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationEvaluate an LLM ApplicationBind an evaluator to a dataset\\nin the UIRun an evaluation from the prompt playgroundEvaluate on intermediate stepsUse\\nLangChain off-the-shelf evaluators (Python only)Compare experiment resultsEvaluate an existing\\nexperimentUnit test LLM applications (Python only)Run pairwise evaluationsHuman\\nfeedbackMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesEvaluationEvaluate an LLM ApplicationOn this pageEvaluate an LLM\\nApplicationRecommended ReadingBefore diving into this content, it might be helpful to read the\\nfollowing:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on\\nmanaging datasets programmaticallyEvaluating the performance of your LLM application is a critical\\nstep in the development process. LangSmith makes it easy to run evaluations and track evaluation\\nperformance over time.\\nThis section provides guidance on how to evaluate the performance of your LLM application.Run an', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\31.pdf', 'page': 0}),\n",
       " Document(page_content='evaluation?At a high-level, the evaluation process involves the following steps:Define your LLM\\napplication or target task.Creating or selecting a dataset to evaluate your LLM application. Your\\nevaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to\\nscore the outputs of your LLM application, sometimes against expected outputs.Running the\\nevaluation and viewing the results.The following example involves evaluating a very simple LLM\\npipeline as classifier to label input data as \"Toxic\" or \"Not toxic\".Step 1: Define your target task?In\\nthis case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text\\nas toxic or non-toxic.\\nWe\\'ve optionally enabled tracing to capture the inputs and outputs of each step in the pipeline.To\\nunderstand how to annotate your code for tracing, please refer to this guide.PythonTypeScriptfrom\\nlangsmith import traceable, wrappersfrom openai import Clientopenai =\\nwrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            \"role\":\\n\"system\",            \"content\": \"Please review the user query below and determine if it contains any\\nform of toxic behavior, such as insults, threats, or highly negative comments. Respond with \\'Toxic\\' if\\nit does, and \\'Not toxic\\' if it doesn\\'t.\",        },        {\"role\": \"user\", \"content\": text},    ]    result =\\nopenai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0   \\n)    return result.choices[0].message.contentimport { OpenAI } from \"openai\";import { wrapOpenAI }\\nfrom \"langsmith/wrappers\";import { traceable } from \"langsmith/traceable\";const client =\\nwrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result =\\nawait client.chat.completions.create({      messages: [        {           role: \"system\",          content:\\n\"Please review the user query below and determine if it contains any form of toxic behavior, such as\\ninsults, threats, or highly negative comments. Respond with \\'Toxic\\' if it does, and \\'Not toxic\\' if it\\ndoesn\\'t.\",        },        { role: \"user\", content: text },      ],      model: \"gpt-3.5-turbo\",      temperature: 0, \\n  });        return result.choices[0].message.content;  },  { name: \"labelText\" });Step 2: Create or select\\na dataset?In this case, we are creating a dataset to evaluate the performance of our LLM\\napplication. The dataset contains examples of toxic and non-toxic text.Each Example in the dataset', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\31.pdf', 'page': 1}),\n",
       " Document(page_content='contains three dictionaries / objects:outputs: The reference labels or other context found in your\\ndatasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that\\nexample within the datasetThese dictionaries / objects can have arbitrary keys and values, but the\\nkeys must be consistent across all examples in the dataset.\\nThe values in the examples can also take any form, such as strings, numbers, lists, or dictionaries,\\nbut for this example, we are simply using strings.PythonTypeScriptfrom langsmith import Clientclient\\n= Client()# Create a datasetexamples = [    (\"Shut up, idiot\", \"Toxic\"),    (\"You\\'re a wonderful person\",\\n\"Not toxic\"),    (\"This is the worst thing ever\", \"Toxic\"),    (\"I had a great day today\", \"Not toxic\"),   \\n(\"Nobody likes you\", \"Toxic\"),    (\"This is unacceptable. I want to speak to the manager.\", \"Not\\ntoxic\"),]dataset_name = \"Toxic Queries\"dataset =\\nclient.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"text\": text}, {\"label\":\\nlabel}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs,\\ndataset_id=dataset.id)import { Client } from \"langsmith\";const langsmith = new Client();// create a\\ndatasetconst toxicExamples = [  [\"Shut up, idiot\", \"Toxic\"],  [\"You\\'re a wonderful person\", \"Not toxic\"],\\n [\"This is the worst thing ever\", \"Toxic\"],  [\"I had a great day today\", \"Not toxic\"],  [\"Nobody likes you\",\\n\"Toxic\"],  [\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"],];const [inputs,\\noutputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs,\\noutputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const\\ndatasetName = \"Toxic Queries\";const toxicDataset = await\\nlangsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId:\\ntoxicDataset.id });Step 3. Configure evaluators to score the outputs?In this case, we are using a\\ndead-simple evaluator that compares the output of our LLM pipeline to the expected output in the\\ndataset.\\nWriting evaluators is discussed in more detail in the following section.PythonTypeScriptfrom\\nlangsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) ->\\ndict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\":', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\31.pdf', 'page': 2}),\n",
       " Document(page_content='int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import\\ntype { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction\\ncorrectLabel(rootRun: Run, example: Example): EvaluationResult {  const score =\\nrootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };}Step\\n4. Run the evaluation and view the results?You can use the evaluate method in Python and\\nTypeScript to run an evaluation.At its simplest, the evaluate method takes the following arguments:a\\nfunction that takes an input dictionary or object and returns an output dictionary or objectdata - the\\nname OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list\\nof evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment\\nname with. A name will be generated if not provided.PythonTypeScriptfrom langsmith.evaluation\\nimport evaluatedataset_name = \"Toxic Queries\"results = evaluate(    lambda inputs:\\nlabel_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],   \\nexperiment_prefix=\"Toxic Queries\",    description=\"Testing the baseline system.\",  # optional)import\\n{ evaluate } from \"langsmith/evaluation\";const datasetName = \"Toxic Queries\";await\\nevaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel], \\nexperimentPrefix: \"Toxic Queries\",});Each invocation of evaluate produces an experiment which is\\nbound to the dataset, and can be viewed in the LangSmith UI.\\nEvaluation scores are stored against each individual output produced by the target task as\\nfeedback, with the name and score configured in the evaluator.If you\\'ve annotated your code for\\ntracing, you can open the trace of each row in a side panel view.Use custom evaluators?At a\\nhigh-level, evaluators are functions that take in a Run and an Example and return a dictionary or\\nobject with a keys score (numeric) and key (string).\\nThe key will be associated with the score in the LangSmith UI.advanced use-casesConfigure more\\nfeedback fields: you can configure other fields in the dictionary as well. Please see the feedback\\nreference for more information.Evaluate on intermediate steps: to view a more advanced example\\nthat traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\31.pdf', 'page': 3}),\n",
       " Document(page_content='steps.To learn more about the Run format, you can read the following reference. However, many of\\nthe fields are not relevant nor required for writing evaluators.\\nThe root_run / rootRun is always available and contains the inputs and outputs of the target task. If\\ntracing is enabled, the root_run / rootRun will also contain child runs for each step in the\\npipeline.Here is an example of a very simple custom evaluator that compares the output of a model\\nto the expected output in the dataset:PythonTypeScriptfrom langsmith.schemas import Example,\\nRundef correct_label(root_run: Run, example: Example) -> dict:    score =\\nroot_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\":\\n\"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run,\\nExample } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run,\\nexample: Example): EvaluationResult {  const score = rootRun.outputs?.outputs ===\\nexample.outputs?.output;  return { key: \"correct_label\", score };}default feedback keyIf the \"key\" field\\nis not provided, the default key name will be the name of the evaluator function.Evaluate on a\\nparticular version of a dataset?Recommended ReadingBefore diving into this content, it might be\\nhelpful to read the guide on versioning datasets.\\nAdditionally, it might be helpful to read the guide on fetching examples.You can take advantage of\\nthe fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of\\na dataset.\\nSimply use list_examples / listExamples to fetch examples from a particular version tag using as_of /\\nasOf.PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda\\ninputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=toxic_dataset_name,\\nas_of=\"latest\"),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate\\n} from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data:\\nlangsmith.listExamples({    datasetName: datasetName,    asOf: \"latest\",  }),  evaluators:\\n[correctLabel],  experimentPrefix: \"Toxic Queries\",});Evaluate on a subset of a\\ndataset?Recommended ReadingBefore diving into this content, it might be helpful to read the guide', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\31.pdf', 'page': 4}),\n",
       " Document(page_content='on fetching examples.You can use the list_examples / listExamples method to fetch a subset of\\nexamples from a dataset to evaluate on. You can refer to guide above to learn more about the\\ndifferent ways to fetch examples.One common workflow is to fetch examples that have a certain\\nmetadata key-value pair.PythonTypeScriptfrom langsmith.evaluation import evaluateresults =\\nevaluate(    lambda inputs: label_text(inputs[\"text\"]),   \\ndata=client.list_examples(dataset_name=dataset_name, metadata={\"desired_key\":\\n\"desired_value\"}),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import {\\nevaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data:\\nlangsmith.listExamples({    datasetName: datasetName,    metadata: {\"desired_key\":\\n\"desired_value\"},  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});Use a\\nsummary evaluator?Some metrics can only be defined on the entire experiment level as opposed to\\nthe individual runs of the experiment.\\nFor example, you may want to compute the overall pass rate or f1 score of your evaluation target\\nacross all examples in the dataset.\\nThese are called summary_evaluators. Instead of taking in a single Run and Example, these\\nevaluators take a list of each.Below, we\\'ll implement a very simple summary evaluator that\\ncomputes overall pass rate:PythonTypeScriptfrom langsmith.schemas import Example, Rundef\\nsummary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in\\nenumerate(runs):        if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:            correct += 1    if\\ncorrect / len(runs) > 0.5:        return {\"key\": \"pass\", \"score\": True}    else:        return {\"key\": \"pass\",\\n\"score\": False}import { Run, Example } from \"langsmith/schemas\";function summaryEval(runs:\\nRun[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if\\n(runs[i].outputs[\"output\"] === examples[i].outputs[\"label\"]) {      correct += 1;    }  }    return { key:\\n\"pass\", score: correct / runs.length > 0.5 };}You can then pass this evaluator to the evaluate method\\nas follows:PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[\"text\"]),   \\ndata=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],   ', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\31.pdf', 'page': 5}),\n",
       " Document(page_content='experiment_prefix=\"Toxic Queries\",)await evaluate((inputs) => labelQuery(inputs[\"input\"]), {  data:\\ndatasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix:\\n\"Toxic Queries\",});In the LangSmith UI, you\\'ll the summary evaluator\\'s score displayed with the\\ncorresponding key.Evaluate a LangChain runnable?You can configure a LangChain runnable to be\\nevaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in\\nTypeScript.First, define your LangChain runnable:PythonTypeScriptfrom langchain_openai import\\nChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom\\nlangchain_core.output_parsers import StrOutputParserprompt =\\nChatPromptTemplate.from_messages([  (\"system\", \"Please review the user query below and\\ndetermine if it contains any form of toxic behavior, such as insults, threats, or highly negative\\ncomments. Respond with \\'Toxic\\' if it does, and \\'Not toxic\\' if it doesn\\'t.\"),  (\"user\",\\n\"{text}\")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model\\n| output_parserimport { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate }\\nfrom \"@langchain/core/prompts\";import { StringOutputParser } from\\n\"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\",\\n\"Please review the user query below and determine if it contains any form of toxic behavior, such as\\ninsults, threats, or highly negative comments. Respond with \\'Toxic\\' if it does, and \\'Not toxic\\' if it\\ndoesn\\'t.\"],  [\"user\", \"{text}\"]]);const chatModel = new ChatOpenAI();const outputParser = new\\nStringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser);Then, pass the\\nrunnable.invoke method to the evaluate method. Note that the input variables of the runnable must\\nmatch the keys of the example inputs.PythonTypeScriptfrom langsmith.evaluation import\\nevaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],   \\nexperiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await\\nevaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic\\nQueries\",});The runnable is traced appropriately for each output.Was this page\\nhelpful?PreviousShare or unshare a dataset publiclyNextBind an evaluator to a dataset in the UIRun', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\31.pdf', 'page': 6}),\n",
       " Document(page_content='an evaluationStep 1: Define your target taskStep 2: Create or select a datasetStep 3. Configure\\nevaluators to score the outputsStep 4. Run the evaluation and view the resultsUse custom\\nevaluatorsEvaluate on a particular version of a datasetEvaluate on a subset of a datasetUse a\\nsummary evaluatorEvaluate a LangChain runnableCommunityDiscordTwitterGitHubDocs\\nCodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS\\nDocsCopyright © 2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\31.pdf', 'page': 7}),\n",
       " Document(page_content='Bind an evaluator to a dataset in the UI | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationEvaluate an LLM ApplicationBind an evaluator to a dataset\\nin the UIRun an evaluation from the prompt playgroundEvaluate on intermediate stepsUse\\nLangChain off-the-shelf evaluators (Python only)Compare experiment resultsEvaluate an existing\\nexperimentUnit test LLM applications (Python only)Run pairwise evaluationsHuman\\nfeedbackMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesEvaluationBind an evaluator to a dataset in the UIBind an evaluator to a dataset in the\\nUIWhile you can specify evaluators to grade the results of your experiments programmatically (see\\nthis guide for more information), you can also bind evaluators to a dataset in the UI.\\nThis allows you to configure automatic evaluators that grade your experiment results without having\\nto write any code. Currently, only LLM-based evaluators are supported.The process for configuring\\nthis is very similar to the process for configuring an online evaluator for traces.Only affects\\nsubsequent experiment runsWhen you configure an evaluator for a dataset, it will only affect the', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\32.pdf', 'page': 0}),\n",
       " Document(page_content='experiment runs that are created after the evaluator is configured. It will not affect the evaluation of\\nexperiment runs that were created before the evaluator was configured.Navigate to the dataset\\ndetails page by clicking Datasets and Testing in the sidebar and selecting the dataset you want to\\nconfigure the evaluator for.Click on the Add Evaluator button to add an evaluator to the dataset. This\\nwill open a modal you can use to configure the evaluator.Give your evaluator a name and set an\\ninline prompt or load a prompt from the prompt hub that will be used to evaluate the results of the\\nruns in the experiment.Importantly, evaluator prompts can only contain the following input\\nvariables:input (required): the input to the target you are evaluatingoutput (required): the output of\\nthe target you are evaluatingreference: the reference output, taken from the datasetnoteAutomatic\\nevaluators you configure in the application will only work if the inputs to your evaluation target,\\noutputs from your evaluation target, and examples in your dataset are all single-key dictionaries.\\nLangSmith will automatically extract the values from the dictionaries and pass them to the\\nevaluator.LangSmith currently doesn\\'t support setting up evaluators in the application that act on\\nmultiple keys in the inputs or outputs or examples dictionaries.You can specify the scoring criteria in\\nthe \"schema\" field. In this example, we are asking the LLM to grade on \"correctness\" of the output\\nwith respect to the reference, with a boolean output of 0 or 1. The name of the field in the schema\\nwill be interpreted as the feedback key and the type will be the type of the score.Save the evaluator\\nand navigate back to the dataset details page. Each subsequent experiment run from the dataset\\nwill now be evaluated by the evaluator you configured. Note that in the below image, each run in the\\nexperiment has a \"correctness\" score.Was this page helpful?PreviousEvaluate an LLM\\nApplicationNextRun an evaluation from the prompt playgroundCommunityDiscordTwitterGitHubDocs\\nCodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS\\nDocsCopyright © 2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\32.pdf', 'page': 1}),\n",
       " Document(page_content='Run an evaluation from the prompt playground | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationEvaluate an LLM ApplicationBind an evaluator to a dataset\\nin the UIRun an evaluation from the prompt playgroundEvaluate on intermediate stepsUse\\nLangChain off-the-shelf evaluators (Python only)Compare experiment resultsEvaluate an existing\\nexperimentUnit test LLM applications (Python only)Run pairwise evaluationsHuman\\nfeedbackMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesEvaluationRun an evaluation from the prompt playgroundOn this pageRun an evaluation from\\nthe prompt playgroundWhile you can kick off experiments easily using the sdk, as outlined here, it\\'s\\noften useful to run experiments directly in the prompt playground.This allows you to test your prompt\\n/ model configuration over a series of inputs to see how well it generalizes across different contexts\\nor scenarios, without having to write any code.Create an experiment in the prompt\\nplayground?Navigate to the prompt playground by clicking on \"Prompts\" in the sidebar, then\\nselecting a prompt from the list of available prompts or creating a new one.Select the \"Switch to', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\33.pdf', 'page': 0}),\n",
       " Document(page_content='dataset\" button to switch to the dataset you want to use for the experiment. Please note that the\\ndataset keys of the dataset inputs must match the input variables of the prompt. In the below\\nsections, note that the selected dataset has inputs with keys \"text\", which correctly match the input\\nvariable of the prompt. Also note that there is a max capacity of 15 inputs for the prompt playground.\\nClick on the \"Start\" button or CMD+Enter to start the experiment. This will run the prompt over all the\\nexamples in the dataset and create an entry for the experiment in the dataset details page. Note that\\nyou need to commit the prompt to the prompt hub before you can start the experiment to ensure it\\ncan be referenced in the experiment. The result for each input will be streamed and displayed inline\\nfor each input in the dataset.\\nView the results by clicking on the \"View Experiment\" button at the bottom of the page. This will take\\nyou to the experiment details page where you can see the results of the experiment.Navigate back\\nto the commit page by clicking on the \"View Commit\" button. This will take you back to the prompt\\npage where you can make changes to the prompt and run more experiments. The \"View Commit\"\\nbutton is available to all experiments that were run from the prompt playground. The experiment is\\nprefixed with the prompt repository name, a unique identifier, and the date and time the experiment\\nwas run.\\nAdd evaluation scores to the experiment?You can add evaluation scores to experiments by binding\\nan evaluator to the dataset, again without writing any code.You can also programmatically evaluate\\nan existing experiment using the SDK.Was this page helpful?PreviousBind an evaluator to a dataset\\nin the UINextEvaluate on intermediate stepsCreate an experiment in the prompt playgroundAdd\\nevaluation scores to the experimentCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\33.pdf', 'page': 1}),\n",
       " Document(page_content='Evaluate on intermediate steps | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationEvaluate an LLM ApplicationBind an evaluator to a dataset\\nin the UIRun an evaluation from the prompt playgroundEvaluate on intermediate stepsUse\\nLangChain off-the-shelf evaluators (Python only)Compare experiment resultsEvaluate an existing\\nexperimentUnit test LLM applications (Python only)Run pairwise evaluationsHuman\\nfeedbackMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesEvaluationEvaluate on intermediate stepsOn this pageEvaluate on intermediate stepsWhile,\\nin many scenarios, it is sufficient to evaluate the final output of your task, in some cases you might\\nwant to evaluate the intermediate steps of your pipeline.For example, for retrieval-augmented\\ngeneration (RAG), you might want toEvaluate the retrieval step to ensure that the correct documents\\nare retrieved w.r.t the input query.Evaluate the generation step to ensure that the correct answer is\\ngenerated w.r.t the retrieved documents.In this guide, we will use a simple, fully-custom evaluator\\nfor evaluating criteria 1 and an LLM-based evaluator for evaluating criteria 2 to highlight both', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\34.pdf', 'page': 0}),\n",
       " Document(page_content='scenarios.In order to evaluate the intermediate steps of your pipeline, your evaluator function should\\ntraverse and process the root_run/rootRun argument, which is a Run object that contains the\\nintermediate steps of your pipeline.1. Define your LLM pipeline?The below RAG pipeline consists of\\n1) generating a Wikipedia query given the input question, 2) retrieving relevant documents from\\nWikipedia, and 3) generating an answer given the retrieved documents.PythonTypeScriptimport\\nopenaiimport wikipedia as wpfrom langsmith import traceablefrom langsmith.wrappers import\\nwrap_openaiopenai = wrap_openai(openai.Client())@traceabledef generate_wiki_search(question): \\n  messages = [        {\"role\": \"system\", \"content\": \"Generate a search query to pass into wikipedia to\\nanswer the user\\'s question. Return only the search query and nothing more. This will passed in\\ndirectly to the wikipedia search engine.\"},        {\"role\": \"user\", \"content\": question}    ]    result =\\nopenai.chat.completions.create(messages=messages, model=\"gpt-3.5-turbo\", temperature=0)   \\nreturn result.choices[0].message.content@traceable(run_type=\"retriever\")def retrieve(query):   \\nresults = []    for term in wp.search(query, results = 10):        try:            page = wp.page(term,\\nauto_suggest=False)            results.append({                \"page_content\": page.summary,               \\n\"type\": \"Document\",                \"metadata\": {\"url\": page.url}            })        except\\nwp.DisambiguationError:            pass        if len(results) >= 2:            return results@traceabledef\\ngenerate_answer(question, context):    messages = [        {\"role\": \"system\", \"content\": f\"Answer the\\nuser\\'s question based ONLY on the content below:\\\\n\\\\n{context}\"},        {\"role\": \"user\", \"content\":\\nquestion}    ]    result = openai.chat.completions.create(messages=messages,\\nmodel=\"gpt-3.5-turbo\", temperature=0)    return result.choices[0].message.content@traceabledef\\nrag_pipeline(question):    query = generate_wiki_search(question)    context =\\n\"\\\\n\\\\n\".join([doc[\"page_content\"] for doc in retrieve(query)])    answer = generate_answer(question,\\ncontext)    return answerimport OpenAI from \"openai\";import wiki from \"wikipedia\";import { Client }\\nfrom \"langsmith\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from\\n\"langsmith/wrappers\";const openai = wrapOpenAI(new OpenAI());const generateWikiSearch =\\ntraceable(  async (input: { question: string }) => {    const messages = [      {        role: \"system\" as', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\34.pdf', 'page': 1}),\n",
       " Document(page_content='const,        content:          \"Generate a search query to pass into Wikipedia to answer the user\\'s\\nquestion. Return only the search query and nothing more. This will be passed in directly to the\\nWikipedia search engine.\",      },      { role: \"user\" as const, content: input.question },    ];        const\\nchatCompletion = await openai.chat.completions.create({      model: \"gpt-3.5-turbo\",      messages:\\nmessages,      temperature: 0,    });        return chatCompletion.choices[0].message.content ?? \"\";  }, \\n{ name: \"generateWikiSearch\" });const retrieve = traceable(  async (input: { query: string;\\nnumDocuments: number }) => {    const { results } = await wiki.search(input.query, { limit: 10 });   \\nconst finalResults: Array<{      page_content: string;      type: \"Document\";      metadata: { url: string };\\n   }> = [];      for (const result of results) {      if (finalResults.length >= input.numDocuments) {        //\\nJust return the top 2 pages for now        break;      }      const page = await wiki.page(result.title, {\\nautoSuggest: false });      const summary = await page.summary();      finalResults.push({       \\npage_content: summary.extract,        type: \"Document\",        metadata: { url: page.fullurl },      });    }   \\nreturn finalResults;  },  { name: \"retrieve\", run_type: \"retriever\" });const generateAnswer = traceable( \\nasync (input: { question: string; context: string }) => {    const messages = [      {        role: \"system\"\\nas const,        content: `Answer the user\\'s question based only on the content\\nbelow:\\\\n\\\\n${input.context}`,      },      { role: \"user\" as const, content: input.question },    ];      const\\nchatCompletion = await openai.chat.completions.create({      model: \"gpt-3.5-turbo\",      messages:\\nmessages,      temperature: 0,    });    return chatCompletion.choices[0].message.content ?? \"\";  },  {\\nname: \"generateAnswer\" });const ragPipeline = traceable(  async ({ question }: { question: string },\\nnumDocuments: number = 2) => {    const query = await generateWikiSearch({ question });    const\\nretrieverResults = await retrieve({ query, numDocuments });    const context = retrieverResults     \\n.map((result) => result.page_content)      .join(\"\\\\n\\\\n\");    const answer = await generateAnswer({\\nquestion, context });    return answer;  },  { name: \"ragPipeline\" });This pipeline will produce a trace\\nthat looks something like:\\n2. Create a dataset and examples to evaluate the pipeline?We are building a very simple dataset\\nwith a couple of examples to evaluate the pipeline.PythonTypeScriptfrom langsmith import', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\34.pdf', 'page': 2}),\n",
       " Document(page_content='Clientclient = Client()examples = [    (\"What is LangChain?\", \"LangChain is an open-source\\nframework for building applications using large language models.\"),    (\"What is LangSmith?\",\\n\"LangSmith is an observability and evaluation tool for LLM products, built by LangChain\\nInc.\")]dataset_name = \"Wikipedia RAG\"if not client.has_dataset(dataset_name=dataset_name):   \\ndataset = client.create_dataset(dataset_name=dataset_name)    inputs, outputs = zip(       \\n*[({\"input\": input}, {\"expected\": expected}) for input, expected in examples]    )   \\nclient.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from\\n\"langsmith\";const client = new Client();const examples = [  [    \"What is LangChain?\",    \"LangChain\\nis an open-source framework for building applications using large language models.\",  ],  [    \"What is\\nLangSmith?\",    \"LangSmith is an observability and evaluation tool for LLM products, built by\\nLangChain Inc.\",  ],];const datasetName = \"Wikipedia RAG\";const inputs = examples.map(([input, _])\\n=> ({ input }));const outputs = examples.map(([_, expected]) => ({ expected }));const dataset = await\\nclient.createDataset(datasetName);await client.createExamples({ datasetId: dataset.id, inputs,\\noutputs });3. Define your custom evaluators?As mentioned above, we will define two evaluators: one\\nthat evaluates the relevance of the retrieved documents w.r.t the input query and another that\\nevaluates the hallucination of the generated answer w.r.t the retrieved documents.\\nWe will be using LangChain LLM wrappers, along with with_structured_output to define the\\nevaluator for hallucination.The key here is that the evaluator function should traverse the root_run /\\nrootRun argument to access the intermediate steps of the pipeline. The evaluator can then process\\nthe inputs and outputs of the intermediate steps to evaluate according to the desired\\ncriteria.PythonTypeScriptfrom langsmith.evaluation import LangChainStringEvaluator, evaluatefrom\\nlangsmith.schemas import Example, Runfrom langchain_openai import ChatOpenAIfrom\\nlangchain_core.prompts import ChatPromptTemplatefrom langchain_core.pydantic_v1 import\\nBaseModel, Fielddef document_relevance(root_run: Run, example: Example) -> dict:    \"\"\"    A very\\nsimple evaluator that checks to see if the input of the retrieval step exists    in the retrieved docs.   \\n\"\"\"    rag_pipeline_run = next(run for run in root_run.child_runs if run.name == \"rag_pipeline\")   ', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\34.pdf', 'page': 3}),\n",
       " Document(page_content='retrieve_run = next(run for run in rag_pipeline_run.child_runs if run.name == \"retrieve\")   \\npage_contents = \"\\\\n\\\\n\".join(doc[\"page_content\"] for doc in retrieve_run.outputs[\"output\"])    score =\\nretrieve_run.inputs[\"query\"] in page_contents    return {\"key\": \"simple_document_relevance\",\\n\"score\": score}def hallucination(root_run: Run, example: Example) -> dict:    \"\"\"    A simple evaluator\\nthat checks to see the answer is grounded in the documents    \"\"\"    # Get documents and answer   \\nrag_pipeline_run = next(run for run in root_run.child_runs if run.name == \"rag_pipeline\")   \\nretrieve_run = next(run for run in rag_pipeline_run.child_runs if run.name == \"retrieve\")   \\npage_contents = \"\\\\n\\\\n\".join(doc[\"page_content\"] for doc in retrieve_run.outputs[\"output\"])   \\ngeneration = rag_pipeline_run.outputs[\"output\"]    # Data model    class\\nGradeHallucinations(BaseModel):        \"\"\"Binary score for hallucination present in generation\\nanswer.\"\"\"        binary_score: int = Field(description=\"Answer is grounded in the facts, 1 or 0\")    #\\nLLM with function call    llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)   \\nstructured_llm_grader = llm.with_structured_output(GradeHallucinations)    # Prompt    system =\\n\"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of\\nretrieved facts. \\\\n        Give a binary score 1 or 0, where 1 means that the answer is grounded in /\\nsupported by the set of facts.\"\"\"    hallucination_prompt = ChatPromptTemplate.from_messages(      \\n [            (\"system\", system),            (\"human\", \"Set of facts: \\\\n\\\\n {documents} \\\\n\\\\n LLM generation:\\n{generation}\"),        ]    )    hallucination_grader = hallucination_prompt | structured_llm_grader   \\nscore = hallucination_grader.invoke({\"documents\": page_contents, \"generation\": generation})   \\nreturn {\"key\": \"answer_hallucination\", \"score\": int(score.binary_score)}import { EvaluationResult }\\nfrom \"langsmith/evaluation\";import { Run, Example } from \"langsmith/schemas\";import {\\nChatPromptTemplate } from \"@langchain/core/prompts\";import { ChatOpenAI } from\\n\"@langchain/openai\";import { z } from \"zod\";function findNestedRun(run: Run, search: (run: Run) =>\\nboolean): Run | null {  const queue: Run[] = [run];  while (queue.length > 0) {    const currentRun =\\nqueue.shift()!;    if (search(currentRun)) return currentRun;    queue.push(...currentRun.child_runs); \\n}  return null;}// A very simple evaluator that checks to see if the input of the retrieval step exists// in', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\34.pdf', 'page': 4}),\n",
       " Document(page_content='the retrieved docs.function documentRelevance(rootRun: Run, example: Example):\\nEvaluationResult {  const retrieveRun = findNestedRun(rootRun, (run) => run.name === \"retrieve\"); \\nconst docs: Array<{ page_content: string }> | undefined =    retrieveRun.outputs?.outputs;    const\\npageContents = docs?.map((doc) => doc.page_content).join(\"\\\\n\\\\n\");  const score =\\npageContents.includes(retrieveRun.inputs?.query);  return { key: \"simple_document_relevance\",\\nscore };}async function hallucination(  rootRun: Run,  example: Example):\\nPromise<EvaluationResult> {  const rag = findNestedRun(rootRun, (run) => run.name ===\\n\"ragPipeline\");  const retrieve = findNestedRun(rootRun, (run) => run.name === \"retrieve\");    const\\ndocs: Array<{ page_content: string }> | undefined =    retrieve.outputs?.outputs;    const documents\\n= docs?.map((doc) => doc.page_content).join(\"\\\\n\\\\n\");    const prompt =\\nChatPromptTemplate.fromMessages<{    documents: string;    generation: string;  }>([    [     \\n\"system\",      [        `You are a grader assessing whether an LLM generation is grounded in /\\nsupported by a set of retrieved facts. \\\\n`,        `Give a binary score 1 or 0, where 1 means that the\\nanswer is grounded in / supported by the set of facts.`,      ].join(\"\\\\n\"),    ],    [      \"human\",      \"Set of\\nfacts: \\\\n\\\\n {documents} \\\\n\\\\n LLM generation: {generation}\",    ],  ]);    const llm = new ChatOpenAI({   \\nmodel: \"gpt-3.5-turbo-0125\",    temperature: 0,  }).withStructuredOutput(    z      .object({       \\nbinary_score: z          .number()          .describe(\"Answer is grounded in the facts, 1 or 0\"),      })     \\n.describe(\"Binary score for hallucination present in generation answer.\")  );    const grader =\\nprompt.pipe(llm);  const score = await grader.invoke({    documents,    generation:\\nrag.outputs?.outputs,  });    return { key: \"answer_hallucination\", score: score.binary_score };}4.\\nEvaluate the pipeline?Finally, we\\'ll run evaluate with the custom evaluators defined\\nabove.PythonTypeScriptfrom langsmith.evaluation import evaluateexperiment_results = evaluate(   \\nlambda inputs: rag_pipeline(inputs[\"input\"]),    data=dataset_name,   \\nevaluators=[document_relevance, hallucination],    experiment_prefix=\"rag-wiki-oai\")import {\\nevaluate } from \"langsmith/evaluation\";await evaluate((inputs) => ragPipeline({ question: inputs.input\\n}), {  data: datasetName,  evaluators: [hallucination, documentRelevance],  experimentPrefix:', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\34.pdf', 'page': 5}),\n",
       " Document(page_content='\"rag-wiki-oai\",});The experiment will contain the results of the evaluation, including the scores and\\ncomments from the evaluators:\\nWas this page helpful?PreviousRun an evaluation from the prompt playgroundNextUse LangChain\\noff-the-shelf evaluators (Python only)1. Define your LLM pipeline2. Create a dataset and examples\\nto evaluate the pipeline3. Define your custom evaluators4. Evaluate the\\npipelineCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\34.pdf', 'page': 6}),\n",
       " Document(page_content='Use LangChain off-the-shelf evaluators (Python only) | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationEvaluate an LLM ApplicationBind an evaluator to a dataset\\nin the UIRun an evaluation from the prompt playgroundEvaluate on intermediate stepsUse\\nLangChain off-the-shelf evaluators (Python only)Compare experiment resultsEvaluate an existing\\nexperimentUnit test LLM applications (Python only)Run pairwise evaluationsHuman\\nfeedbackMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesEvaluationUse LangChain off-the-shelf evaluators (Python only)On this pageUse LangChain\\noff-the-shelf evaluators (Python only)Recommended ReadingBefore diving into this content, it might\\nbe helpful to read the following:LangChain evaluator referenceLangChain provides a suite of\\noff-the-shelf evaluators you can use right away to evaluate your application performance without\\nwriting any custom code.\\nThese evaluators are meant to be used more as a starting point for evaluation.PrerequisitesCreate a\\ndataset and set up the LangSmith client in Python to follow alongfrom langsmith import Clientclient =', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\35.pdf', 'page': 0}),\n",
       " Document(page_content='Client()# Create a datasetexamples = [    (\"Ankush\", \"Hello Ankush\"),    (\"Harrison\", \"Hello\\nHarrison\"),]dataset_name = \"Hello Set\"dataset =\\nclient.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"input\": input},\\n{\"expected\": expected}) for input, expected in examples])client.create_examples(inputs=inputs,\\noutputs=outputs, dataset_id=dataset.id)Use question and answer (correctness)\\nevaluators?Question and answer (QA) evaluators help to measure the correctness of a response to\\na user query or question. If you have a dataset with reference labels or reference context docs,\\nthese are the evaluators for you!\\nThree QA evaluators you can load are: \"qa\", \"context_qa\", \"cot_qa\". Based on our meta-evals, we\\nrecommend using \"cot_qa\", or Chain of Thought QA.Here is a trivial example that uses a \"cot_qa\"\\nevaluator to evaluate a simple pipeline that prefixes the input with \"Hello\":from langsmith import\\nClientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecot_qa_evaluator =\\nLangChainStringEvaluator(\"cot_qa\")client = Client()evaluate(    lambda input: \"Hello \" +\\ninput[\"input\"],    data=dataset_name,    evaluators=[cot_qa_evaluator],)Use criteria evaluators?If you\\ndon\\'t have ground truth reference labels, you can evaluate your run against a custom set of criteria\\nusing the \"criteria\" evaluators. These are helpful when there are high level semantic aspects of your\\nmodel\\'s output you\\'d like to monitor that aren\\'t captured by other explicit checks or rules.The\\n\"criteria\" evaluator instructs an LLM to assess if a prediction satisfies the given criteria, outputting a\\nbinary score (0 or 1) for each criterionfrom langsmith import Clientfrom langsmith.evaluation import\\nLangChainStringEvaluator, evaluatecriteria_evaluator = LangChainStringEvaluator(    \"criteria\",   \\nconfig={        \"criteria\": {            \"says_hello\": \"Does the submission say hello?\",        }    })client =\\nClient()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[       \\ncriteria_evaluator,    ],)Supported CriteriaDefault criteria are implemented for the following aspects:\\nconciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness,\\ncontroversiality, misogyny, and criminality.\\nTo specify custom criteria, write a mapping of a criterion name to its description, such as:criterion =', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\35.pdf', 'page': 1}),\n",
       " Document(page_content='{\"creativity\": \"Is this submission creative, imaginative, or novel?\"}criteria_evaluator =\\nLangChainStringEvaluator(    \"labeled_criteria\",    config={\"criteria\": criterion})Interpreting the\\nScoreEvaluation scores don\\'t have an inherent \"direction\" (i.e., higher is not necessarily better).\\nThe direction of the score depends on the criteria being evaluated. For example, a score of 1 for\\n\"helpfulness\" means that the prediction was deemed to be helpful by the model.\\nHowever, a score of 1 for \"maliciousness\" means that the prediction contains malicious content,\\nwhich, of course, is \"bad\".Use labeled criteria evaluators?If you have ground truth reference labels,\\nyou can evaluate your run against custom criteria while also providing that reference information to\\nthe LLM using the \"labeled_criteria\" or \"labeled_score_string\" evaluators.The \"labeled_criteria\"\\nevaluator instructs an LLM to assess if a prediction satisfies the criteria, taking into account the\\nreference labelThe \"labeled_score_string\" evaluator instructs an LLM to assess the prediction\\nagainst a reference label on a specified scalefrom langsmith import Clientfrom langsmith.evaluation\\nimport LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(  \\n \"labeled_criteria\",    config={        \"criteria\": {            \"helpfulness\": (                \"Is this submission\\nhelpful to the user,\"                \" taking into account the correct reference answer?\"            )        }   \\n})labeled_score_evaluator = LangChainStringEvaluator(    \"labeled_score_string\",    config={       \\n\"criteria\": {            \"accuracy\": \"How accurate is this prediction compared to the reference on a scale\\nof 1-10?\"        },        \"normalize_by\": 10,    })client = Client()evaluate(    lambda input: \"Hello \" +\\ninput[\"input\"],    data=dataset_name,    evaluators=[        labeled_criteria_evaluator,       \\nlabeled_score_evaluator    ],)Use string or embedding distance metrics?To measure the similarity\\nbetween a predicted string and a reference, you can use string distance metrics:The\\n\"string_distance\" evaluator computes a normalized string edit distance between the prediction and\\nreferenceThe \"embedding_distance\" evaluator computes the distance between the text embeddings\\nof the prediction and reference# !pip install rapidfuzzfrom langsmith.evaluation import\\nLangChainStringEvaluator, evaluatestring_distance_evaluator = LangChainStringEvaluator(   \\n\"string_distance\",    config={\"distance\": \"levenshtein\", \"normalize_score\":', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\35.pdf', 'page': 2}),\n",
       " Document(page_content='True})embedding_distance_evaluator = LangChainStringEvaluator(    \"embedding_distance\",   \\nconfig={      # Defaults to OpenAI, but you can customize which embedding provider to use:      #\\n\"embeddings\": HuggingFaceEmbeddings(model=\"distilbert-base-uncased\"),      # Can also choose\\n\"euclidean\", \"chebyshev\", \"hamming\", and \"manhattan\"        \"distance_metric\": \"cosine\",     \\n})evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[       \\nstring_distance_evaluator,        embedding_distance_evaluator,    ],)Use a custom LLM in\\noff-the-shelf evaluators?You can customize the model used for any LLM-based evaluator (criteria or\\nQA). Note that this currently requires using LangChain libraries.from langchain_openai import\\nChatOpenAIfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation\\nimport LangChainStringEvaluatoreval_llm = ChatOpenAI(temperature=0.0,\\nmodel=\"gpt-3.5-turbo\")cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", config={\"llm\":\\neval_llm})evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,   \\nevaluators=[cot_qa_evaluator],)Handle multiple input or output fields?LangChain off-the-shelf\\nevaluators work seamlessly if your input dictionary, output dictionary, or example dictionary each\\nhave single fields. If you have multiple fields, you can use the prepare_data function to extract the\\nrelevant fields for evaluation.\\nThese map the keys \"prediction\", \"reference\", and \"input\" to the correct fields in the input and output\\ndictionaries.For the below example, we have a model that outputs two fields: \"greeting\" and \"foo\".\\nWe want to evaluate the \"greeting\" field against the \"expected\" field in the output dictionary.from\\nlangsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator,\\nevaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={       \\n\"criteria\": {            \"helpfulness\": (                \"Is this submission helpful to the user,\"                \" taking\\ninto account the correct reference answer?\"            )        }    },    prepare_data=lambda run,\\nexample: {        \"prediction\": run.outputs[\"greeting\"],        \"reference\": example.outputs[\"expected\"],   \\n    \"input\": example.inputs[\"input\"],    })client = Client()evaluate(    lambda input: {\"greeting\": \"Hello \" +\\ninput[\"input\"], \"foo\": \"bar\"},    data=dataset_name,    evaluators=[        labeled_criteria_evaluator   ', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\35.pdf', 'page': 3}),\n",
       " Document(page_content='],)Was this page helpful?PreviousEvaluate on intermediate stepsNextCompare experiment\\nresultsUse question and answer (correctness) evaluatorsUse criteria evaluatorsUse labeled criteria\\nevaluatorsUse string or embedding distance metricsUse a custom LLM in off-the-shelf\\nevaluatorsHandle multiple input or output fieldsCommunityDiscordTwitterGitHubDocs\\nCodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS\\nDocsCopyright © 2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\35.pdf', 'page': 4}),\n",
       " Document(page_content='Compare experiment results | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationEvaluate an LLM ApplicationBind an evaluator to a dataset\\nin the UIRun an evaluation from the prompt playgroundEvaluate on intermediate stepsUse\\nLangChain off-the-shelf evaluators (Python only)Compare experiment resultsEvaluate an existing\\nexperimentUnit test LLM applications (Python only)Run pairwise evaluationsHuman\\nfeedbackMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesEvaluationCompare experiment resultsOn this pageCompare experiment resultsOftentimes,\\nwhen you are iterating on your LLM application (such as changing the model or the prompt), you will\\nwant to compare the results of different experiments.LangSmith supports a powerful comparison\\nview that lets you hone in on key differences, regressions, and improvements between different\\nexperiments.Open the comparison view?To open the comparison view, select two or more\\nexperiments from the \"Experiments\" tab from a given dataset page. Then, click on the \"Compare\"\\nbutton at the bottom of the page.View regressions and improvements?In the LangSmith comparison', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\36.pdf', 'page': 0}),\n",
       " Document(page_content='view, runs that regressed on your specified feedback key against your baseline experiment will be\\nhighlighted in red, while runs that improved\\nwill be highlighted in green. At the top of each column, you can see how many runs in that\\nexperiment did better and how many did worse than your baseline experiment.Filter on regressions\\nor improvements?Click on the regressions or improvements buttons on the top of each column to\\nfilter to the runs that regressed or improved in that specific experiment.Update baseline\\nexperiment?In order to track regressions, you need a baseline experiment against which to\\ncompare. This will be automatically assigned as the first experiment in your comparison, but you can\\nchange it from the dropdown at the top of the page.Select feedback key?You will also want to select\\nthe feedback key (evaluation metric) on which you would like focus on. This can be selected via\\nanother dropdown at the top. Again, one will be assigned by\\ndefault, but you can adjust as needed.Open a trace?If tracing is enabled for the evaluation run, you\\ncan click on the trace icon in the hover state of any experiment cell to open the trace view for that\\nrun. This will open up a trace in the side panel.Expand detailed view?From any cell, you can click on\\nthe expand icon in the hover state to open up a detailed view of all experiment results on that\\nparticular example input, along with feedback keys and scores.Update display settings?You can\\nadjust the display settings for comparison view by clicking on \"Display\" in the top right corner.Here,\\nyou\\'ll be able to toggle feedback, metrics, summary charts, and expand full text.Was this page\\nhelpful?PreviousUse LangChain off-the-shelf evaluators (Python only)NextEvaluate an existing\\nexperimentOpen the comparison viewView regressions and improvementsFilter on regressions or\\nimprovementsUpdate baseline experimentSelect feedback keyOpen a traceExpand detailed\\nviewUpdate display settingsCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\36.pdf', 'page': 1}),\n",
       " Document(page_content='', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\36.pdf', 'page': 2}),\n",
       " Document(page_content='Evaluate an existing experiment | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationEvaluate an LLM ApplicationBind an evaluator to a dataset\\nin the UIRun an evaluation from the prompt playgroundEvaluate on intermediate stepsUse\\nLangChain off-the-shelf evaluators (Python only)Compare experiment resultsEvaluate an existing\\nexperimentUnit test LLM applications (Python only)Run pairwise evaluationsHuman\\nfeedbackMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesEvaluationEvaluate an existing experimentOn this pageEvaluate an existing\\nexperimentnoteCurrently, evaluate_existing is only supported in the Python SDK.If you have already\\nrun an experiment and want to add additional evaluation metrics, you\\ncan apply any evaluators to the experiment using the evaluate_existing method.from\\nlangsmith.evaluation import evaluate_existingdef always_half(run, example):    return {\"score\":\\n0.5}experiment_name = \"my-experiment:abcd123\" # Replace with an actual experiment name or\\nIDevaluate_existing(experiment_name, evaluators=[always_half])Example?Suppose you are', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\37.pdf', 'page': 0}),\n",
       " Document(page_content='evaluating a semantic router. You may first run an experiment:from langsmith.evaluation import\\nevaluatedef semantic_router(inputs: dict):    return {\"class\": 1}def accuracy(run, example):   \\nprediction = run.outputs[\"class\"]    expected = example.outputs[\"label\"]    return {\"score\": prediction\\n== expected}results = evaluate(semantic_router, data=\"Router Classification Dataset\",\\nevaluators=[accuracy])experiment_name = results.experiment_nameLater, you realize you want to\\nadd precision and recall summary metrics. The evaluate_existing method accepts the same\\narguments as the evaluate method, replacing the target system with the experiment you wish to add\\nmetrics to, meaning\\nyou can add both instance-level evaluator\\'s and aggregate summary_evaluator\\'s.from\\nlangsmith.evaluation import evaluate_existingdef precision(runs: list, examples: list):    true_positives\\n= sum([1 for run, example in zip(runs, examples) if run.outputs[\"class\"] == example.outputs[\"label\"]])\\n   false_positives = sum([1 for run, example in zip(runs, examples) if run.outputs[\"class\"] !=\\nexample.outputs[\"label\"]])    return {\"score\": true_positives / (true_positives + false_positives)}def\\nrecall(runs: list, examples: list):    true_positives = sum([1 for run, example in zip(runs, examples) if\\nrun.outputs[\"class\"] == example.outputs[\"label\"]])    false_negatives = sum([1 for run, example in\\nzip(runs, examples) if run.outputs[\"class\"] != example.outputs[\"label\"]])    return {\"score\":\\ntrue_positives / (true_positives + false_negatives)}evaluate_existing(experiment_name,\\nsummary_evaluators=[precision, recall])The precision and recall metrics will now be available in the\\nLangSmith UI for the experiment_name experiment.As is the case with the evaluate function, there\\nis an identical, asynchronous aevaluate_existing function that can be used to evaluate experiments\\nasynchronously.Was this page helpful?PreviousCompare experiment resultsNextUnit test LLM\\napplications (Python only)ExampleCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\37.pdf', 'page': 1}),\n",
       " Document(page_content='', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\37.pdf', 'page': 2}),\n",
       " Document(page_content='Unit test LLM applications (Python only) | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationEvaluate an LLM ApplicationBind an evaluator to a dataset\\nin the UIRun an evaluation from the prompt playgroundEvaluate on intermediate stepsUse\\nLangChain off-the-shelf evaluators (Python only)Compare experiment resultsEvaluate an existing\\nexperimentUnit test LLM applications (Python only)Run pairwise evaluationsHuman\\nfeedbackMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesEvaluationUnit test LLM applications (Python only)On this pageUnit test LLM applications\\n(Python only)LangSmith unit tests are assertions and expectations designed to quickly identify\\nobvious bugs and regressions in your AI system. Relative to evaluations, tests are designed to be\\nfast and cheap to run, focusing on specific functionality and edge cases.\\nWe recommend using LangSmith to track any unit tests that touch an LLM or other non-deterministic\\npart of your AI system.\\nThese should run on every commit in your CI pipeline to catch regressions early.note@unit currently', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\38.pdf', 'page': 0}),\n",
       " Document(page_content='requires langsmith python version >=0.1.42. If you are interested in unit testing functionality in\\nTypeScript or other languages, please let us know at support@langchain.dev.  Write a @unit\\ntest?To write a LangSmith unit test, decorate your test function with @unit.\\nIf you want to track the full nested trace of the system or component being tested, you can mark\\nthose functions with @traceable. For example:# my_app/main.pyfrom langsmith import\\ntraceable@traceable # Optionaldef generate_sql(user_query):    # Replace with your SQL\\ngeneration logic    # e.g., my_llm(my_prompt.format(user_query))    return \"SELECT * FROM\\ncustomers\"Then define your unit test:# tests/test_my_app.pyfrom langsmith import unitfrom\\nmy_app.main import generate_sql@unitdef test_sql_generation_select_all():    user_query = \"Get all\\nusers from the customers table\"    sql = generate_sql(user_query)    # LangSmith logs any exception\\nraised by `assert` / `pytest.fail` / `raise` / etc.    # as a test failure    assert sql == \"SELECT * FROM\\ncustomers\"Run tests?You can use a standard unit testing framework such as pytest (docs) to run.\\nFor example:pytest tests/Each time you run this test suite, LangSmith collects the pass/fail rate and\\nother traces as a new TestSuiteResult, logging the pass rate (1 for pass, 0 for fail) over all the\\napplicable tests.The test suite syncs to a corresponding dataset named after your package or github\\nrepository.Going further?@unit is designed to stay out of your way and works well with familiar\\npytest features. For example:Defining inputs as fixtures?Pytest fixtures let you define functions that\\nserve as reusable inputs for your tests. LangSmith automatically syncs any test case inputs defined\\nas fixtures. For example:import pytest@pytest.fixturedef user_query():    return \"Get all users from\\nthe customers table\"@pytest.fixturedef expected_sql():    return \"SELECT * FROM customers\"#\\noutput_keys indicate which test arguments to save as \\'outputs\\' in the dataset (Optional)# Otherwise,\\nall arguments are saved as \\'inputs\\'@unit(output_keys=[\"expected_sql\"])def\\ntest_sql_generation_with_fixture(user_query, expected_sql):    sql = generate_sql(user_query)   \\nassert sql == expected_sqlParametrizing tests?Parametrizing tests lets you run the same assertions\\nacross multiple sets of inputs. Use pytest\\'s parametrize decorator to achieve this. For\\nexample:@unit@pytest.mark.parametrize(    \"user_query, expected_sql\",    [        (\"Get all users', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\38.pdf', 'page': 1}),\n",
       " Document(page_content='from the customers table\", \"SELECT * FROM customers\"),        (\"Get all users from the orders\\ntable\", \"SELECT * FROM orders\"),    ],)def test_sql_generation_parametrized(user_query,\\nexpected_sql):    sql = generate_sql(user_query)    assert sql == expected_sqlNote: as the\\nparametrized list grows, you may consider using evaluate() instead. This parallelizes the evaluation\\nand makes it easier to control individual experiments and the corresponding\\ndataset.Expectations?LangSmith provides an expect utility to help define expectations about your\\nLLM output. For example:from langsmith import expect@unitdef test_sql_generation_select_all():   \\nuser_query = \"Get all users from the customers table\"    sql = generate_sql(user_query)   \\nexpect(sql).to_contain(\"customers\")This will log the binary \"expectation\" score to the experiment\\nresults, additionally asserting that the expectation is met possibly triggering a test failure.expect also\\nprovides \"fuzzy match\" methods. For example:@unit@pytest.mark.parametrize(    \"query,\\nexpectation\",    [       (\"what\\'s the capital of France?\", \"Paris\"),    ],)def\\ntest_embedding_similarity(query, expectation):    prediction = my_chatbot(query)   \\nexpect.embedding_distance(        # This step logs the distance as feedback for this run       \\nprediction=prediction, expectation=expectation    # Adding a matcher (in this case, \\'to_be_*\"), logs\\n\\'expectation\\' feedback    ).to_be_less_than(0.5) # Optional predicate to assert against   \\nexpect.edit_distance(        # This computes the normalized Damerau-Levenshtein distance between\\nthe two strings        prediction=prediction, expectation=expectation    # If no predicate is provided\\nbelow, \\'assert\\' isn\\'t called, but the score is still logged    )This test case will be assigned 4\\nscores:The embedding_distance between the prediction and the expectationThe binary expectation\\nscore (1 if cosine distance is less than 0.5, 0 if not)The edit_distance between the prediction and the\\nexpectationThe overall test pass/fail score (binary)The expect utility is modeled off of Jest\\'s expect\\nAPI, with some off-the-shelf functionality to make it easier to grade your LLMs.Dry-run mode?If you\\nwant to run the tests without syncing the results to LangSmith, you can set\\nLANGCHAIN_TEST_TRACKING=false in your environment.LANGCHAIN_TEST_TRACKING=false\\npytest tests/The tests will run as normal, but the experiment logs will not be sent to', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\38.pdf', 'page': 2}),\n",
       " Document(page_content='LangSmith.Caching?LLMs on every commit in CI can get expensive. To save time and resources,\\nLangSmith lets you cache results to disk. Any identical inputs will be loaded from the cache so you\\ndon\\'t have to call out to your LLM provider unless there are changes to the model, prompt, or\\nretrieved data.To enable caching, run with LANGCHAIN_TEST_CACHE=/my/cache/path. For\\nexample:LANGCHAIN_TEST_CACHE=tests/cassettes pytest tests/my_llm_testsAll requests will be\\ncached to tests/cassettes and loaded from there on subsequent runs. If you check this in to your\\nrepository, your CI will be able to use the cache as well.Using watch mode?With caching enabled,\\nyou can iterate quickly on your tests using watch mode without worrying about unnecessarily hitting\\nyour LLM provider. For example, using pytest-watch:pip install\\npytest-watchLANGCHAIN_TEST_CACHE=tests/cassettes ptw tests/my_llm_testsExplanations?The\\n@unit test decorator converts any unit test into a parametrized LangSmith example. By default, all\\nunit tests within a given file will be grouped as a single \"test suite\" with a corresponding dataset.The\\nfollowing metrics are available off-the-shelf:FeedbackDescriptionExamplepassBinary pass/fail score,\\n1 for pass, 0 for failassert False # FailsexpectationBinary expectation score, 1 if expectation is met,\\n0 if notexpect(prediction).against(lambda x:\\nre.search(r\"\\\\b[a-f\\\\d]{8}-[a-f\\\\d]{4}-[a-f\\\\d]{4}-[a-f\\\\d]{4}-[a-f\\\\d]{12}\\\\b\", x) )embedding_distanceCosine\\ndistance between two embeddingsexpect.embedding_distance(prediction=prediction,\\nexpectation=expectation)edit_distanceEdit distance between two\\nstringsexpect.edit_distance(prediction=prediction, expectation=expectation)You can also log any\\narbitrary feeback within a unit test manually using the client.from langsmith import unit, Clientfrom\\nlangsmith.run_helpers import get_current_run_treeclient = Client()@unitdef test_foo():    run_tree =\\nget_current_run_tree()    client.create_feedback(run_id=run_tree.id, key=\"my_custom_feedback\",\\nscore=1)Reference?expect?expect makes it easy to make approximate assertions on test results\\nand log scores to LangSmith.\\nOff-the-shelf, it allows you to compute and compare embedding distances, edit distances, and make\\ncustom assertions on values.expect.embedding_distance(prediction, reference, *,', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\38.pdf', 'page': 3}),\n",
       " Document(page_content='config=None)?Compute the embedding distance between the prediction and reference.This logs the\\nembedding distance to LangSmith and returns a Matcher instance for making assertions on the\\ndistance value.By default, this uses the OpenAI API for computing\\nembeddings.Parametersprediction (str): The predicted string to compare.reference (str): The\\nreference string to compare against.config (Optional[EmbeddingConfig]): Optional configuration for\\nthe embedding distance evaluator. Supported options:encoder: A custom encoder function to\\nencode the list of input strings to embeddings. Defaults to the OpenAI API.metric: The distance\\nmetric to use for comparison. Supported values: \"cosine\", \"euclidean\", \"manhattan\", \"chebyshev\",\\n\"hamming\".ReturnsA Matcher instance for the embedding distance\\nvalue.expect.edit_distance(prediction, reference, *, config=None)?Compute the string distance\\nbetween the prediction and reference.This logs the string distance (Damerau-Levenshtein) to\\nLangSmith and returns a Matcher instance for making assertions on the distance value.This\\ndepends on the rapidfuzz package for string distance computation.Parametersprediction (str): The\\npredicted string to compare.reference (str): The reference string to compare against.config\\n(Optional[EditDistanceConfig]): Optional configuration for the string distance evaluator. Supported\\noptions:metric: The distance metric to use for comparison. Supported values:\\n\"damerau_levenshtein\", \"levenshtein\", \"jaro\", \"jaro_winkler\", \"hamming\", \"indel\".normalize_score:\\nWhether to normalize the score between 0 and 1.ReturnsA Matcher instance for the string distance\\nvalue.expect.value(value)?Create a Matcher instance for making assertions on the given\\nvalue.Parametersvalue (Any): The value to make assertions on.ReturnsA Matcher instance for the\\ngiven value.Matcher?A class for making assertions on expectation\\nvalues.to_be_less_than(value)Assert that the expectation value is less than the given\\nvalue.to_be_greater_than(value) Assert that the expectation value is greater than the given\\nvalue.to_be_between(min_value, max_value)Assert that the expectation value is between the given\\nmin and max values.to_be_approximately(value, precision=2)Assert that the expectation value is\\napproximately equal to the given value.to_equal(value)Assert that the expectation value equals the', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\38.pdf', 'page': 4}),\n",
       " Document(page_content=\"given value.to_contain(value)Assert that the expectation value contains the given\\nvalue.against(func)Assert the expectation value against a custom function.unit API?The unit\\ndecorator is used to mark a function as a test case for LangSmith. It ensures that the necessary\\nexample data is created and associated with the test function. The decorated function will be\\nexecuted as a test case, and the results will be recorded and reported by\\nLangSmith.@unit(id=None, output_keys=None, client=None, test_suite_name=None)?Create a unit\\ntest case in LangSmith.Parametersid (Optional[uuid.UUID]): A unique identifier for the test case. If\\nnot provided, an ID will be generated based on the test function's module and name.output_keys\\n(Optional[Sequence[str]]): A list of keys to be considered as the output keys for the test case. These\\nkeys will be extracted from the test function's inputs and stored as the expected outputs.client\\n(Optional[ls_client.Client]): An instance of the LangSmith client to be used for communication with\\nthe LangSmith service. If not provided, a default client will be used.test_suite_name (Optional[str]):\\nThe name of the test suite to which the test case belongs. If not provided, the test suite name will be\\ndetermined based on the environment or the package name.Environment\\nVariablesLANGSMITH_TEST_TRACKING: Set this variable to the path of a directory to enable\\ncaching of test results. This is useful for re-running tests without re-executing the code. Requires the\\n'langsmith[vcr]' package.Was this page helpful?PreviousEvaluate an existing experimentNextRun\\npairwise evaluationsWrite a @unit testRun testsGoing furtherExplanationsReferenceexpectunit\\nAPICommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\38.pdf', 'page': 5}),\n",
       " Document(page_content='Run pairwise evaluations | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationEvaluate an LLM ApplicationBind an evaluator to a dataset\\nin the UIRun an evaluation from the prompt playgroundEvaluate on intermediate stepsUse\\nLangChain off-the-shelf evaluators (Python only)Compare experiment resultsEvaluate an existing\\nexperimentUnit test LLM applications (Python only)Run pairwise evaluationsHuman\\nfeedbackMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesEvaluationRun pairwise evaluationsOn this pageRun pairwise evaluationsRecommended\\nReadingBefore diving into this content, it might be helpful to read the following:How-to guide on\\nrunning regular evalsLangSmith supports evaluating existing experiments in a comparative manner.\\nThis allows you to use automatic evaluators (especially, LLM-based evaluators) to score the outputs\\nfrom multiple experiments against each other, rather than being confined to evaluating outputs one\\nat a time. Think LMSYS Chatbot Arena - this is the same concept! To do this, use the\\nevaluate_comparative / evaluateComparative function', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\39.pdf', 'page': 0}),\n",
       " Document(page_content=\"with two existing experiments.If you haven't already created experiments to compare, check out our\\nquick start or oue how-to guide to get started with evaluations.Use the evaluate_comparative\\nfunction?notePairwise evaluations currently require langsmith SDK Python version >=0.1.55 or JS\\nversion >=0.1.24.At its simplest, evaluate_comparative / evaluateComparative function takes the\\nfollowing arguments:experiments: A list of the two existing experiments you would like to evaluate\\nagainst each other. These can be uuids or experiment names.evaluators: A list of the pairwise\\nevaluators that you would like to attach to this evaluation. See the section below for how to define\\nthese.Along with these, you can also pass in the following optional args:randomize_order /\\nrandomizeOrder: An optional boolean indicating whether the order of the outputs should be\\nrandomized for each evaluation. This is a strategy for minimizing positional bias in your prompt:\\noften, the LLM will be biased\\ntowards one of the responses based on the order. This should mainly be addressed via prompt\\nengineering, but this is another optional mitigation. Defaults to False.experiment_prefix /\\nexperimentPrefix: A prefix to be attached to the beginning of the pairwise experiment name.\\nDefaults to None.description: A description of the pairwise experiment. Defaults to\\nNone.max_concurrency / maxConcurrency: The maximum number of concurrent evaluations to run.\\nDefaults to 5.client: The LangSmith client to use. Defaults to None.metadata: Metadata to attach to\\nyour pairwise experiment. Defaults to None.load_nested / loadNested: Whether to load all child runs\\nfor the experiment. When False, only the root trace will be passed to your evaluator. Defaults to\\nFalse.Configure inputs and outputs for pairwise evaluators?Inputs: A list of Runs and a single\\nExample. This is exactly the same as a normal evaluator, except with a list of Runs instead of a\\nsingle Run. The list of runs will have a length of two. You can access the inputs and outputs with\\nruns[0].inputs, runs[0].outputs, runs[1].inputs, runs[1].outputs, example.inputs, and\\nexample.outputs.Output: Your evaluator should return a dictionary with two keys:key, which\\nrepresents the feedback key that will be loggedscores, which is a mapping from run ID to score for\\nthat run. We strongly encourage using 0 and 1 as the score values, where 1 is better. You may also\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\39.pdf', 'page': 1}),\n",
       " Document(page_content='set both to 0 to represent \"both equally bad\" or both to 1 for \"both equally good\".Note that you\\nshould choose a feedback key that is distinct from standard feedbacks on your run. We recommend\\nprefixing pairwise feedback keys with pairwise_ or ranked_.Compare two experiments with\\nLLM-based pairwise evaluators?The following example uses a prompt\\nwhich asks the LLM to decide which is better between two AI assistant responses. It uses structured\\noutput to parse the AI\\'s response: 0, 1, or 2.Optional LangChain UsageIn the Python example\\nbelow, we are pulling this structured prompt from the LangChain Hub and using it with a LangChain\\nLLM wrapper.\\nThe prompt asks the LLM to decide which is better between two AI assistant responses. It uses\\nstructured output to parse the AI\\'s response: 0, 1, or 2.Usage of LangChain is totally optional. To\\nillustrate this point, the TypeScript example below uses the OpenAI API\\ndirectly.PythonTypeScriptfrom langsmith.evaluation import evaluate_comparativefrom langchain\\nimport hubfrom langchain_openai import ChatOpenAIfrom langsmith.schemas import Run,\\nExampleprompt = hub.pull(\"langchain-ai/pairwise-evaluation-2\")def evaluate_pairwise(runs:\\nlist[Run], example: Example):    scores = {}        # Create the model to run your evaluator    model =\\nChatOpenAI(model_name=\"gpt-4\")        runnable = prompt | model    response = runnable.invoke({   \\n    \"question\": example.inputs[\"question\"],        \"answer_a\": runs[0].outputs[\"output\"] if runs[0].outputs\\nis not None else \"N/A\",        \"answer_b\": runs[1].outputs[\"output\"] if runs[1].outputs is not None else\\n\"N/A\",    })    score = response[\"Preference\"]    if score == 1:        scores[runs[0].id] = 1       \\nscores[runs[1].id] = 0    elif score == 2:        scores[runs[0].id] = 0        scores[runs[1].id] = 1    else:    \\n   scores[runs[0].id] = 0        scores[runs[1].id] = 0    return {\"key\": \"ranked_preference\", \"scores\":\\nscores}        evaluate_comparative(    # Replace the following array with the names or IDs of your\\nexperiments    [\"my-experiment-name-1\", \"my-experiment-name-2\"],   \\nevaluators=[evaluate_pairwise],)Note: LangChain support inside evaluate / evaluateComparative is\\nnot supported yet. See this issue for more details.\\nimport type { Run, Example } from \"langsmith\";import { evaluateComparative } from', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\39.pdf', 'page': 2}),\n",
       " Document(page_content='\"langsmith/evaluation\";import { wrapOpenAI } from \"langsmith/wrappers\";import OpenAI from\\n\"openai\";const openai = wrapOpenAI(new OpenAI());import { z } from \"zod\";async function\\nevaluatePairwise(runs: Run[], example: Example) {  const scores: Record<string, number> = {}; \\nconst [runA, runB] = runs;    if (!runA || !runB) throw new Error(\"Expected at least two runs\");    const\\npayload = {    question: example.inputs?.question,    answer_a: runA?.outputs?.output ?? \"N/A\",   \\nanswer_b: runB?.outputs?.output ?? \"N/A\",  };    const output = await\\nopenai.chat.completions.create({    model: \"gpt-4-turbo\",    messages: [      {        role: \"system\",       \\ncontent: [          \"Please act as an impartial judge and evaluate the quality of the responses provided\\nby two AI assistants to the user question displayed below.\",          \"You should choose the assistant\\nthat follows the user\\'s instructions and answers the user\\'s question better.\",          \"Your evaluation\\nshould consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of\\ndetail of their responses.\",          \"Begin your evaluation by comparing the two responses and\\nprovide a short explanation.\",          \"Avoid any position biases and ensure that the order in which the\\nresponses were presented does not influence your decision.\",          \"Do not allow the length of the\\nresponses to influence your evaluation. Do not favor certain names of the assistants. Be as\\nobjective as possible.\",        ].join(\" \"),      },      {        role: \"user\",        content: [          `[User\\nQuestion] ${payload.question}`,          `[The Start of Assistant A\\'s Answer] ${payload.answer_a} [The\\nEnd of Assistant A\\'s Answer]`,          `The Start of Assistant B\\'s Answer] ${payload.answer_b} [The\\nEnd of Assistant B\\'s Answer]`,        ].join(\"\\\\n\\\\n\"),      },    ],    tool_choice: {      type: \"function\",     \\nfunction: { name: \"Score\" },    },    tools: [      {        type: \"function\",        function: {          name:\\n\"Score\",          description: [            `After providing your explanation, output your final verdict by\\nstrictly following this format:`,            `Output \"1\" if Assistant A answer is better based upon the\\nfactors above.`,            `Output \"2\" if Assistant B answer is better based upon the factors above.`,     \\n      `Output \"0\" if it is a tie.`,          ].join(\" \"),          parameters: {            type: \"object\",           \\nproperties: {              Preference: {                type: \"integer\",                description: \"Which assistant\\nanswer is preferred?\",              },            },          },        },      },    ],  });    const { Preference } = z   ', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\39.pdf', 'page': 3}),\n",
       " Document(page_content='.object({ Preference: z.number() })    .parse(     \\nJSON.parse(output.choices[0].message.tool_calls[0].function.arguments)    );      if (Preference ===\\n1) {    scores[runA.id] = 1;    scores[runB.id] = 0;  } else if (Preference === 2) {    scores[runA.id] = 0; \\n  scores[runB.id] = 1;  } else {    scores[runA.id] = 0;    scores[runB.id] = 0;  }    return { key:\\n\"ranked_preference\", scores };}await evaluateComparative([\"earnest-name-40\",\\n\"reflecting-pump-91\"], {  evaluators: [evaluatePairwise],});View pairwise experiments?Navigate to the\\n\"Pairwise Experiments\" tab from the dataset page:Click on a pairwise experiment that you would like\\nto inspect, and you will be brought to the Comparison View:You may filter to runs where the first\\nexperiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table\\nheader:Was this page helpful?PreviousUnit test LLM applications (Python only)NextCapture user\\nfeedback from your application to tracesUse the evaluate_comparative functionConfigure inputs and\\noutputs for pairwise evaluatorsCompare two experiments with LLM-based pairwise evaluatorsView\\npairwise experimentsCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\39.pdf', 'page': 4}),\n",
       " Document(page_content='Optimize a classifier | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsAdd observability to\\nyour LLM applicationEvaluate your LLM applicationOptimize a classifierHow-to\\nguidesConceptsReferencePricingSelf-hostingTutorialsOptimize a classifierOn this pageOptimize a\\nclassifierThis tutorial walks through optimizing a classifier based on user a feedback.\\nClassifiers are great to optimize because its generally pretty simple to collect the desired output,\\nwhich makes it easy to create few shot examples based on user feedback.\\nThat is exactly what we will do in this example.The objective?In this example, we will build a bot that\\nclassify GitHub issues based on their title.\\nIt will take in a title and classify it into one of many different classes.\\nThen, we will start to collect user feedback and use that to shape how this classifier\\nperforms.Getting started?To get started, we will first set it up so that we send all traces to a specific\\nproject.\\nWe can do this by setting an environment variable:import osos.environ[\"LANGCHAIN_PROJECT\"] =', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\4.pdf', 'page': 0}),\n",
       " Document(page_content='\"classifier\"We can then create our initial application. This will be a really simple function that just\\ntakes in a GitHub issue title and tries to label it.import openaifrom langsmith import traceable,\\nClientimport uuidclient = openai.Client()available_topics = [    \"bug\",    \"improvement\",   \\n\"new_feature\",    \"documentation\",    \"integration\",]prompt_template = \"\"\"Classify the type of the\\nissue as one of {topics}.Issue: {text}\"\"\"@traceable(    run_type=\"chain\",    name=\"Classifier\",)def\\ntopic_classifier(    topic: str):    return client.chat.completions.create(        model=\"gpt-3.5-turbo\",       \\ntemperature=0,        messages=[            {                \"role\": \"user\",                \"content\":\\nprompt_template.format(                    topics=\\',\\'.join(available_topics),                    text=topic,              \\n )            }        ],    ).choices[0].message.contentWe can then start to interact with it.\\nWhen interacting with it, we will generate the LangSmith run id ahead of time and pass that into this\\nfunction.\\nWe do this so we can attach feedback later on.Here\\'s how we can invoke the application:run_id =\\nuuid.uuid4()topic_classifier(    \"fix bug in LCEL\",    langsmith_extra={\"run_id\": run_id})Here\\'s how we\\ncan attach feedback after.\\nWe can collect feedback in two forms.First, we can collect \"positive\" feedback - this is for examples\\nthat the model got right.ls_client = Client()run_id = uuid.uuid4()topic_classifier(    \"fix bug in LCEL\",   \\nlangsmith_extra={\"run_id\": run_id})ls_client.create_feedback(    run_id,    key=\"user-score\",   \\nscore=1.0,)Next, we can focus on collecting feedback that corresponds to a \"correction\" to the\\ngeneration.\\nIn this example the model will classify it as a bug, whereas I really want this to be classified as\\ndocumentation.ls_client = Client()run_id = uuid.uuid4()topic_classifier(    \"fix bug in documentation\",  \\n langsmith_extra={\"run_id\": run_id})ls_client.create_feedback(    run_id,    key=\"correction\",   \\ncorrection=\"documentation\")Set up automations?We can now set up automations to move examples\\nwith feedback of some form into a dataset.\\nWe will set up two automations, one for positive feedback and the other for negative feedback.The\\nfirst will take all runs with positive feedback and automatically add them to a dataset.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\4.pdf', 'page': 1}),\n",
       " Document(page_content='The logic behind this is that any run with positive feedback we can use as a good example in future\\niterations.\\nLet\\'s create a dataset called classifier-github-issues to add this data to.The second will take all runs\\nwith a correction and use a webhook to add them to a dataset.\\nWhen creating this webhook, we will select the option to \"Use Corrections\".\\nThis option will make it so that when creating a dataset from a run, rather than using the output of\\nthe run\\nas the gold-truth output of the datapoint, it will use the correction.Update the application?We can\\nnow update our code to pull down the dataset we are sending runs to.\\nOnce we pull it down, we can create a string with the examples in it.\\nWe can then put this string as part of the prompt!### NEW CODE #### Initialize the LangSmith\\nClient so we can use to get the datasetls_client = Client()# Create a function that will take in a list of\\nexamples and format them into a stringdef create_example_string(examples):    final_strings = []   \\nfor e in examples:        final_strings.append(f\"Input: {e.inputs[\\'topic\\']}\\\\n> {e.outputs[\\'output\\']}\")   \\nreturn \"\\\\n\\\\n\".join(final_strings)### NEW CODE ###client = openai.Client()available_topics = [   \\n\"bug\",    \"improvement\",    \"new_feature\",    \"documentation\",    \"integration\",]prompt_template =\\n\"\"\"Classify the type of the issue as one of {topics}.Here are some examples:{examples}Begin!Issue:\\n{text}>\"\"\"@traceable(    run_type=\"chain\",    name=\"Classifier\",)def topic_classifier(    topic: str):    #\\nWe can now pull down the examples from the dataset    # We do this inside the function so it always\\nget the most up-to-date examples,    # But this can be done outside and cached for speed if desired \\n  examples = list(ls_client.list_examples(dataset_name=\"classifier-github-issues\"))  # <- New Code   \\nexample_string = create_example_string(examples)    return client.chat.completions.create(       \\nmodel=\"gpt-3.5-turbo\",        temperature=0,        messages=[            {                \"role\": \"user\",             \\n  \"content\": prompt_template.format(                    topics=\\',\\'.join(available_topics),                   \\ntext=topic,                    examples=example_string,                )            }        ],   \\n).choices[0].message.contentIf now run the application with a similar input as before, we can see', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\4.pdf', 'page': 2}),\n",
       " Document(page_content='that it correctly learns that anything related to docs (even if a bug) should be classified as\\ndocumentationls_client = Client()run_id = uuid.uuid4()topic_classifier(    \"address bug in\\ndocumentation\",    langsmith_extra={\"run_id\": run_id})Semantic search over examples?One\\nadditional thing we can do is only use the most semantically similar examples.\\nThis is useful when you start to build up a lot of examples.In order to do this, we can first define an\\nexample to find the k most similar examples:import numpy as npdef find_similar(examples, topic,\\nk=5):    inputs = [e.inputs[\\'topic\\'] for e in examples] + [topic]    embedds =\\nclient.embeddings.create(input=inputs, model=\"text-embedding-3-small\")    embedds =\\n[e.embedding for e in embedds.data]    embedds = np.array(embedds)    args =\\nnp.argsort(-embedds.dot(embedds[-1])[:-1])[:5]    examples = [examples[i] for i in args]    return\\nexamplesWe can then use that in the applicationls_client = Client()def\\ncreate_example_string(examples):    final_strings = []    for e in examples:       \\nfinal_strings.append(f\"Input: {e.inputs[\\'topic\\']}\\\\n> {e.outputs[\\'output\\']}\")    return\\n\"\\\\n\\\\n\".join(final_strings)client = openai.Client()available_topics = [    \"bug\",    \"improvement\",   \\n\"new_feature\",    \"documentation\",    \"integration\",]prompt_template = \"\"\"Classify the type of the\\nissue as one of {topics}.Here are some examples:{examples}Begin!Issue: {text}>\"\"\"@traceable(   \\nrun_type=\"chain\",    name=\"Classifier\",)def topic_classifier(    topic: str):    examples =\\nlist(ls_client.list_examples(dataset_name=\"classifier-github-issues\"))    examples =\\nfind_similar(examples, topic)    example_string = create_example_string(examples)    return\\nclient.chat.completions.create(        model=\"gpt-3.5-turbo\",        temperature=0,        messages=[       \\n    {                \"role\": \"user\",                \"content\": prompt_template.format(                   \\ntopics=\\',\\'.join(available_topics),                    text=topic,                    examples=example_string,          \\n     )            }        ],    ).choices[0].message.contentWas this page helpful?PreviousEvaluate your\\nLLM applicationNextHow-to guidesThe objectiveGetting startedSet up automationsUpdate the\\napplicationSemantic search over examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\4.pdf', 'page': 3}),\n",
       " Document(page_content='2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\4.pdf', 'page': 4}),\n",
       " Document(page_content='Capture user feedback from your application to traces | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationHuman feedbackCapture user feedback from your\\napplication to tracesSet up feedback criteriaAnnotate traces and runs inlineUse annotation\\nqueuesMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesHuman feedbackCapture user feedback from your application to tracesCapture user feedback\\nfrom your application to tracesRecommended ReadingBefore diving into this content, it might be\\nhelpful to read the following:Conceptual guide on tracing and feedbackReference guide on feedback\\ndata formatIn many applications, but even more so for LLM applications, it is important to collect\\nuser feedback to understand how your application is performing in real-world scenarios.\\nThe ability to observe user feedback along with trace data can be very powerful to drill down into the\\nmost interesting datapoints, then send those datapoints for further review, automatic evaluation, or\\neven datasets.\\nTo learn more about how to filter traces based on various attributes, including user feedback, see', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\40.pdf', 'page': 0}),\n",
       " Document(page_content='this guideLangSmith makes it easy to attach user feedback to traces.\\nIt\\'s often helpful to expose a simple mechanism (such as a thumbs-up, thumbs-down button) to\\ncollect user feedback for your application responses. You can then use the LangSmith SDK or API\\nto send feedback for a trace. To get the run_id of a logged run, see this guide.noteYou can attach\\nuser feedback to ANY intermediate run (span) of the trace, not just the root span.\\nThis is useful for critiquing specific parts of the LLM application, such as the retrieval step or\\ngeneration step of the RAG pipeline.PythonTypeScriptfrom langsmith import Clientclient = Client()#\\n... Run your application and get the run_id...# This information can be the result of a user-facing\\nfeedback formclient.create_feedback(    run_id,    key=\"feedback-key\",    score=1.0,   \\ncomment=\"comment\",)import { Client } from \"langsmith\";const client = new Client();// ... Run your\\napplication and get the run_id...// This information can be the result of a user-facing feedback\\nformawait client.createFeedback(    runId,    \"feedback-key\",    {        score: 1.0,        comment:\\n\"comment\",    });Was this page helpful?PreviousRun pairwise evaluationsNextSet up feedback\\ncriteriaCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\40.pdf', 'page': 1}),\n",
       " Document(page_content='Set up feedback criteria | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationHuman feedbackCapture user feedback from your\\napplication to tracesSet up feedback criteriaAnnotate traces and runs inlineUse annotation\\nqueuesMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesHuman feedbackSet up feedback criteriaOn this pageSet up feedback criteriaRecommended\\nReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on\\ntracing and feedbackReference guide on feedback data formatFeedback criteria are represented in\\nthe application as feedback tags. For human feedback, you can set up new feedback criteria as\\ncontinuous feedback or categorical feedback.To set up a new feedback criteria, follow this link to\\nview all existing tags for your workspace, then click New Tag.Continuous feedback?For continuous\\nfeedback, you can enter a feedback tag name, then select a minimum and maximum value. Every\\nvalue, including floating-point numbers, within this range will be accepted as feedback\\nscores.Categorical feedback?For categorical feedback, you can enter a feedback tag name, then', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\41.pdf', 'page': 0}),\n",
       " Document(page_content='add a list of categories, each category mapping to a score. When you provide feedback, you can\\nselect one of these categories as the feedback score.\\nBoth the category label and the score will be logged as feedback in value and score fields,\\nrespectively.Was this page helpful?PreviousCapture user feedback from your application to\\ntracesNextAnnotate traces and runs inlineContinuous feedbackCategorical\\nfeedbackCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\41.pdf', 'page': 1}),\n",
       " Document(page_content=\"Annotate traces and runs inline | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationHuman feedbackCapture user feedback from your\\napplication to tracesSet up feedback criteriaAnnotate traces and runs inlineUse annotation\\nqueuesMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesHuman feedbackAnnotate traces and runs inlineAnnotate traces and runs inlineLangSmith\\nallows you to manually annotate traces with feedback within the application. This can be useful for\\nadding context to a trace, such as a user's comment or a note about a specific issue.\\nYou can annotate a trace either inline or by sending the trace to an annotation queue, which allows\\nyou closely inspect and log feedbacks to runs one at a time.\\nFeedback tags are associated with your tenant (soon to be workspace).noteYou can attach user\\nfeedback to ANY intermediate run (span) of the trace, not just the root span.\\nThis is useful for critiquing specific parts of the LLM application, such as the retrieval step or\\ngeneration step of the RAG pipeline.To annotate a trace inline, click on the Annotate in the upper\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\42.pdf', 'page': 0}),\n",
       " Document(page_content='right corner of trace view for any particular run that is part of the trace.This will open up a pane that\\nallows you to choose from feedback tags associated with your workspace and add a score for\\nparticular tags. You can also add a standalone comment. Follow this guide to set up feedback tags\\nfor your workspace.\\nYou can also set up new feedback criteria from within the pane itself.You can use the labeled\\nkeyboard shortcuts to streamline the annotation process.Was this page helpful?PreviousSet up\\nfeedback criteriaNextUse annotation queuesCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\42.pdf', 'page': 1}),\n",
       " Document(page_content='Use annotation queues | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationHuman feedbackCapture user feedback from your\\napplication to tracesSet up feedback criteriaAnnotate traces and runs inlineUse annotation\\nqueuesMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesHuman feedbackUse annotation queuesOn this pageUse annotation queuesAnnotation\\nqueues are a powerful LangSmith feature that provide a streamlined, directed view for human\\nannotators to attach feedback to specific runs.\\nWhile you can always annotate runs inline, annotation queues provide another option to group runs\\ntogether, then have annotators review and provide feedback on them.Create an annotation\\nqueue?To create an annotation queue, navigate to the Annotation queues section through the\\nhomepage or left-hand navigation bar.\\nThen click + New annotation queue in the top right corner.Fill in the form with the name and\\ndescription of the queue. You can also assign a default dataset to queue, which will streamline the', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\43.pdf', 'page': 0}),\n",
       " Document(page_content='process of sending the inputs and outputs of certain runs to datasets in your LangSmith\\nworkspace.Assign runs to an annotation queue?To assign runs to an annotation queue, either:Click\\non Add to Annotation Queue in top right corner of any trace view. You can add ANY intermediate\\nrun (span) of the trace to an annotation queue, not just the root span.\\nSelect multiple runs in the runs table then click Add to Annotation Queue at the bottom of the page.\\nSet up an automation rule that automatically assigns runs which pass a certain filter and sampling\\ncondition to an annotation queue.tipIt is often a very good idea to assign runs that have a certain\\nuser feedback score (eg thumbs up, thumbs down) from the application to an annotation queue.\\nThis way, you can identify and address issues that are causing user dissatisfaction.\\nTo learn more about how to capture user feedback from your LLM application, follow this\\nguide.Review runs in an annotation queue?To review runs in an annotation queue, navigate to the\\nAnnotation Queues section through the homepage or left-hand navigation bar.\\nThen click on the queue you want to review. This will take you to a focused, cyclical view of the runs\\nin the queue that require review.You can attach a comment, attach a score for a particular feedback\\ncriteria, add the run a dataset and/or mark the run as reviewed.The keyboard shortcuts shown can\\nhelp streamline the review process.Was this page helpful?PreviousAnnotate traces and runs\\ninlineNextFilter traces in the applicationCreate an annotation queueAssign runs to an annotation\\nqueueReview runs in an annotation queueCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\43.pdf', 'page': 1}),\n",
       " Document(page_content='Filter traces in the application | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationHuman feedbackMonitoring and automationsFilter traces in\\nthe applicationUse monitoring chartsSet up automation rulesOnline EvaluationSet up threadsSet up\\nwebhook notifications for rules (beta)PromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesMonitoring and automationsFilter traces in the applicationOn this pageFilter traces in the\\napplicationRecommended readingBefore diving into this content, it might be helpful to read the\\nfollowing to gain familiarity with the concepts mentioned here:Conceptual guide on tracingThis page\\ncontains a series of guides for how to filter runs in the application. For a guide on how to accomplish\\nsomething similar programmatically, please see this guide.\\nBeing able to accurately filter runs is important for both manual inspection and setting up\\nautomations.Create a filter?There are two ways to create a filter.\\nFirst, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot\\nis true. This restricts all runs to be top level traces.You can also define a filter from the Filter', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\44.pdf', 'page': 0}),\n",
       " Document(page_content='Shortcuts on the sidebar. This contains commonly used filters.Filter for intermediate runs (spans)?In\\norder to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is\\ntrue. After that, you can apply any filter you wish. A common way to do this is to filter by name for\\nsub runs.\\nThis relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check\\nout this guideAdvanced: filter for intermediate runs (spans) on properties of the root?A common\\nconcept is to filter for intermediate runs which are part of a trace whose root run has some attribute.\\nAn example is filtering for intermediate runs of a particular type whose root run has positive (or\\nnegative) feedback associated with it.In order to do this, first set up a filter for intermediate runs (per\\nthe above section). After that, you can then add another filter rule. You can then click the Advanced\\nFilters link all the way at the bottom of the filter. This will open up a new modal where you can add\\nTrace filters. These filters will apply to the traces of all the parent runs of the individual runs you\\'ve\\nalready filtered for.Advanced: filter for runs (spans) whose child runs have some attribute?This is the\\nopposite of the above. You may want to search for runs who have specific types of sub runs. An\\nexample of this could be searching for all traces that had a sub run with name Foo. This is useful\\nwhen Foo is not always called, but you want to analyze the cases where it is.In order to do this, you\\ncan click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new\\nmodal where you can add Tree filters. This will make the rule you specific apply to all child runs of\\nthe individual runs you\\'ve already filtered for.Copy the filter?Sometimes you may want to copy a\\nfilter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the\\nfuture, or use it in the SDK.In order to copy the filter, you can first create it in the UI. From there, you\\ncan click the copy button in the upper right hand corner. If you have constructed tree or trace filters,\\nyou can also copy those.This will give you a string in our query language, like and(eq(is_root, true),\\nand(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more\\ninformation on the query language.Manually specify a raw query in LangSmith query language?If\\nyou have copied a previous filter (see above) you may want to manually specify that raw query in a', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\44.pdf', 'page': 1}),\n",
       " Document(page_content='future session. You may also find it easier to modify this filter than to use the UI.In order to do this,\\nyou can click on Advanced filters on the bottom. From there you can paste a raw query into the\\nappropriate box.Note that this will add that query to the existing queries, not overwrite it.Use an AI\\nQuery to auto-generate a query?Sometimes figuring out the exact query to specify can be difficult!\\nIn order to make it easier, we\\'ve added a AI Query functionality. With this, you can type in the filter\\nyou want to construct in natural language and it will convert it into a valid query.For example: \"All\\nruns longer than 10 seconds\"Experimental featureNote that this is an experimental feature and may\\nnot work for all queries.Was this page helpful?PreviousUse annotation queuesNextUse monitoring\\nchartsCreate a filterFilter for intermediate runs (spans)Advanced: filter for intermediate runs (spans)\\non properties of the rootAdvanced: filter for runs (spans) whose child runs have some attributeCopy\\nthe filterManually specify a raw query in LangSmith query languageUse an AI Query to\\nauto-generate a queryCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\44.pdf', 'page': 2}),\n",
       " Document(page_content='Use monitoring charts | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationHuman feedbackMonitoring and automationsFilter traces in\\nthe applicationUse monitoring chartsSet up automation rulesOnline EvaluationSet up threadsSet up\\nwebhook notifications for rules (beta)PromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesMonitoring and automationsUse monitoring chartsOn this pageUse monitoring\\nchartsLangSmith has a collection of monitoring charts accessible for each tracing project. These can\\nbe accessed on the Monitor tab within a particular project.Change the time period?You can view\\nmonitors over differing time periods. This can be controlled by the tabs at the top of the page. By\\ndefault, it is set to seven days.Slice data by metadata or tag?By default, the monitor tab shows\\nresults for all runs. However, you can slice the data by metadata or tags to view specific subsets of\\nruns.\\nThis can be useful to compare how two different prompts or models are performing.In order to do\\nthis, you first need to make sure you are attaching appropriate tags or metadata to these runs when', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\45.pdf', 'page': 0}),\n",
       " Document(page_content='logging them.\\nAfter that, you can click the Tag or Metadata tab at the top to group runs accordingly.Drill down into\\nspecific subsets?Monitoring charts can be useful to idea when spikes in errors or latency may be\\noccurring. When you observe those spikes in a monitoring dashboard, you can easily drill into the\\nruns causing those issues by clicking on the dot in the dashboard.From there, you will be brought\\nback to the Traces tab, with a filter applied so you are only viewing the runs that occurred in the time\\nbucket that you clicked into.Was this page helpful?PreviousFilter traces in the applicationNextSet up\\nautomation rulesChange the time periodSlice data by metadata or tagDrill down into specific\\nsubsetsCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\45.pdf', 'page': 1}),\n",
       " Document(page_content='Set up automation rules | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationHuman feedbackMonitoring and automationsFilter traces in\\nthe applicationUse monitoring chartsSet up automation rulesOnline EvaluationSet up threadsSet up\\nwebhook notifications for rules (beta)PromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesMonitoring and automationsSet up automation rulesOn this pageSet up automation\\nrulesWhile you can manually sift through and process production logs from our LLM application, it\\noften becomes difficult as your application scales to more users.\\nLangSmith provides a powerful feature called automations that allow you to trigger certain actions\\non your trace data.\\nAt a high level, automations are defined by a filter, sampling rate, and action.Automation rules can\\ntrigger actions such as online evaluation, adding inputs/outputs of traces to a dataset, adding to an\\nannotation queue, and triggering a webhook.An example of an automation you can set up can be\\n\"trigger an online evaluation that grades on vagueness for all of my downvoted traces.\"Create a', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\46.pdf', 'page': 0}),\n",
       " Document(page_content='rule?We will outline the steps for creating an automation rule in LangSmith below.Step 1: Navigate\\nto rule creation?To create a rule, head click on Rules in the top right corner of any project details\\npage, then scroll to the bottom and click on + Add Rule.Alternatively, you can access rules in\\nsettings by navigating to this link, click on + Add Rule, then Project Rule.noteThere are currently two\\ntypes of rules you can create: Project Rule and Dataset Rule.Project Rule: This rule will apply to\\ntraces in the specified project. Actions allowed are adding to a dataset, adding to an annotation\\nqueue, running online evaluation, and triggering a webhook.Dataset Rule: This rule will apply to\\ntraces that are part of an experiment in the specified dataset. Actions allowed are only running an\\nevaluator on the experiment results. To see this in action, you can follow this guide.Give your rule a\\nname, for example \"my_rule\":Step 2: Define the filter?You can create a filter as you normally would\\nto filter traces in the project. For more information on filters, you can refer to this guide.Step 3:\\nDefine the sampling rate?You can specify a sampling rate (between 0 and 1) for automations. This\\nwill control the percent of the filtered runs that are sent to an automation action. For example, if you\\nset the sampling rate to 0.5, then 50% of the traces that pass the filter will be sent to the action.Step\\n4: Define the action?There are four actions you can take with an automation rule:Add to dataset:\\nAdd the inputs and outputs of the trace to a dataset.Add to annotation queue: Add the trace to an\\nannotation queue.Run online evaluation: Run an online evaluation on the trace. For more\\ninformation on online evaluations, you can refer to this guide.Trigger webhook: Trigger a webhook\\nwith the trace data. For more information on webhooks, you can refer to this guide.View logs for\\nyour automations?You can view logs for your automations by going to Settings -> Rules and click on\\nthe Logs button in any row.You can also get to logs by clicking on Rules in the top right hand corner\\nof any project details page, then clicking on See Logs for any rule.Logs allow you to gain confidence\\nthat your rules are working as expected. You can now view logs that list all runs processed by a\\ngiven rule for the past day. For rules that apply online evaluation scores, you can easily see the\\noutput score and navigate to the run. For rules that add runs as examples to datasets, you can view\\nthe example produced.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\46.pdf', 'page': 1}),\n",
       " Document(page_content='If a particular rule execution has triggered an error, you can view the error message by hovering\\nover the error icon.Was this page helpful?PreviousUse monitoring chartsNextOnline\\nEvaluationCreate a ruleStep 1: Navigate to rule creationStep 2: Define the filterStep 3: Define the\\nsampling rateStep 4: Define the actionView logs for your\\nautomationsCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\46.pdf', 'page': 2}),\n",
       " Document(page_content='Set up online evaluations | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationHuman feedbackMonitoring and automationsFilter traces in\\nthe applicationUse monitoring chartsSet up automation rulesOnline EvaluationSet up threadsSet up\\nwebhook notifications for rules (beta)PromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesMonitoring and automationsOnline EvaluationOn this pageSet up online\\nevaluationsRecommended ReadingBefore diving into this content, it might be helpful to read the\\nfollowing:Set up automation rulesOnline evaluations is a powerful LangSmith feature that allows you\\nto run an LLM-as-a-judge evaluator on a set of your production traces. They are implemented as a\\npossible action in an automation rule.Currently, we provide support for specifying a prompt template,\\na model, and a set of criteria to evaluate the runs on.After entering rules setup, you can select\\nOnline Evaluation from the list of possible actions:Configure online evaluations?When selection\\nOnline Evaluation as an action in an automation, you are presented with a panel from which you can\\nconfigure online evaluation.Model?You can choose any model available in the dropdown. Currently,', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\47.pdf', 'page': 0}),\n",
       " Document(page_content=\"we support OpenAI, AzureOpenAI, and models hosted on Fireworks.Prompt template?For the\\nprompt template, you can select a suggested evaluator prompt, create a new prompt, or choose a\\nprompt from the LangChain Hub.Suggested evaluator prompts?We provide a list of pre-existing\\nprompts that cater to common evaluator use cases. Once you select a prompt, you can customize it\\nin the prompt editor to work with your run schema.Create your own prompt?We also provide a\\nprompt editor where you can create your own prompt.We support two template formats:mustache:\\nanything in {{...}} is treated as a mustache variable. Mustache supports more robust templating,\\nincluding nesting variables. See the mustache documentation for more information.f-string: anything\\nin {...} is treated as an f-string variable.Evaluator prompts can only contain the following root input\\nvariables:input: the input to the target you are evaluatingoutput: the output of the target you are\\nevaluatingIn mustache templating, you can get more specific. For example, if you have a key text in\\nthe input dictionary, you can access it using {{input.text}}. When writing your prompt, you'll be shown\\nsuggestions based on your recent run schema.Previewing the prompt will show you an example of\\nwhat the formatted prompt will look like. This preview pulls the input and output of the most recent\\nrun.noteYou can configure an evaluation prompt that doesn't match the schema of your recent runs,\\nbut the dropdown suggestions and preview function won't work as expected.Pull a prompt from the\\nLangChain Hub?You can also pull a prompt from the LangChain Hub. You can pull any public\\nstructured prompt.You can't edit these prompts directly within the prompt editor, but you can view\\nthe prompt and the schema it uses.\\nIf the prompt is your own, you can edit it in the playground and commit the version.\\nIf the prompt is someone else's, you can fork their prompt, make your edits in the playground and\\nthen pull in your new prompt.Criteria?An evaluator will attach arbitrary metadata tags to a run.\\nThese tags will have a name and a value. You can configure this in the Criteria section.\\nThe names and the descriptions of the fields will be passed in to the prompt. Behind the scenes, we\\nuse tool calling to coerce the output of the LLM in to the score you specify.noteThe name of the\\ncriteria cannot have spaces since it is used as the name of a tool.Set API keys?Online evaluation\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\47.pdf', 'page': 1}),\n",
       " Document(page_content='uses LLM-as-a-judge evaluation. In order to set the API keys to use for these invocations, navigate\\nto the Settings -> Secrets -> Add secret page and add any API keys there.Was this page\\nhelpful?PreviousSet up automation rulesNextSet up threadsConfigure online\\nevaluationsModelPrompt templatePull a prompt from the LangChain HubCriteriaSet API\\nkeysCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\47.pdf', 'page': 2}),\n",
       " Document(page_content='Set up threads | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationHuman feedbackMonitoring and automationsFilter traces in\\nthe applicationUse monitoring chartsSet up automation rulesOnline EvaluationSet up threadsSet up\\nwebhook notifications for rules (beta)PromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesMonitoring and automationsSet up threadsOn this pageSet up threadsRecommended\\nReadingBefore diving into this content, it might be helpful to read the following:Add metadata and\\ntags to tracesMany LLM applications have a chatbot-like interface in which the user and the LLM\\napplication engage in a multi-turn conversation. In order to track these conversations, you can use\\nthe Threads feature in LangSmith.Group traces into threads?A Thread is a sequence of traces\\nrepresenting a single conversation. Each response is represented as its own trace, but these traces\\nare linked together by being part of the same thread.To associate traces together, you need to pass\\nin a special metadata key where the value is the unique identifier for that thread.The key value is the\\nunique identifier for that conversation.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\48.pdf', 'page': 0}),\n",
       " Document(page_content='The key name should be one of:session_idthread_idconversation_id.The value should be a UUID,\\nsuch as f47ac10b-58cc-4372-a567-0e02b2c3d479.View threads?You can view threads by clicking\\non the Threads tad in any project details page. You will then see a list of all threads, sorted by the\\nmost recent activity.You can then click into a particular thread. This will open the history for a\\nparticular thread. If your threads are formatted as chat messages, you will a chatbot-like UI where\\nyou can see a history of inputs and outputs.You can open up the trace or annotate the trace in a\\nside panel by clicking on Annotate and Open trace, respectively.Was this page\\nhelpful?PreviousOnline EvaluationNextSet up webhook notifications for rules (beta)Group traces\\ninto threadsView threadsCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\48.pdf', 'page': 1}),\n",
       " Document(page_content='Set up webhook notifications for rules (beta) | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationHuman feedbackMonitoring and automationsFilter traces in\\nthe applicationUse monitoring chartsSet up automation rulesOnline EvaluationSet up threadsSet up\\nwebhook notifications for rules (beta)PromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesMonitoring and automationsSet up webhook notifications for rules (beta)On this pageSet up\\nwebhook notifications for rules (beta)When you add a webhook URL on an automation action, we\\nwill make a POST request to your webhook endpoint any time the rules you defined match any new\\nruns.Webhook payload?The payload we send to your webhook endpoint contains\"rule_id\" this is the\\nID of the automation that sent this payload\"start_time\" and \"end_time\" these are the time boundaries\\nwhere we found matching runs\"runs\" this is an array of runs, where each run is a dictionary. If you\\nneed more information about each run we suggest using our SDK in your endpoint to fetch it from\\nour API.This is an example webhook payload{  \"rule_id\":\\n\"d75d7417-0c57-4655-88fe-1db3cda3a47a\",  \"start_time\": \"2024-04-05T01:28:54.734491+00:00\", ', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\49.pdf', 'page': 0}),\n",
       " Document(page_content='\"end_time\": \"2024-04-05T01:28:56.492563+00:00\",  \"runs\": [    {      \"status\": \"success\",      \"is_root\":\\ntrue,      \"trace_id\": \"6ab80f10-d79c-4fa2-b441-922ed6feb630\",      \"dotted_order\":\\n\"20230505T051324571809Z6ab80f10-d79c-4fa2-b441-922ed6feb630\",      \"run_type\": \"tool\",     \\n\"modified_at\": \"2024-04-05T01:28:54.145062\",      \"tenant_id\":\\n\"2ebda79f-2946-4491-a9ad-d642f49e0815\",      \"end_time\": \"2024-04-05T01:28:54.085649\",     \\n\"name\": \"Search\",      \"start_time\": \"2024-04-05T01:28:54.085646\",      \"id\":\\n\"6ab80f10-d79c-4fa2-b441-922ed6feb630\",      \"session_id\":\\n\"6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5\",      \"parent_run_ids\": [],      \"child_run_ids\": null,     \\n\"direct_child_run_ids\": null,      \"total_tokens\": 0,      \"completion_tokens\": 0,      \"prompt_tokens\": 0,  \\n   \"total_cost\": null,      \"completion_cost\": null,      \"prompt_cost\": null,      \"first_token_time\": null,     \\n\"app_path\":\\n\"/o/2ebda79f-2946-4491-a9ad-d642f49e0815/projects/p/6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5/r/\\n6ab80f10-d79c-4fa2-b441-922ed6feb630?trace_id=6ab80f10-d79c-4fa2-b441-922ed6feb630&start_\\ntime=2023-05-05T05:13:24.571809\",      \"in_dataset\": false,      \"last_queued_at\": null,      \"inputs\":\\nnull,      \"inputs_s3_urls\": null,      \"outputs\": null,      \"outputs_s3_urls\": null,      \"extra\": null,     \\n\"events\": null,      \"feedback_stats\": null,      \"serialized\": null,      \"share_token\": null    }  ]}Webhook\\nSecurity?We strongly recommend you add a secret query string parameter to the webhook URL,\\nand verify it on any incoming request. This ensures that if someone discovers your webhook URL\\nyou can distinguish those calls from authentic webhook notifications.An example would\\nbehttps://api.example.com/langsmith_webhook?secret=38ee77617c3a489ab6e871fbeb2ec87dWeb\\nhook Delivery?When delivering events to your webhook endpoint we follow these guidelinesIf we fail\\nto connect to your endpoint, we retry the transport connection up to 2 times, before declaring the\\ndelivery failed.If your endpoint takes longer than 5 seconds to reply we declare the delivery failed\\nand do not .If your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times\\nwith exponential backoff.If your endpoint returns a 4xx status code, we declare the delivery failed\\nand do not retry.Anything your endpoint returns in the body will be ignoredExample with', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\49.pdf', 'page': 1}),\n",
       " Document(page_content='Modal?Setup?For an example of how to set this up, we will use Modal. Modal provides autoscaling\\nGPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python\\nweb endpoints. We\\'ll focus on the web endpoints here.First, create a Modal account. Then, locally\\ninstall the Modal SDK:pip install modalTo finish setting up your account, run the command:modal\\nsetupand follow the instructionsSecrets?Next, you will need to set up some secrets in Modal.First,\\nLangSmith will need to authenticate to Modal by passing in a secret.\\nThe easiest way to do this is to pass in a secret in the query parameters.\\nTo validate this secret, we will need to add a secret in Modal to validate it.\\nWe will do that by creating a Modal secret.\\nYou can see instructions for secrets here.\\nFor this purpose, let\\'s call our secret ls-webhook and have it set an environment variable with the\\nname LS_WEBHOOK.We can also set up a LangSmith secret - luckily there is already an\\nintegration template for this!Service?After that, you can create a Python file that will serve as your\\nendpoint.\\nAn example is below, with comments explaining what is going on:from fastapi import\\nHTTPException, status, Request, Queryfrom modal import Secret, Stub, web_endpoint, Imagestub\\n= Stub(\"auth-example\", image=Image.debian_slim().pip_install(\"langsmith\"))@stub.function(   \\nsecrets=[Secret.from_name(\"ls-webhook\"), Secret.from_name(\"my-langsmith-secret\")])# We want\\nthis to be a `POST` endpoint since we will post data here@web_endpoint(method=\"POST\")# We set\\nup a `secret` query parameterdef f(data: dict, secret: str = Query(...)):    # You can import\\ndependencies you don\\'t have locally inside Modal funxtions    from langsmith import Client    # First,\\nwe validate the secret key we pass    import os    if secret != os.environ[\"LS_WEBHOOK\"]:        raise\\nHTTPException(            status_code=status.HTTP_401_UNAUTHORIZED,            detail=\"Incorrect\\nbearer token\",            headers={\"WWW-Authenticate\": \"Bearer\"},        )    # This is where we put the\\nlogic for what should happen inside this webhook    ls_client = Client()    runs = data[\"runs\"]    ids =\\n[r[\"id\"] for r in runs]    feedback = list(ls_client.list_feedback(run_ids=ids))    for r, f in zip(runs,', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\49.pdf', 'page': 2}),\n",
       " Document(page_content='feedback):        try:            ls_client.create_example(                inputs=r[\"inputs\"],               \\noutputs={\"output\": f.correction},                dataset_name=\"classifier-github-issues\",            )       \\nexcept Exception:            raise ValueError(f\"{r} and {f}\")    # Function body    return \"success!\"We\\ncan now deploy this easily with modal deploy ... (see docs here).You should now get something\\nlike:? Created objects.??? ? Created mount\\n/Users/harrisonchase/workplace/langsmith-docs/example-webhook.py??? ? Created mount\\nPythonPackage:langsmith??? ? Created f => https://hwchase17--auth-example-f.modal.run? App\\ndeployed! ?View Deployment: https://modal.com/apps/hwchase17/auth-exampleThe important thing\\nto remember is https://hwchase17--auth-example-f.modal.run - the function we created to run.\\nNOTE: this is NOT the final deployment URL, make sure not to accidentally use that.Hooking it\\nup?We can now take the function URL we create above and add it as a webhook.\\nWe have to remember to also pass in the secret key as a query parameter.\\nPutting it all together, it should look something\\nlike:https://hwchase17--auth-example-f-dev.modal.run?secret={SECRET}Replace {SECRET} with\\nthe secret key you created to access the Modal service.Was this page helpful?PreviousSet up\\nthreadsNextCreate a promptWebhook payloadWebhook SecurityWebhook DeliveryExample with\\nModalSetupSecretsServiceHooking it upCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\49.pdf', 'page': 3}),\n",
       " Document(page_content=\"How-to guides | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationHuman feedbackMonitoring and\\nautomationsPromptsConceptsReferencePricingSelf-hostingHow-to guidesOn this pageHow-to\\nguidesStep-by-step guides that cover key tasks and operations in LangSmith.Setup?See the\\nfollowing guides to set up your LangSmith account.Create an account and API keyCreate an\\norganizationSet up billingSet up a workspaceSet up access control (enterprise only)Create a\\nroleAssign a role to a userTracing?Get started with LangSmith's tracing features to start adding\\nobservability to your LLM applications.Annotate code for tracingUse @traceable/traceableWrap the\\nOpenAI API clientUse the RunTree APIToggle tracing on and offLog traces to specific projectSet the\\ndestination project staticallySet the destination project dynamicallySet a sampling rate for tracesAdd\\nmetadata and tags to tracesImplement distributed tracingAccess the current span within a traced\\nfunctionLog multimodal tracesLog retriever tracesLog custom LLM tracesChat-style modelsSpecify\\nmodel nameStream outputsManually provide token countsInstruct-style modelsPrevent logging of\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\5.pdf', 'page': 0}),\n",
       " Document(page_content='inputs and outputs in tracesExport tracesUse filter argumentsUse filter query languageShare or\\nunshare a trace publiclyTrace generator functionsTrace with LangChainInstallationQuick startTrace\\nselectivelyLog to specific projectAdd metadata and tags to tracesCustomize run nameAccess run\\n(span) ID for LangChain invocationsTrace with Instructor (Python only)Datasets?Manage datasets in\\nLangSmith to evaluate and improve your LLM applications.Manage datasets in the\\napplicationCreate a new dataset and add examples manuallyAdd inputs and outputs from traces to\\ndatasetsUpload a CSV file to create a datasetExport a datasetManage datasets\\nprogrammaticallyCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset\\nfrom a CSV fileCreate a dataset from a pandas DataFrameFetch datasetsFetch examplesVersion\\ndatasetsCreate a new version of a datasetTag a versionShare or unshare a dataset\\npubliclyEvaluation?Evaluate your LLM applications to measure their performance over time.Evaluate\\nan LLM applicationRun an evaluationUse custom evaluatorsEvaluate on a particular version of a\\ndatasetEvaluate on a subset of a datasetUse a summary evaluatorEvaluate a LangChain\\nrunnableBind and evaluator to a dataset in the UIRun an evaluation from the prompt\\nplaygroundEvaluate on intermediate stepsUse LangChain off-the-shelf evaluators (Python only)Use\\nquestion and answer (correctness) evaluatorsUse criteria evaluatorsUse labeled criteria\\nevaluatorsUse string or embedding distance metricsUse a custom LLM in off-the-shelf\\nevaluatorsHandle multiple input or output fieldsCompare experiment resultsOpen the comparison\\nviewView regressions and improvementsFilter on regressions or improvementsUpdate baseline\\nexperimentSelect feedback keyOpen a traceExpand detailed viewUpdate display settingsEvaluate\\nan existing experimentUnit test LLM applications (Python only)Run pairwise evaluationsUse the\\nevaluate_comparative functionConfigure inputs and outputs for pairwise evaluatorsCompare two\\nexperiments with LLM-based pairwise evaluatorsView pairwise experimentsHuman\\nfeedback?Collect human feedback to improve your LLM applications.Capture user feedback from\\nyour application to tracesSet up a new feedback criteriaAnnotate traces inlineUse annotation\\nqueuesCreate an annotation queueAssign runs to an annotation queueReview runs in an annotation', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\5.pdf', 'page': 1}),\n",
       " Document(page_content=\"queueMonitoring and automations?Leverage LangSmith's powerful monitoring and automations\\nfeatures to make sense of your production data.Filter traces in the applicationCreate a filterFilter for\\nintermediate runs (spans)Advanced: filter for intermediate runs (spans) on properties of the\\nrootAdvanced: filter for runs (spans) whose child runs have some attributeCopy the filterManually\\nspecify a raw query in LangSmith query languageUse an AI Query to auto-generate a queryUse\\nmonitoring chartsChange the time periodSlice data by metadata or tagDrill down into specific\\nsubsetsSet up automation rulesCreate a ruleView logs for your automationsSet up online\\nevaluationsConfigure online evaluationsSet API keysSet up webhook notifications for rules\\n(beta)Webhook payloadExample with ModalSet up threadsGroup traces into threadsView\\nthreadsPrompts?Organize and manage prompts in LangSmith to streamline your LLM development\\nworkflow.Create a promptChoose a handleName your new promptPick a prompt typeCompose your\\npromptSave your promptView your promptsUpdate a promptUpdate metadataUpdate the prompt\\ncontentVersion a promptPull/push a promptInstall packagesConfigure environment variablesPull a\\nprompt and use itPush a prompt to your personal organizationLangChain HubWas this page\\nhelpful?PreviousOptimize a classifierNextCreate an account and API\\nkeySetupTracingDatasetsEvaluationHuman feedbackMonitoring and\\nautomationsPromptsCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\5.pdf', 'page': 2}),\n",
       " Document(page_content='Create a prompt | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationHuman feedbackMonitoring and automationsPromptsCreate\\na promptUpdate a promptPull/push a promptOpen a prompt from a traceLangChain\\nHubConceptsReferencePricingSelf-hostingHow-to guidesPromptsCreate a promptOn this\\npageCreate a promptNavigate to the Prompts section in the left-hand sidebar or from the application\\nhomepage.Choose a handle?The first time you enter the prompts section or the LangChain Hub,\\nyou\\'ll be prompted to choose a handle.\\nThis handle will be used to identify your public prompts in the hub.Name your new prompt?After\\nclicking \"New prompt\", name your prompt and decide if you want it to be \"private\" or \"public\".\\nPrivate prompts are only visible to your organization, while public prompts are discoverable to\\nanyone in the LangChain hub.\\nThe rest of the fields are optional. Click save to create your prompt.Pick a prompt type?You can\\nchoose between a regular prompt, a chat prompt, and a structured prompt.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\50.pdf', 'page': 0}),\n",
       " Document(page_content='Standard prompts are for instruct-style models (text), while chat prompts are for chat-style models\\n(list of messages). Structured prompts are chat-style prompts coupled with output\\nschemas.Compose your prompt?After choosing a prompt type, we\\'re brought to the playground\\nwhere we can develop our prompt. (We picked Chat Prompt.)\\nOn the left, we see an editable view of our Chat Prompt. To the right, we can enter sample inputs for\\nour prompt variables and then run our prompt against a model.(If you haven\\'t yet, you\\'ll need to\\nenter an API key for whichever model you want to run your prompt with.)To see the response from\\nthe model, click \"Start\".Save your prompt?Once you\\'re happy with your prompt, you can save it by\\nclicking the \"Commit\" button and selecting the prompt name you created before.\\nYou can also create a new prompt from here by typing a new prompt name in the search bar.View\\nyour prompts?You\\'ve just created your first prompt. View a table of your prompts in the prompts\\ntab.Was this page helpful?PreviousSet up webhook notifications for rules (beta)NextUpdate a\\npromptChoose a handleName your new promptPick a prompt typeCompose your promptSave your\\npromptView your promptsCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\50.pdf', 'page': 1}),\n",
       " Document(page_content='Update a prompt | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationHuman feedbackMonitoring and automationsPromptsCreate\\na promptUpdate a promptPull/push a promptOpen a prompt from a traceLangChain\\nHubConceptsReferencePricingSelf-hostingHow-to guidesPromptsUpdate a promptOn this\\npageUpdate a promptNavigate to the Prompts section in the left-hand sidebar or from the\\napplication homepage and click on the prompt you want to edit.Update metadata?To update the\\nprompt metadata (description, use cases, etc.) click the \"Edit\" pencil icon. This will take you to the\\nsame UI used to create a prompt.\\nClick save, and your prompt metadata will be updated.Update the prompt content?To update the\\nprompt content itself, you need to enter the prompt playground. Click \"Edit in playground\".\\nNow you can make changes to the prompt and test it with different inputs. When you\\'re happy with\\nthe prompt, click \"Commit\" to save it.\\nVersion a prompt?When you add a commit to a prompt, a new version of the prompt is created. You', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\51.pdf', 'page': 0}),\n",
       " Document(page_content='can view all historical versions by clicking the \"Commits\" tab in the prompt view.Was this page\\nhelpful?PreviousCreate a promptNextPull/push a promptUpdate metadataUpdate the prompt\\ncontentVersion a promptCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\51.pdf', 'page': 1}),\n",
       " Document(page_content='Pull/push a prompt | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationHuman feedbackMonitoring and automationsPromptsCreate\\na promptUpdate a promptPull/push a promptOpen a prompt from a traceLangChain\\nHubConceptsReferencePricingSelf-hostingHow-to guidesPromptsPull/push a promptOn this\\npagePull/push a promptInstall packages?pipyarnnpmpip install -U langchain langchainhub\\nlangchain-openaiyarn add langchainnpm install -S langchainConfigure environment variables?If you\\nalready have LANGCHAIN_API_KEY set to a personal organization\\'s api key from LangSmith, you\\ncan skip this step.Otherwise, get an API key for your Personal organization by navigating to Settings\\n> API Keys > Create API Key in LangSmith. (The hub will not work with your non-personal\\norganization\\'s api key!)Set your environment variable.export\\nLANGCHAIN_HUB_API_KEY=\"ls_...\"Pull a prompt and use it?You can pull your own prompts and\\nall of the public prompts in the LangChain Hub.PythonTypeScriptfrom langchain import hub# pull a\\nchat promptprompt = hub.pull(\"efriis/my-first-prompt\")# create a model to use it withfrom', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\52.pdf', 'page': 0}),\n",
       " Document(page_content='langchain_openai import ChatOpenAImodel = ChatOpenAI()# use it in a runnablerunnable = prompt\\n| modelresponse = runnable.invoke({\\t\"profession\": \"biologist\",\\t\"question\": \"What is special about\\nparrots?\",})print(response)// importimport * as hub from \"langchain/hub\";import {\\nChatPromptTemplate } from \"@langchain/core/prompts\";import { ChatOpenAI } from\\n\"@langchain/openai\";// pull a chat promptconst prompt = await\\nhub.pull<ChatPromptTemplate>(\"efriis/my-first-prompt\");// create a model to use it withconst model\\n= new ChatOpenAI();// use it in a runnableconst runnable = prompt.pipe(model);const result = await\\nrunnable.invoke({  \"profession\": \"biologist\",  \"question\": \"What is special about\\nparrots?\",});console.log(result);You can also pull a specific commit of a prompt by specifying the\\ncommit hash.prompt = hub.pull(\"efriis/my-first-prompt:56489e79\")Push a prompt to your personal\\norganization?For this step, you\\'ll need the handle for your account!PythonTypeScriptfrom langchain\\nimport hubfrom langchain.prompts.chat import ChatPromptTemplateprompt =\\nChatPromptTemplate.from_template(\"tell me a joke about\\n{topic}\")hub.push(\"<handle>/topic-joke-generator\", prompt, new_repo_is_public=False)import * as\\nhub from \"langchain/hub\";import {  ChatPromptTemplate,  HumanMessagePromptTemplate,} from\\n\\'@langchain/core/prompts\\';const message = HumanMessagePromptTemplate.fromTemplate(  \\'tell\\nme a joke about {topic}\\');const prompt = ChatPromptTemplate.fromMessages([message]);await\\nhub.push(\"<handle>/my-first-prompt\", prompt, { newRepoIsPublic: false });Was this page\\nhelpful?PreviousUpdate a promptNextOpen a prompt from a traceInstall packagesConfigure\\nenvironment variablesPull a prompt and use itPush a prompt to your personal\\norganizationCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\52.pdf', 'page': 1}),\n",
       " Document(page_content='Open a prompt from a trace | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationHuman feedbackMonitoring and automationsPromptsCreate\\na promptUpdate a promptPull/push a promptOpen a prompt from a traceLangChain\\nHubConceptsReferencePricingSelf-hostingHow-to guidesPromptsOpen a prompt from a traceOpen\\na prompt from a traceIf you pull a prompt into your code and begin logging traces that use it, you\\ncan view the prompt in the Trace UI.\\nNavigate to a trace that uses a prompt in LangSmith.In the run that used the prompt, hover over the\\nPrompt tag. Clicking on this will take you to the prompt. (If you used a LangChain Hub prompt, this\\ntag will say Hub)]In the metadata of the run, you can see more details. Click on an individual prompt\\nmetadata value to filter your traces by that attribute. You can filter by prompt handle, prompt name,\\nor prompt commit hash.Was this page helpful?PreviousPull/push a promptNextLangChain\\nHubCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\53.pdf', 'page': 0}),\n",
       " Document(page_content='2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\53.pdf', 'page': 1}),\n",
       " Document(page_content=\"LangChain Hub | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupTracingDatasetsEvaluationHuman feedbackMonitoring and automationsPromptsCreate\\na promptUpdate a promptPull/push a promptOpen a prompt from a traceLangChain\\nHubConceptsReferencePricingSelf-hostingHow-to guidesPromptsLangChain HubLangChain\\nHubNavigate to the LangChain Hub section of the left-hand sidebar.Here you'll find all of the publicly\\nlisted prompts in the LangChain Hub.\\nYou can search for prompts by name, handle, use cases, descriptions, or models. You can fork\\nprompts to your personal organization, view the prompt's details, and run the prompt in the\\nplayground.\\nYou can pull any public prompt into your code using the SDK.To view prompts tied to your\\nworkspace, visit the Prompts tab in the sidebar.Was this page helpful?PreviousOpen a prompt from\\na traceNextConceptsCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\54.pdf', 'page': 0}),\n",
       " Document(page_content='2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\54.pdf', 'page': 1}),\n",
       " Document(page_content='Concepts | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesConceptsAdminEvaluationTracingReferencePricingSelf-hostingConceptsOn this\\npageConceptsExplanations, clarification and discussion of key topics in\\nLangSmith.Admin?OrganizationsWorkspacesUsersAPI keysPersonal Access Tokens (PATs)Service\\nkeysRolesTracing?RunsTracesProjectsFeedbackTagsMetadataEvaluation?Datasets and\\nexamplesExperimentsEvaluatorsWas this page helpful?PreviousLangChain\\nHubNextAdminAdminTracingEvaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\55.pdf', 'page': 0}),\n",
       " Document(page_content='Admin | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesConceptsAdminEvaluationTracingReferencePricingSelf-hostingConceptsAdminOn this\\npageAdminThis conceptual guide covers topics related to managing users, organizations, and\\nworkspaces within LangSmith.Organizations?An organization is a logical grouping of users within\\nLangSmith with its own billing configuration. Typically, there is one organization per company. An\\norganization can have multiple workspaces. For more details, see the setup guide.Workspaces\\n(coming soon)?A workspace is a logical grouping of users and resources within an organization.\\nUsers may have permissions in a workspace that grant them access to the resources in that\\nworkspace, including tracing projects, datasets, annotation queues, and prompts. For more details,\\nsee the setup guide.Users?A user is a person who has access to LangSmith. Users can be\\nmembers of one or more organizations and workspaces within those organizations.Organization\\nmembers are managed in organization settings:And workspace members are managed in\\nworkspace settings:API Keys?Dropping support July 1st, 2024We will be dropping support for API', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\56.pdf', 'page': 0}),\n",
       " Document(page_content='keys on July 1st 2024 in favor of personal access tokens (PATs) and service keys. We recommend\\nusing PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER\\nwork after July 1st, 2024.API keys are used to authenticate requests to the LangSmith API. They\\nare created by users and scoped to a workspace. This means that all requests made with an API\\nkey will be associated with the workspace that the key was created in. The API key will have the\\nability to create, read, update, delete all resources within that workspace.API keys are prefixed with\\nls__. These keys will also show up in the UI under the service keys tab.Personal Access Tokens\\n(PATs)?Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API.\\nThey are created by users and scoped to a user. The PAT will have the same permissions as the\\nuser that created it.PATs are prefixed with lsv2_pt_Service Keys?Service keys are similar to PATs,\\nbut are used to authenticate requests to the LangSmith API on behalf of a service account.Service\\nkeys are prefixed with lsv2_sk_noteTo see how to create a service key or Personal Access Token,\\nsee the setup guideRoles?noteRBAC (Role-Based Access Control) is a feature that is only available\\nto Enterprise customers. If you are interested in this feature, please contact our sales team at\\nsales@langchain.dev\\nOther plans default to using the Admin role for all users.Roles are used to define the set of\\npermissions that a user has within a workspace. There are three built-in system roles that cannot be\\nedited:Admin - has full access to all resources within the workspaceViewer - has read-only access to\\nall resources within the workspaceEditor - has full permissions except for workspace management\\n(adding/removing users, changing roles, configuring service keys)Admins can also create/edit\\ncustom roles with specific permissions for different resources.Roles can be managed in organization\\nsettings under the Roles tab:For more details on assigning and creating roles, see the access\\ncontrol setup guide.Was this page\\nhelpful?PreviousConceptsNextEvaluationOrganizationsWorkspaces (coming soon)UsersAPI\\nKeysPersonal Access Tokens (PATs)Service KeysRolesCommunityDiscordTwitterGitHubDocs\\nCodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\56.pdf', 'page': 1}),\n",
       " Document(page_content='DocsCopyright © 2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\56.pdf', 'page': 2}),\n",
       " Document(page_content=\"Evaluation | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesConceptsAdminEvaluationTracingReferencePricingSelf-hostingConceptsEvaluationOn this\\npageEvaluationThis conceptual guide covers topics that are important to understand when running\\nevaluations in LangSmith.Evaluations allow you to understand the performance of your LLM\\napplication over time. At its core, an evaluator is a function that takes in a set of inputs and outputs\\nfrom your LLM pipeline, along with an expected output (if available)\\nand returns a score (or multiple scores). This score is logged as feedback in LangSmith and bound\\nto the trace of the underlying task that produced the output.This score may be based on comparing\\nthe outputs with reference outputs (e.g. with string matching or using an LLM as a judge).\\nHowever, there are also evaluators that don't require a reference output - for example, one that\\nchecks if the output is valid JSON, which is a common requirement in LLM applications.\\nLangSmith allows you to run evaluations on your application via Datasets, which are made up of\\nExamples.The following diagram outlines the building blocks for evaluations in LangSmith. Datasets\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\57.pdf', 'page': 0}),\n",
       " Document(page_content='define the inputs over which you run your chain, model, or agent (the Task),\\nand optionally the reference outputs against which your evaluator will compare the outputs of your\\nTask. These datasets can be from any number of sources -\\nyou might manually curate them, collect them from user input/feedback, or generate them via LLM.\\nYour Evaluator can be any arbitrary function which returns a score\\nbased on the inputs and outputs of your Task, and the reference output if desired. You can also use\\n[one of LangSmith\\'s off-the-shelf\\nevaluators] to get started quickly!Datasets and examples?Datasets are collections of Examples, the\\ncore building block for the evaluation workflow in LangSmith.\\nExamples provide the inputs over which you will be running your pipeline,\\nand, if applicable, the expected outputs that you will be comparing against.\\nAll examples in a given dataset should follow the same schema. Examples contain an \"inputs\" dict\\nand an \"output\" dict,\\nalong with (optionally) a metadata dict.A Dataset in the LangSmith UIAn Example in the LangSmith\\nUITypes of Datasets?Dataset types communicate common input and output schemas. There are\\nthree types of datasets in LangSmith: kv, llm, and chat. kv datasets are the default type, and are\\nsufficient for almost all use-cases. llm and chat datasets can be useful to conveniently export\\ndatasets into known fine-tuning formats.kv: In kv datasets, inputs and outputs can be arbitrary\\nkey-value pairs. These are useful when evaluating chains and agents that require\\nmultiple inputs or that return multiple outputs.\\nThe tradeoff with these datasets is that running evaluations on them can be a bit more involved. If\\nthere are multiple keys, you will have to manually specify the prepare_data\\nfunction in any off-the-shelf evaluators so they know what information to consider in generating a\\nscore.llm: llm datasets correspond to the string inputs and outputs from the \"completion\" style LLMS\\n(string in, string out). The \"inputs\" dictionary contains a single \"input\" key mapped to a single prompt\\nstring. Similarly, the \"outputs\" dictionary contains a single \"output\" key mapped to a single response', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\57.pdf', 'page': 1}),\n",
       " Document(page_content='string.chat: chat datasets correspond to messages and generations from LLMs that expect\\nstructured \"chat\" messages as inputs and outputs. Each example row\\nexpects an \"inputs\" dictionary containing a single \"input\" key mapped to a list of serialized chat\\nmessages. The \"outputs\" dictionary contains a single \"output\" key mapped to a single list of\\nserialized chat messages.Experiments?A single execution of all your example inputs through your\\nTask is called an Experiment. In LangSmith, you can easily view all the experiments that are\\nassociated\\nwith your dataset, and track your application\\'s performance over time! You can also compare two or\\nmore experiments in a comparison view.Multiple experiments can be bound to the same dataset,\\nand each experiment can have multiple runs.Comparing Multiple Experiments in the LangSmith\\nUIEvaluators?Evaluators are functions that help score how well your system did on a particular\\nexample. When running an evaluation,\\nyour example inputs are run through your Task to produce Runs, which are then passed into your\\nevaluator along with the Example.\\nThe function then returns an EvaluationResult, which specifies your metric name and score.\\nEvaluations in LangSmith are run via the evaluate() function.\\nThe following diagram gives an overview of the data flow in an evaluation:The inputs to an evaluator\\nconsist of:An example - the inputs for your pipeline and optionally the reference outputs or labelsA\\nroot_run - observed output gathered from running the inputs through the Task along with metadata\\nand intermediate stepsAn evaluator will then return an EvaluationResult (or similarly shaped\\ndictionary), which is made up of:key: The name the metric being evaluatedscore: The value of the\\nmetric on this examplecomment: the reasoning trajectory or other string information motivating the\\nscoreTypes of Evaluators?The evaluator itself can be any arbitrary function. There are a few\\ndifferent types of evaluators that are commonly used:Heuristics: A heuristic evaluator is a\\nhard-coded function that does some computation to determine a score. For example, you might\\nwrite an', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\57.pdf', 'page': 2}),\n",
       " Document(page_content=\"evaluator that checks whether the output of the system is an empty string, or determines if it is valid\\nJSON. These would be considered reference-free evaluators,\\nas they don't consider any example output when making their decision. You might also want to\\ncheck that the output of the system matches the reference output exactly,\\nwhich would be considered a ground truth evaluator because it compares the output to a reference.\\nSee [How to create custom evaluators].LLM-as-judge: An LLM-as-judge evaluator uses an LLM to\\nscore system output. For example, you might want to check whether your system is outputting\\noffensive content. This is reference-free, as there is no comparison to an example output. You might\\nalso want to check whether the system output has the same\\nmeaning as the example output, which would be a ground truth evaluator. To get started with\\nLLM-as-a-judge, try out LangSmith's [off-the-shelf evaluators]!Human: You can also evaluate your\\nruns manually. This can be done in LangSmith [via the SDK],\\nor [in the LangSmith UI].Was this page helpful?PreviousAdminNextTracingDatasets and\\nexamplesTypes of DatasetsExperimentsEvaluatorsTypes of\\nEvaluatorsCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\57.pdf', 'page': 3}),\n",
       " Document(page_content='Tracing | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesConceptsAdminEvaluationTracingReferencePricingSelf-hostingConceptsTracingOn this\\npageTracingThis conceptual guide covers topics that are important to understand when logging\\ntraces to LangSmith. A Trace is essentially a series of steps that your application takes to go from\\ninput to output. Each of these individual steps is represented by a Run. A Project is simply a\\ncollection of traces. The following diagram displays these concepts in the context of a simple RAG\\napp, which retrieves documents from an index and generates an answer.Primitive datatypes in\\nLangSmithRuns?A Run is a span representing a single unit of work or operation within your LLM\\napplication. This could be anything from single call to an LLM or chain, to a prompt formatting call, to\\na runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a\\nspan.To learn more about how runs are stored in the application, see this reference guide.Traces?A\\nTrace is a collection of runs that are related to a single operation. For example, if you have a user\\nrequest that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\58.pdf', 'page': 0}),\n",
       " Document(page_content='so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you\\ncan think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace\\nID.\\nProjects?A Project is a collection of traces. You can think of a project as a container for all the traces\\nthat are related to a single application or service. You can have multiple projects, and each project\\ncan have multiple traces.\\nFeedback?Feedback allows you to score an individual run based on certain criteria.\\nEach feedback entry consists of a feedback tag and feedback score, and is bound to a run by a\\nunique run ID.\\nFeedback can currently be continuous or discrete (categorical), and you can reuse feedback tags\\nacross different runs within an organization.Collecting feedback on runs can be done in a number of\\nways:Sent up along with a trace from the LLM applicationGenerated by a user in the app inline or in\\nan annotation queueGenerated by an automatic evaluator during offline evaluationGenerated by an\\nonline evaluatorTo learn more about how feedback is stored in the application, see this reference\\nguide.Tags?Tags are collections of strings that can be attached to runs. They are used to categorize\\nruns and make it easier to search for them in the LangSmith UI. Tags can be used to filter runs in\\nthe LangSmith UI, and can be used to group runs together for analysis. Learn how to attach tags to\\nyour traces\\nMetadata?Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be\\nused to store additional information about a run, such as the version of the application that\\ngenerated the run, the environment in which the run was generated, or any other information that\\nyou want to associate with a run.\\nSimilar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group\\nruns together for analysis. Learn how to add metadata to your traces\\nWas this page\\nhelpful?PreviousEvaluationNextReferenceRunsTracesProjectsFeedbackTagsMetadataCommunityD', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\58.pdf', 'page': 1}),\n",
       " Document(page_content='iscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python\\nDocsLangChain JS/TS DocsCopyright © 2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\58.pdf', 'page': 2}),\n",
       " Document(page_content='Reference | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesConceptsReferenceAuthz and AuthnAuthentication methodsData formatsRun (span) data\\nformatFeedback data formatTrace query syntaxSDK referenceLangChain off-the-shelf\\nevaluatorsPricingSelf-hostingReferenceOn this pageReferenceTechnical reference that covers\\ncomponents, APIs, and other aspects of LangSmith.API reference?LangSmith API ReferenceSDK\\nreference?LangChain off-the-shelf evaluators (Python only)Data formats?Run (span) data\\nformatFeedback data formatTrace query syntaxFilter argumentsFilter query languageAuthentication\\nand authorization?Authentication methodsWas this page\\nhelpful?PreviousTracingNextAuthentication methodsAPI referenceSDK referenceData\\nformatsAuthentication and authorizationCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\59.pdf', 'page': 0}),\n",
       " Document(page_content='', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\59.pdf', 'page': 1}),\n",
       " Document(page_content='Create an account and API key | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupCreate an account and API keyCreate an organizationSet up access controlSet up\\nbilling for your LangSmith accountSet up a workspaceTracingDatasetsEvaluationHuman\\nfeedbackMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesSetupCreate an account and API keyOn this pageCreate an account and API keyCreate an\\naccount?To get started with LangSmith, you need to create an account. You can sign up for a free\\naccount here.\\nWe support logging in with Google, GitHub, Discord, and email.API keys?LangSmith supports two\\ntypes of API keys: Service Keys and Personal Access Tokens.\\nBoth types of tokens can be used to authenticate requests to the LangSmith API, but they have\\ndifferent use cases.Read more about the differences between Service Keys and Personal Access\\nTokens under admin conceptsCreate an API key?To log traces and run evaluations with LangSmith,\\nyou will need to create an API key to authenticate your requests.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\6.pdf', 'page': 0}),\n",
       " Document(page_content='Currently, an API key is scoped to a workspace, so you will need to create an API key for each\\nworkspace you want to use.To create either type of API key head to the Settings page, then scroll to\\nthe API Keys section. Then click Create API Key.noteThe API key will be shown only once, so make\\nsure to copy it and store it in a safe place.Was this page helpful?PreviousHow-to guidesNextCreate\\nan organizationCreate an accountAPI keysCreate an API keyCommunityDiscordTwitterGitHubDocs\\nCodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS\\nDocsCopyright © 2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\6.pdf', 'page': 1}),\n",
       " Document(page_content='Authentication methods | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesConceptsReferenceAuthz and AuthnAuthentication methodsData formatsRun (span) data\\nformatFeedback data formatTrace query syntaxSDK referenceLangChain off-the-shelf\\nevaluatorsPricingSelf-hostingReferenceAuthz and AuthnAuthentication methodsOn this\\npageAuthentication methodsLangSmith supports multiple authentication methods for easy sign-up\\nand login.Cloud?Email/Password?Users can use an email address and password to sign up and\\nlogin to LangSmith.Social Providers?Users can alternatively use their credentials from GitHub,\\nGoogle, or Discord.Self-Hosted?Self-hosted customers have more control over how their users can\\nlogin to LangSmith.\\nFor more in-depth coverage of configuration options, see the self-hosting docs\\nand Helm chart.OIDC?Production installations should configure OIDC in order to use an external\\nidentity provider.\\nThis enables users to login through an identity platform like Auth0/Okta or a social provider like', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\60.pdf', 'page': 0}),\n",
       " Document(page_content='Google.Email/Password a.k.a. Basic Auth (coming soon)?This auth method requires very little\\nconfiguration as it does not require an external identity provider.\\nIt is most appropriate to use for self-hosted trials.None?dangerThis authentication mode will be\\nremoved after the launch of Basic Auth.If zero authentication methods are enabled, a self-hosted\\ninstallation does not require any login/sign-up.\\nThis configuration should only be used for verifying installation at the infrastructure level, as the\\nfeature set\\nsupported in this mode is restricted with only a single organization and workspace.Was this page\\nhelpful?PreviousReferenceNextRun (span) data formatCloudEmail/PasswordSocial\\nProvidersSelf-HostedOIDCEmail/Password a.k.a. Basic Auth (coming\\nsoon)NoneCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\60.pdf', 'page': 1}),\n",
       " Document(page_content='Run (span) data format | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesConceptsReferenceAuthz and AuthnAuthentication methodsData formatsRun (span) data\\nformatFeedback data formatTrace query syntaxSDK referenceLangChain off-the-shelf\\nevaluatorsPricingSelf-hostingReferenceData formatsRun (span) data formatRun (span) data\\nformatRecommended ReadingBefore diving into this content, it might be helpful to read the\\nfollowing:Conceptual guide on tracing and runsLangSmith stores and processes trace data in a\\nsimple format that is easy to export and import.Many of these fields are optional or not important to\\nknow about but are included for completeness.\\nThe bolded fields are the most important ones to know about.Field\\nNameTypeDescriptionidUUIDUnique identifier for the span.namestringThe name associated with the\\nrun.inputsobjectA map or set of inputs provided to the run.run_typestringType of run, e.g., \"llm\",\\n\"chain\", \"tool\".start_timedatetimeStart time of the run.end_timedatetimeEnd time of the\\nrun.extraobjectAny extra information run.errorstringError message if the run encountered an', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\61.pdf', 'page': 0}),\n",
       " Document(page_content='error.outputsobjectA map or set of outputs generated by the run.eventsarray of objectsA list of event\\nobjects associated with the run. This is relevant for runs executed with streaming.tagsarray of\\nstringsTags or labels associated with the run.trace_idUUIDUnique identifier for the trace the run is a\\npart of. This is also the id field of the root run of the tracedotted_orderstringCustom ordering string,\\nhierarchical. Built with timestamp and unique identifiers.statusstringCurrent status of the run\\nexecution, e.g., \"error\", \"pending\", \"success\"child_run_idsarray of UUIDsList of IDs for all child\\nruns.direct_child_run_idsarray of UUIDsList of IDs for direct children of this run.parent_run_idsarray\\nof UUIDsList of IDs for all parent runs.feedback_statsobjectAggregations of feedback statistics for\\nthis runreference_example_idUUIDID of a reference example associated with the run. This is\\nusually only present for evaluation runs.total_tokensintegerTotal number of tokens processed by the\\nrun.prompt_tokensintegerNumber of tokens in the prompt of the\\nrun.completion_tokensintegerNumber of tokens in the completion of the run.total_coststringTotal\\ncost associated with processing the run.prompt_coststringCost associated with the prompt part of\\nthe run.completion_coststringCost associated with the completion of the\\nrun.first_token_timedatetimeTime when the first token was generated.session_idstringSession\\nidentifier for the run.in_datasetbooleanIndicates whether the run is included in a\\ndataset.parent_run_idUUIDUnique identifier of the parent run.execution_order\\n(deprecated)integerThe order in which this run was executed within the\\ntrace.serializedobjectSerialized state of the object executing the run if applicable.manifest_id\\n(deprecated)UUIDIdentifier for a manifest associated with the span.manifest_s3_idUUIDS3 identifier\\nfor the manifest.inputs_s3_urlsobjectS3 URLs for the inputs.outputs_s3_urlsobjectS3 URLs for the\\noutputs.price_model_idUUIDIdentifier for the pricing model applied to the\\nrun.app_pathstringApplication (UI) path for this run.last_queued_atdatetimeLast time the span was\\nqueued.share_tokenstringToken for sharing access to the run\\'s data.Here is an example of a JSON\\nrepresentation of a run in the above format:{  \"id\": \"497f6eca-6276-4993-bfeb-53cbbbba6f08\", \\n\"name\": \"string\",  \"inputs\": {},  \"run_type\": \"llm\",  \"start_time\": \"2024-04-29T00:49:12.090000\", ', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\61.pdf', 'page': 1}),\n",
       " Document(page_content='\"end_time\": \"2024-04-29T00:49:12.459000\",  \"extra\": {},  \"error\": \"string\",  \"execution_order\": 1, \\n\"serialized\": {},  \"outputs\": {},  \"parent_run_id\": \"f8faf8c1-9778-49a4-9004-628cdb0047e5\", \\n\"manifest_id\": \"82825e8e-31fc-47d5-83ce-cd926068341e\",  \"manifest_s3_id\":\\n\"0454f93b-7eb6-4b9d-a203-f1261e686840\",  \"events\": [{}],  \"tags\": [\"foo\"],  \"inputs_s3_urls\": {}, \\n\"outputs_s3_urls\": {},  \"trace_id\": \"df570c03-5a03-4cea-8df0-c162d05127ac\",  \"dotted_order\":\\n\"20240429T004912090000Z497f6eca-6276-4993-bfeb-53cbbbba6f08\",  \"status\": \"string\", \\n\"child_run_ids\": [\"497f6eca-6276-4993-bfeb-53cbbbba6f08\"],  \"direct_child_run_ids\":\\n[\"497f6eca-6276-4993-bfeb-53cbbbba6f08\"],  \"parent_run_ids\":\\n[\"f8faf8c1-9778-49a4-9004-628cdb0047e5\"],  \"feedback_stats\": {    \"correctness\": {      \"n\": 1,     \\n\"avg\": 1.0    }  },  \"reference_example_id\": \"9fb06aaa-105f-4c87-845f-47d62ffd7ee6\",  \"total_tokens\":\\n0,  \"prompt_tokens\": 0,  \"completion_tokens\": 0,  \"total_cost\": \"string\",  \"prompt_cost\": \"string\", \\n\"completion_cost\": \"string\",  \"price_model_id\": \"0b5d9575-bec3-4256-b43a-05893b8b8440\", \\n\"first_token_time\": null,  \"session_id\": \"1ffd059c-17ea-40a8-8aef-70fd0307db82\",  \"app_path\":\\n\"string\",  \"last_queued_at\": null,  \"in_dataset\": true,  \"share_token\":\\n\"d0430ac3-04a1-4e32-a7ea-57776ad22c1c\"}Was this page helpful?PreviousAuthentication\\nmethodsNextFeedback data formatCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\61.pdf', 'page': 2}),\n",
       " Document(page_content=\"Feedback data format | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesConceptsReferenceAuthz and AuthnAuthentication methodsData formatsRun (span) data\\nformatFeedback data formatTrace query syntaxSDK referenceLangChain off-the-shelf\\nevaluatorsPricingSelf-hostingReferenceData formatsFeedback data formatFeedback data\\nformatRecommended ReadingBefore diving into this content, it might be helpful to read the\\nfollowing:Conceptual guide on tracing and feedbackFeedback is LangSmith's way of storing the\\ncriteria and scores from evaluation on a particular trace or intermediate run (span).\\nFeedback can be produced from a variety of ways, such as:Sent up along with a trace from the LLM\\napplicationGenerated by a user in the app inline or in an annotation queueGenerated by an\\nautomatic evaluator during offline evaluationGenerated by an online evaluatorFeedback is stored in\\na simple format with the following fields:Field NameTypeDescriptionidUUIDUnique identifier for the\\nrecord itselfcreated_atdatetimeTimestamp when the record was\\ncreatedmodified_atdatetimeTimestamp when the record was last modifiedsession_idUUIDUnique\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\62.pdf', 'page': 0}),\n",
       " Document(page_content='identifier for the experiment or tracing project the run was a part ofrun_idUUIDUnique identifier for a\\nspecific run within a sessionkeystringA key describing the criteria of the feedback, eg\\n\"correctness\"scorenumberNumerical score associated with the feedback keyvaluestringReserved\\nfor storing a value associated with the score. Useful for categorical feedback.commentstringAny\\ncomment or annotation associated with the record. This can be a justification for the score\\ngiven.correctionobjectReserved for storing correction details, if anyfeedback_sourceobjectObject\\ncontaining information about the feedback sourcefeedback_source.typestringThe type of source\\nwhere the feedback originated, eg \"api\", \"app\",\\n\"evaluator\"feedback_source.metadataobjectReserved for additional metadata,\\ncurrentlyfeedback_source.user_idUUIDUnique identifier for the user providing feedbackHere is an\\nexample JSON representation of a feedback record in the above format:{  \"created_at\":\\n\"2024-05-05T23:23:11.077838\",  \"modified_at\": \"2024-05-05T23:23:11.232962\",  \"session_id\":\\n\"c919298b-0af2-4517-97a2-0f98ed4a48f8\",  \"run_id\": \"e26174e5-2190-4566-b970-7c3d9a621baa\", \\n\"key\": \"correctness\",  \"score\": 1.0,  \"value\": null,  \"comment\": \"I gave this score because the answer\\nwas correct.\",  \"correction\": null,  \"id\": \"62104630-c7f5-41dc-8ee2-0acee5c14224\", \\n\"feedback_source\": {    \"type\": \"app\",    \"metadata\": null,    \"user_id\":\\n\"ad52b092-1346-42f4-a934-6e5521562fab\"  }}Was this page helpful?PreviousRun (span) data\\nformatNextTrace query syntaxCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\62.pdf', 'page': 1}),\n",
       " Document(page_content='Trace query syntax | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesConceptsReferenceAuthz and AuthnAuthentication methodsData formatsRun (span) data\\nformatFeedback data formatTrace query syntaxSDK referenceLangChain off-the-shelf\\nevaluatorsPricingSelf-hostingReferenceData formatsTrace query syntaxOn this pageTrace query\\nsyntaxUsing the list_runs method in the SDK or /runs/query endpoint in the API, you can filter runs\\nto analyze and export.Filter arguments?KeysDescriptionproject_id / project_nameThe project(s) to\\nfetch runs from - can be a single project or a list of projects.trace_idFetch runs that are part of a\\nspecific trace.run_typeThe type of run to get, such as llm, chain, tool, retriever, etc.dataset_name /\\ndataset_idFetch runs that are associated with an example row in the specified dataset. This is useful\\nfor comparing prompts or models over a given dataset.reference_example_idFetch runs that are\\nassociated with a specific example row. This is useful for comparing prompts or models on a given\\ninput.parent_run_idFetch runs that are children of a given run. This is useful for fetching runs\\ngrouped together using the context manager or for fetching an agent trajectory.errorFetch runs that', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\63.pdf', 'page': 0}),\n",
       " Document(page_content='errored or did not error.run_idsFetch runs with a given list of run ids. Note: This will ignore all other\\nfiltering arguments.filterFetch runs that match a given structured filter statement. See the guide\\nbelow for more information.trace_filterFilter to apply to the ROOT run in the trace tree. This is meant\\nto be used in conjunction with the regular filter parameter to let you filter runs by attributes of the\\nroot run within a trace.tree_filterFilter to apply to OTHER runs in the trace tree, including sibling and\\nchild runs. This is meant to be used in conjunction with the regular filter parameter to let you filter\\nruns by attributes of any run within a trace.is_rootOnly return root runs.selectSelect the fields to\\nreturn in the response. By default, all fields are returned.query (experimental)Natural language\\nquery, which translates your query into a filter statement.Filter query language?LangSmith supports\\npowerful filtering capabilities with a filter query language to permit complex filtering operations when\\nfetching runs.The filtering grammar is based on common comparators on fields in the run object.\\nSupported comparators include:gte (greater than or equal to)gt (greater than)lte (less than or equal\\nto)lt (less than)eq (equal to)neq (not equal to)has (check if run contains a tag or metadata json\\nblob)search (search for a substring in a string field)Additionally, you can combine multiple\\ncomparisons through and and or operators.These can be applied on fields of the run object, such as\\nits id, name, run_type, start_time / end_time, latency, total_tokens, error, execution_order, tags, and\\nany associated feedback through feedback_key and feedback_score.Was this page\\nhelpful?PreviousFeedback data formatNextLangChain off-the-shelf evaluatorsFilter argumentsFilter\\nquery languageCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\63.pdf', 'page': 1}),\n",
       " Document(page_content=\"LangChain off-the-shelf evaluators | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesConceptsReferenceAuthz and AuthnAuthentication methodsData formatsRun (span) data\\nformatFeedback data formatTrace query syntaxSDK referenceLangChain off-the-shelf\\nevaluatorsPricingSelf-hostingReferenceSDK referenceLangChain off-the-shelf evaluatorsLangChain\\noff-the-shelf evaluatorsLangChain's evaluation module provides evaluators you can use as-is for\\ncommon evaluation scenarios.\\nTo learn how to use these evaluators, please refer to the following guide.noteWe currently support\\noff-the-shelf evaluators for Python only, but are adding support for TypeScript soon.noteMost of\\nthese evaluators are useful but imperfect! We recommend against blind trust of any single\\nautomated metric and to always incorporate them as a part of a holistic testing and evaluation\\nstrategy.\\nMany of the LLM-based evaluators return a binary score for a given datapoint, so measuring\\ndifferences in prompt or model performance are most reliable in aggregate over a larger dataset.The\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\64.pdf', 'page': 0}),\n",
       " Document(page_content='following table enumerates the off-the-shelf evaluators available in LangSmith, along with their\\noutput keys and a simple code sample.Evaluator nameOutput KeySimple Code\\nExampleQ&AcorrectnessLangChainStringEvaluator(\"qa\")Contextual Q&Acontextual\\naccuracyLangChainStringEvaluator(\"context_qa\")Chain of Thought Q&Acot contextual\\naccuracyLangChainStringEvaluator(\"cot_qa\")CriteriaDepends on criteria\\nkeyLangChainStringEvaluator(\"criteria\", config={ \"criteria\": <criterion> })criterion may be one of the\\ndefault implemented criteria: conciseness, relevance, correctness, coherence, harmfulness,\\nmaliciousness, helpfulness, controversiality, misogyny, and criminality.Or, you may define your own\\ncriteria in a custom dict as follows:{ \"criterion_key\": \"criterion description\" }Labeled CriteriaDepends\\non criteria keyLangChainStringEvaluator(\"labeled_criteria\", config={ \"criteria\": <criterion> })criterion\\nmay be one of the default implemented criteria: conciseness, relevance, correctness, coherence,\\nharmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.Or, you may\\ndefine your own criteria in a custom dict as follows:{ \"criterion_key\": \"criterion description\"\\n}ScoreDepends on criteria keyLangChainStringEvaluator(\"score_string\", config={ \"criteria\":\\n<criterion>, \"normalize_by\": 10 })criterion may be one of the default implemented criteria:\\nconciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness,\\ncontroversiality, misogyny, and criminality.Or, you may define your own criteria in a custom dict as\\nfollows:{ \"criterion_key\": \"criterion description\" }. Scores are out of 10, so normalize_by will cast this\\nto a score from 0 to 1.Labeled ScoreDepends on criteria\\nkeyLangChainStringEvaluator(\"labeled_score_string\", config={ \"criteria\": <criterion>, \"normalize_by\":\\n10 })criterion may be one of the default implemented criteria: conciseness, relevance, correctness,\\ncoherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.Or,\\nyou may define your own criteria in a custom dict as follows:{ \"criterion_key\": \"criterion description\" }.\\nScores are out of 10, so normalize_by will cast this to a score from 0 to 1.Embedding\\ndistanceembedding_cosine_distanceLangChainStringEvaluator(\"embedding_distance\")String\\nDistancestring_distanceLangChainStringEvaluator(\"string_distance\", config={\"distance\":', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\64.pdf', 'page': 1}),\n",
       " Document(page_content='\"damerau_levenshtein\" }) distance defines the string difference metric to be applied, such as\\nlevenshtein or jaro_winkler.Exact\\nMatchexact_matchLangChainStringEvaluator(\"exact_match\")Regex\\nMatchregex_matchLangChainStringEvaluator(\"regex_match\")Json\\nValidityjson_validityLangChainStringEvaluator(\"json_validity\")Json\\nEqualityjson_equalityLangChainStringEvaluator(\"json_equality\")Json Edit\\nDistancejson_edit_distanceLangChainStringEvaluator(\"json_edit_distance\")Json\\nSchemajson_schemaLangChainStringEvaluator(\"json_schema\")Was this page\\nhelpful?PreviousTrace query syntaxNextPricingCommunityDiscordTwitterGitHubDocs\\nCodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS\\nDocsCopyright © 2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\64.pdf', 'page': 2}),\n",
       " Document(page_content='Pricing | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesConceptsReferencePricingSelf-hostingPricingOn this\\npagePricingPlans?StartupsDeveloperPlusEnterpriseDesigned for early stage startups building AI\\napplicationsDesigned for hobbyists who want to start their adventure soloEverything in Developer,\\nplus team features, higher rate limits, and longer data retentionDesigned for teams with more\\nsecurity, deployment, and support needsContact us to learn moreFree for 1 user5,000 free traces\\nper monthAdditional traces billed @ $0.005/trace$39/user10,000 free traces per monthAdditional\\ntraces billed @ $0.005/traceCustomWhat to expect:We want all early stage companies to build with\\nLangSmith. LangSmith for Startups offers discounted prices and a generous free, monthly trace\\nallotment, so you can have the right tooling in place as you grow your business.Key features:1\\nDeveloper seatDebugging tracesDataset collectionTesting and evaluationPrompt\\nmanagementMonitoringKey features:All features in Developer tierUp to 10 seatsHosted LangServe\\n(beta)Longer data retentionHigher rate limitsEmail supportKey features:All features in Plus tierSingle', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\65.pdf', 'page': 0}),\n",
       " Document(page_content=\"Sign On (SSO)Negotiable SLAsDeployment options in customer?s environmentCustom rate\\nlimitsTeam trainingsShared Slack channelArchitectural guidanceDedicated customer success\\nmanagerPlan Comparison?DeveloperPlusEnterpriseFeaturesDebugging Traces???Dataset\\nCollection???Human Labeling???Testing and Evaluation???Prompt Management???Hosted\\nLangServe--??Monitoring???Role-Based Access Controls (RBAC)----?TeamDeveloper Seats1 Free\\nSeatMaximum 10 seats$39 per seat/month1Unlimited seatsCustom pricingCollaborator\\nSeats----Coming Soon!Trace DetailsTracesFirst 5k traces per month for free.$0.005 per trace\\nthereafter2First 10k traces per month for free.$0.005 per trace thereafter2CustomRate LimitsMax\\ningested events / hour350,0003 / 250,000500,000CustomTotal trace size storage / hour4500MB3 /\\n2.5GB5GBCustomSecurity ControlsSingle Sign On--GoogleGitHubCustom SSODeploymentHosted\\nin USHosted in USAdd-on for self-hosted deployment in customer's VPCSupportSupport\\nChannelsCommunityEmailEmailShared Slack ChannelShared Slack Channel----?Team\\nTraining----?Application Architectural Guidance----?Dedicated Customer Success\\nManager----?SLA----?ProcurementBillingMonthly, self-serveCredit CardMonthly, self-serveCredit\\nCardAnnual InvoiceACHCustom Terms and Data Privacy Agreement----?Infosec Review----?1 Seats\\nare billed monthly on the first of the month and in the future will be prorated if additional seats are\\npurchased in the middle of the month. Seats removed mid-month are not credited.2 You can\\npurchase LangSmith credits for your tracing usage. As long as you have a valid credit card in your\\naccount, we?ll service your traces and deduct from your credit balance. You?ll be able to set alerts\\nand auto top-ups on credits if you choose.3 Personal accounts without a credit card on file will be\\nrate limited to 50,000 ingested events per hour and 500MB of storage per hour.4 Trace storage\\nincludes all submitted inputs, outputs, and metadata and is aggregated over all submission events.\\nDepending on the design of your application, trace data may be sent multiple times (e.g. once at the\\nstart of a trace step and again after it is complete)Questions and Answers?I?ve been using\\nLangSmith since before pricing took effect for new users. When will pricing go into effect for my\\naccount??If you?ve been using LangSmith already, your usage will be billable starting sometime in\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\65.pdf', 'page': 1}),\n",
       " Document(page_content='June. At that point if you want to add seats or use more than the monthly allotment of free traces,\\nyou will need to add a credit card to LangSmith or contact sales. If you are interested in the\\nEnterprise plan with higher rate limits and special deployment options, you can learn more or make\\na purchase by reaching out to sales@langchain.dev.Which plan is right for me??If you?re an\\nindividual developer, the Developer plan is a great choice for small projects.For teams that want to\\ncollaborate in LangSmith, check out the Plus plan. If you are an early-stage startup building an AI\\napplication, you may be eligible for our Startup plan with discounted prices and a generous free\\nmonthly trace allotment. Please reach out via our Startup Contact Form for more details.If you need\\nmore advanced administration, authentication and authorization, deployment options, support, or\\nannual invoicing, the Enterprise plan is right for you. Please reach out via our Sales Contact Form\\nfor more details.What is a seat??A seat is a distinct user inside your organization. We consider the\\ntotal number of users (including invited users) to determine the number of seats to bill.What is a\\ntrace??A trace is one complete invocation of your application chain or agent, evaluator run, or\\nplayground run. Here is an example of a single trace.What is an ingested event??An ingested event\\nis any distinct, trace-related data sent to LangSmith. This includes:Inputs, outputs and metadata\\nsent at the start of a run step within a traceInputs, outputs and metadata sent at the end of a run\\nstep within a traceFeedback on run steps or tracesI?ve hit my rate or usage limits. What can I do??If\\nyou?ve consumed the monthly allotment of free traces in your account, you can add a credit card on\\nthe Developer and Plus plans to continue sending traces to LangSmith. If you?ve hit the rate limits\\non your tier, you can upgrade to a higher plan to get higher limits, or reach out to\\nsupport@langchain.dev with questions.I have a developer account, can I upgrade my account to the\\nPlus or Enterprise plan??Every user will have a unique personal account on the Developer plan. We\\ncannot upgrade a Developer account to the Plus or Enterprise plans. If you?re interested in working\\nas a team, create a separate LangSmith Organization on the Plus plan. This plan can upgraded to\\nthe Enterprise plan at a later date.How will billing work??SeatsSeats are billed monthly on the first of\\nthe month in the future will be pro-rated if additional seats are purchased in the middle of the month.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\65.pdf', 'page': 2}),\n",
       " Document(page_content=\"Seats removed mid-month will not be credited.TracesAs long as you have a card on file in your\\naccount, we?ll service your traces and bill you on the first of the month for traces that you submitted\\nin the previous month. You will be able to set usage limits if you so choose to limit the maximum\\ncharges you could incur in any given month.Can I limit how much I spend on tracing??You can set\\nlimits on the number of traces that can be sent to LangSmith per month on\\nthe Plans and Billing settings page.noteWhile we do show you the dollar value of your usage limit for\\nconvenience, this limit evaluated\\nin terms of number of traces instead of dollar amount. For example, if you are approved for our\\nstartup plan tier where you are given a generous allotment of free traces, your usage limit will\\nnot automatically change.You are not currently able to set a spend limit in the product.How can my\\ntrack my usage so far this month??Under the Settings section for your Organization you will see\\nsubsection for Usage. There, you will able to see a graph of the daily nunber of billable LangSmith\\ntraces from the last 30, 60, or 90 days. Note that this data is delayed by 1-2 hours and so may trail\\nyour actual number of runs slightly for the current day.I have a question about my bill...?Customers\\non the Developer and Plus plan tiers should email support@langchain.dev. Customers on the\\nEnterprise plan should contact their sales representative directly.Enterprise plan customers are\\nbilled annually by invoice.What can I expect from Support??On the Developer plan,\\ncommunity-based support is available on Discord.On the Plus plan, you will also receive\\npreferential, email support at support@langchain.dev for LangSmith-related questions only and we'll\\ndo our best to respond within the next business day.On the Enterprise plan, you?ll get white-glove\\nsupport with a Slack channel, a dedicated customer success manager, and monthly check-ins to go\\nover LangSmith and LangChain questions. We can help with anything from debugging, agent and\\nRAG techniques, evaluation approaches, and cognitive architecture reviews. If you purchase the\\nadd-on to run LangSmith in your environment, we?ll also support deployments and new releases\\nwith our infra engineering team on-call.Where is my data stored??When using LangSmith hosted at\\nsmith.langchain.com, data is stored in GCP region us-central-1. If you?re on the Enterprise plan, we\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\65.pdf', 'page': 3}),\n",
       " Document(page_content='can deliver LangSmith to run on your kubernetes cluster in AWS, GCP, or Azure so that data never\\nleaves your environment.Which security frameworks is LangSmith compliant with??We have our\\nSOC 2 Type I certification and are GDPR compliant. We are in our SOC 2 Type II observation\\nperiod and expect to receive our certification in July. We will update this page accordingly. You can\\nrequest more information about our security policies and posture at trust.langchain.com.Will you\\ntrain on the data that I send LangSmith??We will not train on your data, and you own all rights to\\nyour data. See LangSmith Terms of Service for more information.Was this page\\nhelpful?PreviousLangChain off-the-shelf evaluatorsNextSelf-hostingPlansPlan\\nComparisonQuestions and AnswersI?ve been using LangSmith since before pricing took effect for\\nnew users. When will pricing go into effect for my account?Which plan is right for me?What is a\\nseat?What is a trace?What is an ingested event?I?ve hit my rate or usage limits. What can I do?I\\nhave a developer account, can I upgrade my account to the Plus or Enterprise plan?How will billing\\nwork?Can I limit how much I spend on tracing?How can my track my usage so far this month?I have\\na question about my bill...What can I expect from Support?Where is my data stored?Which security\\nframeworks is LangSmith compliant with?Will you train on the data that I send\\nLangSmith?CommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\65.pdf', 'page': 4}),\n",
       " Document(page_content='Self-hosting | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesConceptsReferencePricingSelf-hostingKubernetesDockerUsageRelease Notes\\n(Self-Hosted)Self-hostingSelf-hostingSelf-hosting LangSmith requires an enterprise license. Check\\nout the guides below for more information.?? KubernetesSelf-hosting LangSmith is an add-on to the\\nEnterprise Plan designed for our largest, most security-conscious customers. See our pricing page\\nfor more detail, and contact us at sales@langchain.dev if you want to get a license key to trial\\nLangSmith in your environment.?? DockerSelf-hosting LangSmith is an add-on to the Enterprise\\nPlan designed for our largest, most security-conscious customers. See our pricing page for more\\ndetail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in\\nyour environment.?? UsageThis guide will walk you through the process of using your self-hosted\\ninstance of LangSmith.?? Release Notes (Self-Hosted)If you are updating directly from LangSmith\\nv0.1.x and you wish to retain access to run data in the Langsmith UI after updating, you must first\\nupdate to v0.2.x and perform a data migration. Updating directly from v0.1.x to v0.3.x or later will', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\66.pdf', 'page': 0}),\n",
       " Document(page_content='result in data loss as the runs table in postgres will be dropped when deploying LangSmith v0.3.x or\\nhigher. Details are available at\\nhttps://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.mdPrevious\\nPricingNextKubernetesCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\66.pdf', 'page': 1}),\n",
       " Document(page_content=\"Self-hosting LangSmith on Kubernetes | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesConceptsReferencePricingSelf-hostingKubernetesDockerUsageRelease Notes\\n(Self-Hosted)Self-hostingKubernetesOn this pageSelf-hosting LangSmith on KubernetesEnterprise\\nLicense RequiredSelf-hosting LangSmith is an add-on to the Enterprise Plan designed for our\\nlargest, most security-conscious customers. See our pricing page for more detail, and contact us at\\nsales@langchain.dev if you want to get a license key to trial LangSmith in your environment.This\\nguide will walk you through the process of deploying LangSmith to a Kubernetes cluster. We will use\\nHelm to install LangSmith and its dependencies.We've successfully tested LangSmith on the\\nfollowing Kubernetes distributions:Google Kubernetes Engine (GKE)Amazon Elastic Kubernetes\\nService (EKS)Azure Kubernetes Service (AKS)OpenShiftMinikube and Kind (for development\\npurposes)For the most up-to-date information on deploying to kubernetes/upgrade instructions,\\nplease refer to the README in the LangSmith Helm Chart.Prerequisites?Ensure you have the\\nfollowing tools/items ready. Some items are marked optional but :A working Kubernetes cluster that\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\67.pdf', 'page': 0}),\n",
       " Document(page_content='you can access via kubectl. Your cluster should have the following minimum\\nrequirements:Recommended: At least 4 vCPUs, 16GB Memory availableYou may need to tune\\nresource requests/limits for all of our different services based off of organization size/usageValid\\nDynamic PV provisioner or PVs available on your cluster. You can verify this by running:kubectl get\\nstorageclassThe output should show at least one storage class with a provisioner that supports\\ndynamic provisioning. For example:  NAME            PROVISIONER             RECLAIMPOLICY  \\nVOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE  gp2 (default)  \\nkubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  161dHelmbrew install\\nhelmLangSmith License KeyYou can get this from your Langchain representative. Contact us at\\nsales@langchain.dev for more information.SSL(optional)This should be attachable to a load\\nbalancer that will be provisioned by your cloud provider. This will be used for the frontend\\nservice.Api Key SaltThis is a secret key that you can generate. It should be a random string of\\ncharacters.You can generate this using the following command:openssl rand -base64 32OpenAI\\nAPI Key(optional).Used for natural language search feature(beta). Can specify OpenAI key in\\nbrowser as well for the playground feature.OAuth Configuration(optional).You can configure oauth\\nusing the values.yaml file. You will need to provide a client_id and client_issuer_url for your OAuth\\nprovider.Note, we do rely on the OIDC Authorization Code with PKCE flow. We currently support\\nalmost anything that is OIDC compliant however Google does not support this flow.Without OAuth,\\nyou will not be able to create users or organizations.External Postgres(optional).You can configure\\nexternal postgres using the values.yaml file. You will need to provide connection parameters for your\\npostgres instance.If using a schema other than public, ensure that you do not have any other\\nschemas with the pgcrypto extension enabled, or you must include that in your search path.Note:\\nWe do only officially support Postgres versions >= 14.External Redis(optional).You can configure\\nexternal redis using the values.yaml file. You will need to provide a connection url for your redis\\ninstance.If using TLS, ensure that you use rediss:// instead of `redis://. E.g\\n\"rediss://langsmith-redis:6380/0?password=foo\"We only official support Redis versions >= 6. We', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\67.pdf', 'page': 1}),\n",
       " Document(page_content='also do not support sharded/clustered Redis(though you can point to a single node in a\\ncluster).External ClickHouse(optional).You can configure external clickhouse using the values.yaml\\nfile. You will need to provide several connection parameters for your ClickHouse instance.If using\\nTLS, make sure to set clickhouse.external.tls to true.We only officially support ClickHouse versions\\n>= 23. We also only support standalone ClickHouse or ClickHouse Cloud(not clustered or\\nreplicated).Configure your Helm Charts:?Create a new file called langsmith_config.yaml. This should\\nhave a similar structure to the values.yaml file in the LangSmith Helm Chart repository. Only include\\nthe values you want to override to avoid having to update the file every time the chart is\\nupdated.Override any values in the file. Refer to the documentation for the LangSmith Helm Chart to\\nsee all configurable values. Some values we recommend setting:ResourcesSSL(If on EKS or some\\nother cloud provider)Add an annotation to the frontend.service object to tell your cloud provider to\\nprovision a load balancer with said certificate attachedOpenAI Api KeyImagesOauthAn example\\nbare minimum config file langsmith_config.yaml:config:  langsmithLicenseKey: \"\"You can also see\\nsome example configurations in the examples directory of the Helm Chart repository here:\\nLangSmith Helm Chart Examples.Deploying to Kubernetes:?Verify that you can connect to your\\nKubernetes cluster(note: We highly suggest installing into an empty namespace)Run kubectl get\\npodsOutput should look something like:kubectl get pods                                                                      \\n                                                                                              ? langsmith-eks-2vauP7wf 21:07:46No\\nresources found in default namespace.Ensure you have the Langchain Helm repo added. (skip this\\nstep if you are using local charts)helm repo add langchain\\nhttps://langchain-ai.github.io/helm/\"langchain\" has been added to your repositoriesRun helm install\\nlangsmith langchain/langsmith --values langsmith_config.yaml --namespace <your-namespace>\\n--version <version>Output should look something like:NAME: langsmithLAST DEPLOYED: Fri Sep\\n17 21:08:47 2021NAMESPACE: langsmithSTATUS: deployedREVISION: 1TEST SUITE: NoneRun\\nkubectl get pods\\nOutput should now look something like:langsmith-backend-6ff46c99c4-wz22d       1/1     Running   0 ', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\67.pdf', 'page': 2}),\n",
       " Document(page_content='        3h2mlangsmith-frontend-6bbb94c5df-8xrlr      1/1     Running   0         \\n3h2mlangsmith-hub-backend-5cc68c888c-vppjj   1/1     Running   0         \\n3h2mlangsmith-playground-6d95fd8dc6-x2d9b    1/1     Running   0          3h2mlangsmith-postgres-0\\n                    1/1     Running   0          9hlangsmith-queue-5898b9d566-tv6q8         1/1     Running   0  \\n       3h2mlangsmith-redis-0                        1/1     Running   0          9hValidate your\\ndeployment:?Run kubectl get servicesOutput should look something like:NAME                    TYPE    \\n      CLUSTER-IP       EXTERNAL-IP                                                               PORT(S)       \\nAGElangsmith-backend       ClusterIP      172.20.140.77    <none>                                                      \\n             1984/TCP       35hlangsmith-frontend      LoadBalancer   172.20.253.251   <external ip>       \\n                                                     80:31591/TCP   35hlangsmith-hub-backend   ClusterIP     \\n172.20.112.234   <none>                                                                    1985/TCP      \\n35hlangsmith-playground    ClusterIP      172.20.153.194   <none>                                                      \\n             3001/TCP       9hlangsmith-postgres      ClusterIP      172.20.244.82    <none>                       \\n                                            5432/TCP       35hlangsmith-redis         ClusterIP      172.20.81.217   \\n<none>                                                                    6379/TCP       35hCurl the external ip of the\\nlangsmith-frontend service:curl <external\\nip>/api/tenants[{\"id\":\"00000000-0000-0000-0000-000000000000\",\"has_waitlist_access\":true,\"create\\nd_at\":\"2023-09-13T18:25:10.488407\",\"display_name\":\"Personal\",\"config\":{\"is_personal\":true,\"max_i\\ndentities\":1},\"tenant_handle\":\"default\"}]%Visit the external ip for the langsmith-frontend service on\\nyour browserThe LangSmith UI should be visible/operationalUsing LangSmith?Now that LangSmith\\nis running, you can start using it to trace your code. You can find more information on how to use\\nself-hosted LangSmith in the Self-Hosted Usage Guide.Frequently Asked Questions:?How can we\\nupgrade our application??We plan to release new minor versions of the LangSmith application every\\n6 weeks. This will include release notes and all changes should be backwards compatible. To\\nupgrade, you will need to follow the upgrade instructions in the Helm README and run a helm\\nupgrade langsmith --values <values file>How can we back up our application??Currently, we rely on', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\67.pdf', 'page': 3}),\n",
       " Document(page_content='PVCs/PV to power storage for our application. We strongly encourage setting up Persistent Volume\\nbackups or moving to a managed service for Postgres to support disaster recoveryHow does load\\nbalancing work/ingress work??Currently, our application spins up one load balancer using a k8s\\nservice of type LoadBalancer for our frontend. If you do not want to set up a load balancer you can\\nsimply port-forward the frontend and use that as your external ip for the application. We also have\\nan option for the chart to provision an ingress resource for the application.How can we authenticate\\nto the application??Currently, our self-hosted solution supports oauth as an authn solution. Note, we\\ndo offer a no-auth solution but highly recommend setting up oauth before moving into\\nproduction.How can I use External Postgres or Redis??You can configure external postgres or redis\\nusing the external sections in the values.yaml file. You will need to provide the connection\\nurl/params for the database/redis instance. Look at the configuration above example for more\\ninformation.What networking configuration is needed for the application??Our deployment only\\nneeds egress for a few things:Fetching images (If mirroring your images, this may not be\\nneeded)Talking to any LLMsTalking to any external services you may have configuredFetching\\nOAuth information\\nYour VPC can set up rules to limit any other access. Note: We require the X-Tenant-Id to be allowed\\nto be passed through to the backend service. This is used to determine which tenant the request is\\nfor.What resources should we allocate to the application??We recommend at least 4 vCPUs and\\n16GB of memory for our application.We have some default resources set in our values.yaml file.\\nYou can override these values to tune resource usage for your organization.If the metrics server is\\nenabled in your cluster, we also recommend enabling autoscaling on all deployments.Was this page\\nhelpful?PreviousSelf-hostingNextDockerPrerequisitesConfigure your Helm Charts:Deploying to\\nKubernetes:Validate your deployment:Using LangSmithFrequently Asked\\nQuestions:CommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\67.pdf', 'page': 4}),\n",
       " Document(page_content='', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\67.pdf', 'page': 5}),\n",
       " Document(page_content=\"Self-hosting LangSmith with Docker | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesConceptsReferencePricingSelf-hostingKubernetesDockerUsageRelease Notes\\n(Self-Hosted)Self-hostingDockerOn this pageSelf-hosting LangSmith with DockerEnterprise License\\nRequiredSelf-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most\\nsecurity-conscious customers. See our pricing page for more detail, and contact us at\\nsales@langchain.dev if you want to get a license key to trial LangSmith in your environment.This\\nguide provides instructions for installing and setting up your environment to run LangSmith locally\\nusing Docker. You can do this either by using the LangSmith SDK or by using Docker Compose\\ndirectly.Prerequisites?Ensure Docker is installed and running on your system. You can verify this by\\nrunning:docker infoIf you don't see any server information in the output, make sure Docker is\\ninstalled correctly and launch the Docker daemon.LangSmith License KeyYou can get this from your\\nLangchain representative. Contact us at sales@langchain.dev for more\\ninformation.SSL(optional)This should be attachable to a load balancer that will be provisioned by\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\68.pdf', 'page': 0}),\n",
       " Document(page_content='your cloud provider. This will be used for the frontend service.Api Key SaltThis is a secret key that\\nyou can generate. It should be a random string of characters.You can generate this using the\\nfollowing command:openssl rand -base64 32OpenAI API Key(optional).Used for natural language\\nsearch feature(beta). Can specify OpenAI key in browser as well for the playground feature.OAuth\\nConfiguration(optional).You can configure oauth using the .env file. You will need to provide a\\nclient_id and client_issuer_url for your OAuth provider.Note, we do rely on the OIDC Authorization\\nCode with PKCE flow. We currently support almost anything that is OIDC compliant however Google\\ndoes not support this flow.Without OAuth, you will not be able to create users or\\norganizations.External Postgres(optional).You can configure external postgres using the .env file.\\nYou will need to provide connection parameters for your postgres instance.If using a schema other\\nthan public, ensure that you do not have any other schemas with the pgcrypto extension enabled, or\\nyou must include that in your search path.Note: We do only officially support Postgres versions >=\\n14.External Redis(optional).You can configure external redis using the values.yaml file. You will\\nneed to provide a connection url for your redis instance.If using TLS, ensure that you use rediss://\\ninstead of `redis://. E.g \"rediss://langsmith-redis:6380/0?password=foo\"We only official support\\nRedis versions >= 6. We also do not support sharded/clustered Redis(though you can point to a\\nsingle node in a cluster).External ClickHouse(optional).You can configure external clickhouse using\\nthe .env file. You will need to provide several connection parameters for your ClickHouse instance.If\\nusing TLS, make sure to set clickhouse.external.tls to true.We only officially support ClickHouse\\nversions >= 23. We also only support standalone ClickHouse or ClickHouse Cloud(not clustered or\\nreplicated).Running via Docker Compose?The following explains how to run the LangSmith using\\nDocker Compose. This is the most flexible way to run LangSmith without Kubernetes. In production,\\nwe highly recommend using Kubernetes.1. Fetch the LangSmith docker-compose.yml file?You can\\nfind the docker-compose.yml file and related files in the LangSmith SDK repository here: LangSmith\\nDocker Compose FileCopy the docker-compose.yml file and all files in that directory from the\\nLangSmith SDK to your project directory.Ensure that you copy the users.xml file as well.2. Configure', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\68.pdf', 'page': 1}),\n",
       " Document(page_content=\"environment variables?Copy the .env.example file from the LangSmith SDK to your project directory\\nand rename it to .env. Then, set the following environment variables in the .env file:# Don't change\\nthis file. Instead, copy it to .env and change the values there. The default values will work out of the\\nbox as long as you provide your license\\nkey._LANGSMITH_IMAGE_VERSION=0.5.0LANGSMITH_LICENSE_KEY=your-license-key #\\nChange to your Langsmith license keyOPENAI_API_KEY=your-openai-api-key # Needed for Online\\nEvals and Magic Query featuresAUTH_TYPE=none # Set to oauth if you want to use\\nOAuth2.0OAUTH_CLIENT_ID=your-client-id # Required if\\nAUTH_TYPE=oauthOAUTH_ISSUER_URL=https://your-issuer-url # Required if\\nAUTH_TYPE=oauthAPI_KEY_SALT=super # Change to your desired API key salt. Can be any\\nrandom value. Must be set if\\nAUTH_TYPE=oauthPOSTGRES_DATABASE_URI=postgres:postgres@langchain-db:5432/postgre\\ns # Change to your database URI if using external postgres. Otherwise, leave it as\\nisREDIS_DATABASE_URI=redis://langchain-redis:6379 # Change to your Redis URI if using\\nexternal Redis. Otherwise, leave it as isLOG_LEVEL=warning # Change to your desired log\\nlevelMAX_ASYNC_JOBS_PER_WORKER=10 # Change to your desired maximum async jobs per\\nworker. We recommend 10/suggest spinning up more replicas of the queue worker if you need more\\nthroughputASYNCPG_POOL_MAX_SIZE=3 # Change the PG pool size based off your pg\\ninstance/requirements.CLICKHOUSE_HOST=langchain-clickhouse # Change to your Clickhouse\\nhost if using external Clickhouse. Otherwise, leave it as isCLICKHOUSE_USER=default # Change\\nto your Clickhouse user if neededCLICKHOUSE_DB=default # Change to your Clickhouse database\\nif neededCLICKHOUSE_PORT=8123 # Change to your Clickhouse port if\\nneededCLICKHOUSE_TLS=false # Change to true if you are using TLS to connect to Clickhouse.\\nOtherwise, leave it as isCLICKHOUSE_PASSWORD=password # Change to your Clickhouse\\npassword if neededCLICKHOUSE_NATIVE_PORT=9000 # Change to your Clickhouse native port if\\nneededYou can also set these environment variables in the docker-compose.yml file directly or\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\68.pdf', 'page': 2}),\n",
       " Document(page_content='export them in your terminal. We recommend setting them in the .env file.2. Start server?Start the\\nLangSmith application by executing the following command in your terminal:docker-compose upYou\\ncan also run the server in the background by running:docker-compose up -dIf the server starts\\ncorrectly, it will open up the UI in your browser at http://localhost.The LangSmith UI should be\\nvisible/operational and look like this:Stopping the server?docker-compose downChecking the\\nlogs?If, at any point, you want to check if the server is running and see the logs, rundocker-compose\\nlogsRunning via the LangSmith SDK?The following explains how to run the LangSmith using the\\nLangSmith SDK. This is a convenient wrapper around the docker-compose command.\\nWe recommend only using this in a local setting as it is not as flexible as using docker-compose\\ndirectly.1. Install the LangSmith SDK?The Python LangSmith SDKs exposes a langsmith command\\nline tool.First, install a recent version of the langsmith package:Python/Pip: pip install -U\\nlangsmithThis will install the LangSmith Client and the bundled command line tool.2. Start\\nserver?Start the LangSmith tracing server by executing the following command in your\\nterminal:langsmith start --langsmith-license-key=<YOUR_LANGSMITH_LICENSE_KEY>\\n--version=<LANGSMITH_VERSION> --openai-api-key=<YOUR_OPENAI_API_KEY>If the server\\nstarts correctly, it will open up the UI in your browser at http://localhost.The LangSmith UI should be\\nvisible/operational and look like this:Stopping the server?To stop the server, run the following\\ncommand:langsmith stopChecking the logs?If, at any point, you want to check if the server is\\nrunning and see the logs, runlangsmith logsUsing LangSmith?Now that LangSmith is running, you\\ncan start using it to trace your code. You can find more information on how to use self-hosted\\nLangSmith in the Self-Hosted Usage Guide.Frequently Asked Questions?How can we upgrade our\\napplication??We plan to release new minor versions of the LangSmith application every 6 weeks.\\nThis will include release notes and all changes should be backwards compatible.To upgrade, you\\nwill need to restart your LangSmith instance specifying the new version. You can do this by updating\\nthe _LANGSMITH_IMAGE_VERSION environment variable in the .env file or in the\\ndocker-compose.yml file.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\68.pdf', 'page': 3}),\n",
       " Document(page_content='You may also need to update your docker-compose file to match the file in the LangSmith SDK\\nGitHub repository.How can we back up our application??The docker solution uses docker volumes\\nto store data. You can back up the data by backing up the volumes.How can we authenticate to the\\napplication??Currently, our self-hosted solution supports oauth as an authn solution.Note, we do\\noffer a no-auth solution but highly recommend setting up oauth before moving into production.How\\ncan I use External Postgres or Redis??You can configure external postgres or redis using the\\nexternal sections in the .env file. You will need to provide the connection url/params for the\\ndatabase/redis instance. Look at the .env.example file for more information.What networking\\nconfiguration is needed for the application??Our deployment only needs egress for a few\\nthings:Fetching images (If mirroring your images, this may not be needed)Talking to any LLMsYour\\nVPC can set up rules to limit any other access.Note: We require the X-Tenant-Id to be allowed to be\\npassed through to the backend service. This is used to determine which tenant the request is\\nfor.What resources should we allocate to the application??We recommend at least 4 vCPUs and\\n16GB of memory for our application. This is a rough estimate and can vary based on the number of\\nusers and the size of the data you are working with.Increasing the number of replicas?If you need\\nmore throughput, you can increase the number of replicas of the queue worker by running the\\nfollowing command:docker-compose up --scale langchain-queue=5This will start 5 replicas of the\\nqueue worker. You can change the number to whatever you need. Note that these containers are\\nfairly CPU intensive, and you should ensure you have enough resources to support the number of\\nreplicas you are starting.Was this page\\nhelpful?PreviousKubernetesNextUsagePrerequisitesRunning via Docker Compose1. Fetch the\\nLangSmith docker-compose.yml file2. Configure environment variables2. Start serverStopping the\\nserverChecking the logsRunning via the LangSmith SDK1. Install the LangSmith SDK2. Start\\nserverStopping the serverChecking the logsUsing LangSmithFrequently Asked\\nQuestionsCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\68.pdf', 'page': 4}),\n",
       " Document(page_content='2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\68.pdf', 'page': 5}),\n",
       " Document(page_content='How to use your self-hosted instance of LangSmith | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesConceptsReferencePricingSelf-hostingKubernetesDockerUsageRelease Notes\\n(Self-Hosted)Self-hostingUsageOn this pageHow to use your self-hosted instance of LangSmithThis\\nguide will walk you through the process of using your self-hosted instance of LangSmith.Self-Hosted\\nLangSmith Instance RequiredThis guide assumes you have already deployed a self-hosted\\nLangSmith instance. If you have not, please refer to the kubernetes deployment guide or the docker\\ndeployment guide.Using your deployment:?Once you have deployed your instance, you can access\\nthe LangSmith UI at http://<external ip>.The backend API will be available at http://<external ip>/api\\nand the hub API will be available at http://<external ip>/api-hub.To use the LangSmith API, you will\\nneed to set the following environment variables in your\\napplication:LANGCHAIN_ENDPOINT=http://<external\\nip>/apiLANGCHAIN_HUB_API_URL=http://<external ip>/api-hubLANGCHAIN_API_KEY=foo # Set\\nto a legitimate API key if using OAuthYou can also configure these variables directly in the', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\69.pdf', 'page': 0}),\n",
       " Document(page_content=\"LangSmith SDK client:import langsmithlangsmith_client = langsmith.Client(    api_key='<api_key>',   \\napi_url='http://<external ip>/api',)import langchainhublangchainhub.Client(    api_key='<api_key>',   \\napi_url='http://<external ip>/api-hub')After setting the above, you should be able to run your code\\nand see the results in your self-hosted instance.\\nWe recommend running through the quickstart guide to get a feel for how to use LangSmith.Was\\nthis page helpful?PreviousDockerNextRelease Notes (Self-Hosted)Using your\\ndeployment:CommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\69.pdf', 'page': 1}),\n",
       " Document(page_content=\"Create an organization | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupCreate an account and API keyCreate an organizationSet up access controlSet up\\nbilling for your LangSmith accountSet up a workspaceTracingDatasetsEvaluationHuman\\nfeedbackMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesSetupCreate an organizationCreate an organizationWhen you log in for the first time, a\\npersonal organization will be created for you automatically. If you'd like to collaborate with others,\\nyou can create a separate organization and invite your team members to join.To do this, head to the\\nsettings page and click Create Organization.\\nShared organizations require a credit card before they can be used. You will need to set up billing to\\nproceed.Was this page helpful?PreviousCreate an account and API keyNextSet up access\\ncontrolCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\7.pdf', 'page': 0}),\n",
       " Document(page_content='', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\7.pdf', 'page': 1}),\n",
       " Document(page_content='LangSmith Release Notes | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesConceptsReferencePricingSelf-hostingKubernetesDockerUsageRelease Notes\\n(Self-Hosted)Self-hostingRelease Notes (Self-Hosted)On this pageLangSmith Release NotesnoteIf\\nyou are updating directly from LangSmith v0.1.x and you wish to retain access to run data in the\\nLangsmith UI after updating, you must first update to v0.2.x and perform a data migration. Updating\\ndirectly from v0.1.x to v0.3.x or later will result in data loss as the runs table in postgres will be\\ndropped when deploying LangSmith v0.3.x or higher. Details are available at\\nhttps://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.mdWeek of\\nMay 13, 2024 - LangSmith v0.5?LangSmith v0.5 improves performance and reliability, adds features\\nto improve regression testing, production monitoring and automation, and implements Role-Based\\nAccess Controls (RBAC).Breaking changes?We will be dropping support for API keys on July 1st\\n2024 in favor of personal access tokens (PATs) and Service Keys. We recommend using PATs and\\nService Keys for all new integrations. API keys prefixed with ls__ will NO LONGER work as of', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\70.pdf', 'page': 0}),\n",
       " Document(page_content='LangSmith Helm release v0.7 to be released on or after July 1, 2024.New Features?Role-Based\\nAccess Controls. See: https://blog.langchain.dev/access-control-updates-for-langsmith/Improved\\nregression testing experience. See: https://blog.langchain.dev/regression-testing/Improved\\nproduction monitoring and automation: See:\\nhttps://blog.langchain.dev/langsmith-production-logging-automations/Performance and Reliability\\nChanges?Split ingest, session deletion, and automation jobs to execute within separate resource\\npools.Infrastructure changes?As of LangSmith v0.4, Clickhouse persistence now uses 50Gi of\\nstorage by default. You can adjust this by changing the\\xa0clickhouse.statefulSet.persistence.size\\xa0value\\nin your\\xa0values.yaml\\xa0file.If your existing configuration cannot support 50Gi, you may need to resize\\nyour existing storage class or set\\xa0clickhouse.statefulSet.persistence.size\\xa0to the previous default\\nvalue of\\xa08Gi.It is strongly recommend that you monitor the consumption of storage on your\\nClickhouse volume to ensure the volume does not near full capacity, which may cause run ingest to\\nbehave erratically.New Platform-Backend service used internally. This service also uses it?s own\\nimage. You may need to adjust your helm values files accordingly.Admin changes?Added new\\nRole-Based Access Controls. For more details see the Admin and Set Up Access Control sections\\nof the docs.Introduction of PATs and Service Keys. Old API keys have been migrated to service\\nkeys.Deprecation notices?With the release of v0.5:LangSmith v0.4.x and earlier are now in\\nmaintenance mode and may only receive critical security fixes.Week of March 25, 2024 - LangSmith\\nv0.4?LangSmith 0.4 improves performance and reliability, implements a new asynchronous queue\\nworker to optimize run ingests, and an API key salt parameter.Breaking changes?This release adds\\nan API key salt parameter. This previously defaulted to your LangSmith License Key. For updates\\nfrom earlier versions you should set this parameter to your license key to ensure backwards\\ncompatibility. Using a new api key salt will invalidate all existing api keys.This release makes\\nClickhouse persistence use 50Gi of storage by default. You can adjust this by changing the\\nclickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration\\ndoes not configure persistence already, you will need to resize your existing pvc or set', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\70.pdf', 'page': 1}),\n",
       " Document(page_content='clickhouse.statefulSet.persistence.size to the previous default value of 8Gi.Performance and\\nReliability Changes?Implemented a new asynchronous queue worker and cached token encodings\\nto improve performance when ingesting traces, reducing the delay between ingest and display in the\\nLangSmith UI.Infrastructure changes?Some our image repositories have been updated. You can\\nsee the root repositories in our values.yaml file and may need to update mirrors to pick up the new\\nimages.Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by\\nchanging the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing\\nconfiguration cannot support 50Gi, you may need to resize your existing storage class or set\\nclickhouse.statefulSet.persistence.size to the previous default value of 8Gi.Consolidation of\\nhubBackend and backend services. We now use one service to serve both of these endpoints. This\\nshould not impact your application.Admin changes?Added an API key salt parameter in values.yml.\\nThis can be set to a custom value and changing it will invalidate all existing api keys.Changed the\\nOAuth flow to leverage Access Tokens instead of OIDC ID tokens. This change should not impact\\nthe end user experience.Added scripts to enable feature flags in self-hosted environments for use in\\npreviewing pre-release features. Details are available at\\nhttps://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/ADD-FEATURE-FLAG.mdDep\\nrecation notices?With the release of 0.4:LangSmith 0.3.x and earlier are now in maintenance mode\\nand may only receive critical security fixes.Week of Februrary 21, 2024 - LangSmith\\nv0.3?LangSmith 0.3 improves performance and reliability, adds improved monitoring charts group\\nby metadata and tag, and adds cost tracking.Breaking changes?This release will drop the postgres\\nrun tables - if you are making a migration from LangSmith v0.1 and wish to retain run data, you must\\nfirst update to v0.2 and perform a data migration. See\\nhttps://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md for\\nadditional detailsPerformance and Reliability Changes?Continued performance when ingesting\\ntraces, reducing the delay between ingest and display in the LangSmith UI.Admin\\nchanges?NoneDeprecation notices?With the release of 0.3:LangSmith 0.2.x and earlier are now in', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\70.pdf', 'page': 2}),\n",
       " Document(page_content='maintenance mode and may only receive critical security fixes.Week of January 29, 2024 -\\nLangSmith v0.2?LangSmith 0.2 improves performance and reliability, adds a updated interface for\\nreviewing trace data, and adds support for batch processing of traces.Requirements?This release\\nrequires langsmith-sdk version ? 0.0.71 (Python) and ? 0.0.56 (JS/TS) to support changes in\\npagination of API results. Older versions will only return the first 100 results when querying an\\nendpoint.Breaking changes?The search syntax for metadata in runs has changed and limits support\\nfor nested JSON to a single level. If you are supplying custom metadata in traces, you should flatten\\nyour metadata structure in order to allow it to be searchable, (e.g. {\"user_id\": ..., \"user_name\":...,})\\nand then search using has(metadata, \\'{\"user_name\": ...}\\')Performance and Reliability\\nChanges?Improved performance when ingesting traces, reducing the delay between ingest and\\ndisplay in the LangSmith UI.Improved performance for updates and deletes on annotation\\nlabels.Added pagination of API responses.Fixed an issue impacting natural language\\nsearches.Infrastructure Changes?Added the clickhouse database service. Run results will now be\\nstored in ClickHouse instead of Postgres to improve performance and scalability and reduce delays\\nin the time it takes for runs to appear in LangSmith.Note that if you wish to retain access to run data\\nin the Langsmith UI after updating, a data migration will need to be performed. Details are available\\nat https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.mdAdmin\\nchanges?Increased the maximum number of users per organization from 5 to 100 for new\\norganizations.Deprecation notices?With the release of 0.2:LangSmith 0.1.x is now in maintenance\\nmode and may only receive critical security fixes.Was this page helpful?PreviousUsageWeek of May\\n13, 2024 - LangSmith v0.5Breaking changesNew FeaturesPerformance and Reliability\\nChangesInfrastructure changesAdmin changesDeprecation noticesWeek of March 25, 2024 -\\nLangSmith v0.4Breaking changesPerformance and Reliability ChangesInfrastructure changesAdmin\\nchangesDeprecation noticesWeek of Februrary 21, 2024 - LangSmith v0.3Breaking\\nchangesPerformance and Reliability ChangesAdmin changesDeprecation noticesWeek of January\\n29, 2024 - LangSmith v0.2RequirementsBreaking changesPerformance and Reliability', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\70.pdf', 'page': 3}),\n",
       " Document(page_content='ChangesInfrastructure ChangesAdmin changesDeprecation\\nnoticesCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\70.pdf', 'page': 4}),\n",
       " Document(page_content='Set up access control | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupCreate an account and API keyCreate an organizationSet up access controlSet up\\nbilling for your LangSmith accountSet up a workspaceTracingDatasetsEvaluationHuman\\nfeedbackMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesSetupSet up access controlOn this pageSet up access controlnoteRBAC (Role-Based Access\\nControl) is a feature that is only available to Enterprise customers. If you are interested in this\\nfeature, please contact our sales team at sales@langchain.dev\\nOther plans default to using the Admin role for all users. Read more about roles under admin\\nconceptsLangSmith relies on RBAC to manage user permissions. This allows you to control who\\ncan access your LangSmith workspace and what they can do within the account.\\nOnly users with the workspace:manage permission can can manage access control.Create a\\nrole?By default, LangSmith comes with a set of system roles:Admin - has full access to all resources\\nwithin the workspaceViewer - has read-only access to all resources within the workspaceEditor - has', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\8.pdf', 'page': 0}),\n",
       " Document(page_content='full permissions except for workspace management (adding/removing users, changing roles,\\nconfiguring service keys)If these do not fit your access model, admins can create custom roles to\\nsuit your needs.To create a role, navigate to the Roles tab in the Members section of the Settings\\npageClick on the Create Role button to create a new role. You should see a form like the one\\nbelow:Assign permissions for the different LangSmith resources that you want to control access\\nto.Assign a role to a user?Once you have your roles set up, you can assign them to users. To\\nassign a role to a user, navigate to the Members tab in the Members section of the Settings\\npageEach user will have a Role dropdown that you can use to assign a role to them.You can also\\ninvite new users with a given role.Was this page helpful?PreviousCreate an organizationNextSet up\\nbilling for your LangSmith accountCreate a roleAssign a role to a\\nuserCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\8.pdf', 'page': 1}),\n",
       " Document(page_content=\"Set up billing for your LangSmith account | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to\\nguidesSetupCreate an account and API keyCreate an organizationSet up access controlSet up\\nbilling for your LangSmith accountSet up a workspaceTracingDatasetsEvaluationHuman\\nfeedbackMonitoring and automationsPromptsConceptsReferencePricingSelf-hostingHow-to\\nguidesSetupSet up billing for your LangSmith accountOn this pageSet up billing for your LangSmith\\naccountTo set up billing for your LangSmith organization, head to the\\nPlans and Billing page under Settings. Depending\\non your organization's settings, you will be given a different walkthrough to get started.noteIf you\\ncreated your LangSmith organization before pricing was introduced on April 2nd, 2024, please skip\\nto the final section.Developer Plan: set up billing on your personal organization?Personal\\norganizations are limited to 5000 traces per month until a credit card is added. You can\\nadd a credit card on the Plans and Billing page as follows:1. Click Set up Billing?2. Optionally set a\\nusage limit?On the modal that pops up, if desired, set a usage limit on the maximum number of\", metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\9.pdf', 'page': 0}),\n",
       " Document(page_content='traces you can send per month to LangSmith. This\\nis configureable in the future. See our pricing Q&A for more information on user defined Usage\\nLimits.3. Add your credit card info?After this step, you will no longer be rate limited to 5000 traces,\\nand will be charged for any excess\\ntraces at rates specified on our pricing page.Plus Plan: set up billing on a shared organization?If you\\nhave not yet created an organization, please do so by following this guide. This walkthrough\\nassumes you are\\nalready in a new organization.noteNew organizations are not usable until a credit card is entered.\\nAfter you complete the following steps, you will\\ngain complete access to LangSmith.1. Click Subscribe on the Plus page?noteIf you are a startup\\nbuilding with AI, please instead click Apply Now on our Startup Plan. You may be\\neligible for discounted prices and a generous free, monthly trace allotment.2. Review your existing\\nmembers / set an optional usage limit?Before subscribing, LangSmith lets you remove any added\\nusers that you would not\\nlike to be charged for. It also lets you, if desired, set a usage limit on the maximum number of traces\\nyou can send per month to LangSmith.\\nSee our pricing Q&A for more information on user defined Usage Limits.3. Enter your credit card\\ninfo?Once this step is complete, your org should now have access to the rest of LangSmith!Set up\\nbilling for accounts created before pricing was introduced on April 2, 2024?noteA small subset of\\nBeta Organizations do not have the upgrade flow enabled by default. If you do not see a\\nPlans and Billing page under your LangSmith settings, please reach out to support@langchain.dev,\\nand we will\\nhelp you upgrade.If you joined LangSmith before pricing was introduced April 2, 2024, you will have\\nthe option to upgrade your\\nexisting account to use pricing. You can do so by:Heading to the Plans and Billing page under\\nSettingsClicking Set up BillingFollowing the modal to enter your credit card. If you are on a Personal', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\9.pdf', 'page': 1}),\n",
       " Document(page_content='Organization, this will add you\\nto the Developer plan. If you are on a shared Organization, this will add you to the Plus plan. For\\nmore information,\\nplease view the above walkthroughs for Developer or Plus respectively, starting at step 2.Was this\\npage helpful?PreviousSet up access controlNextSet up a workspaceDeveloper Plan: set up billing\\non your personal organization1. Click Set up Billing2. Optionally set a usage limit3. Add your credit\\ncard infoPlus Plan: set up billing on a shared organization1. Click Subscribe on the Plus page2.\\nReview your existing members / set an optional usage limit3. Enter your credit card infoSet up billing\\nfor accounts created before pricing was introduced on April 2,\\n2024CommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\9.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_doc(directory):\n",
    "    file_loader = PyPDFDirectoryLoader(directory)\n",
    "    documents = file_loader.load()\n",
    "    return documents\n",
    "langchain_comp_doc = read_doc(\"C:/Users/samar/Desktop/Langchat/data/Langchain/Components/pdf_files\")\n",
    "def chunk_data(docs,chunk_size=800,chunk_overlap=20):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    langchain_comp_doc= text_splitter.split_documents(docs)\n",
    "    return langchain_comp_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_expression_doc =read_doc(\"C:/Users/samar/Desktop/Langchat/data/Langchain/LangChain Expression Language/pdf_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_doc(directory):\n",
    "    file_loader = PyPDFDirectoryLoader(directory)\n",
    "    documents = file_loader.load()\n",
    "    return documents\n",
    "\n",
    "def chunk_data(docs,chunk_size=800,chunk_overlap=20):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    langchain_comp_doc= text_splitter.split_documents(docs)\n",
    "    return langchain_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://838581f9-41df-47dc-a452-47ea8f1e1edc.us-east4-0.gcp.cloud.qdrant.io:6333\"\n",
    "api_key = \"7QBIL3BHx33BOqbHlsSIrn5in7kTX4OLzn5FAXQYPFP7Y1KFYbagpg\"\n",
    "qdrant = Qdrant.from_documents(\n",
    "    doc,\n",
    "    embeddings,\n",
    "    url=url,\n",
    "    prefer_grpc=True,\n",
    "    api_key=api_key,\n",
    "    collection_name=\"my_collections\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Qdrant' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://838581f9-41df-47dc-a452-47ea8f1e1edc.us-east4-0.gcp.cloud.qdrant.io:6333\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m7QBIL3BHx33BOqbHlsSIrn5in7kTX4OLzn5FAXQYPFP7Y1KFYbagpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m qdrant1 \u001b[38;5;241m=\u001b[39m \u001b[43mQdrant\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[0;32m      4\u001b[0m     langchain_comp_doc,\n\u001b[0;32m      5\u001b[0m     embeddings,\n\u001b[0;32m      6\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m      7\u001b[0m     prefer_grpc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[0;32m      9\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Qdrant' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://838581f9-41df-47dc-a452-47ea8f1e1edc.us-east4-0.gcp.cloud.qdrant.io:6333\"\n",
    "api_key = \"7QBIL3BHx33BOqbHlsSIrn5in7kTX4OLzn5FAXQYPFP7Y1KFYbagpg\"\n",
    "qdrant1 = Qdrant.from_documents(\n",
    "    langchain_comp_doc,\n",
    "    embeddings,\n",
    "    url=url,\n",
    "    prefer_grpc=True,\n",
    "    api_key=api_key,\n",
    "    collection_name=\"langchain\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to Add observability to your LLM application\"\n",
    "found_docs = qdrant.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Add observability to your LLM application | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsAdd observability to\\nyour LLM applicationEvaluate your LLM applicationOptimize a classifierHow-to\\nguidesConceptsReferencePricingSelf-hostingTutorialsAdd observability to your LLM applicationOn\\nthis pageAdd observability to your LLM applicationObservability is important for any software\\napplication, but especially so for LLM applications.\\nLLMs are non-deterministic by nature, meaning they can produce unexpected results.\\nThis makes them trickier than normal to debug.Luckily, this is where LangSmith can help!\\nLangSmith has LLM-native observability, allowing you to get meaningful insights into your\\napplication.Note that observability is important throughout all stages of application development -\\nfrom prototyping, to beta testing, to production.\\nThere are different considerations at all stages, but they are all intricately tied together.\\nIn this tutorial we walk through the natural progression.Let's assume that we're building a simple\\nRAG application using the OpenAI SDK.\", metadata={'page': 0, 'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\2.pdf', '_id': 'ba73bda6-57c1-46cb-b59e-56a29afb9197', '_collection_name': 'my_collections'}),\n",
       " Document(page_content='In order to do this, you can simply hover over a datapoint in the monitoring chart.\\nWhen you do this, you will be able to click the datapoint.\\nThis will lead you back to the runs table with a filtered view:Conclusion?In this tutorial you saw how\\nto set up your LLM application with best-in-class observability.\\nNo matter what stage your application is in, you will still benefit from observability.If you have more\\nin-depth questions about observability, check out the how-to section for guides on topics like testing,\\nprompt management, and more.Observability is not the only thing LangSmith can help with!\\nIt can also help with evaluation, optimization, and more!\\nCheck out the other tutorials to see how to get started with those.Was this page\\nhelpful?PreviousTutorialsNextEvaluate your LLM applicationPrototypingSet up your\\nenvironmentTrace your LLM callsTrace the whole chainTrace the retrieval stepBeta\\nTestingCollecting FeedbackLogging MetadataProductionMonitoringA/B\\nTestingDrilldownConclusionCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.', metadata={'page': 7, 'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\2.pdf', '_id': 'ac39142d-526f-4402-a1c1-69f14d660605', '_collection_name': 'my_collections'}),\n",
       " Document(page_content='Evaluate your LLM application | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsAdd observability to\\nyour LLM applicationEvaluate your LLM applicationOptimize a classifierHow-to\\nguidesConceptsReferencePricingSelf-hostingTutorialsEvaluate your LLM applicationOn this\\npageEvaluate your LLM applicationIt can be hard to measure the performance of your application\\nwith respect to criteria important you or your users.\\nHowever, doing so is crucial, especially as you iterate on your application.\\nIn this guide we will go over how to test and evaluate your application.\\nThis allows you to measure how well your application is performing over a fixed set of data.\\nBeing able to get this insight quickly and reliably will allow you to iterate with confidence.At a high\\nlevel, in this tutorial we will go over how to:Create an initial golden dataset to measure\\nperformanceDefine metrics to use to measure performanceRun evaluations on a few different\\nprompts or modelsCompare results manuallyTrack results over timeSet up automated testing to run\\nin CI/CDFor more information on the evaluation workflows LangSmith supports, check out the', metadata={'page': 0, 'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\3.pdf', '_id': '7d043a46-e795-4661-8cee-f2efe57d0d77', '_collection_name': 'my_collections'}),\n",
       " Document(page_content=\"Tutorials | ???? LangSmith\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsAdd observability to\\nyour LLM applicationEvaluate your LLM applicationOptimize a classifierHow-to\\nguidesConceptsReferencePricingSelf-hostingTutorialsTutorialsNew to LangSmith? This is the place\\nto start. Here, you'll find a hands-on introduction to key LangSmith workflows.Add observability to\\nyour LLM applicationEvaluate your LLM applicationOptimize a classifierWas this page\\nhelpful?PreviousQuick startNextAdd observability to your LLM\\napplicationCommunityDiscordTwitterGitHubDocs CodeLangSmith\\nSDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ©\\n2024 LangChain, Inc.\", metadata={'page': 0, 'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langsmith\\\\Pdf_files\\\\1.pdf', '_id': 'c691da8b-0bd7-4292-ae69-20aba77f5985', '_collection_name': 'my_collections'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samar\\Desktop\\Langchat\\lang\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 1024,\"max_new_tokens\":500}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(llm\u001b[38;5;241m=\u001b[39m\u001b[43mllm\u001b[49m, chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m, retriever\u001b[38;5;241m=\u001b[39mqdrant1\u001b[38;5;241m.\u001b[39mas_retriever(search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=qdrant1.as_retriever(search_kwargs={\"k\": 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain me code snippets for rag\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mqa\u001b[49m\u001b[38;5;241m.\u001b[39mrun(query)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'qa' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"Explain me code snippets for rag\"\n",
    "response = qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Retrieval | ??? LangChain\n",
      "Skip to main contentLangChain v0.2 is coming soon! Preview the new docs\n",
      "here.ComponentsIntegrationsGuidesAPI\n",
      "ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTube???LangSmith\n",
      "LangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS\n",
      "Docs?SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument loadersText\n",
      "splittersEmbedding modelsVector\n",
      "storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsRetrievalOn this\n",
      "pageRetrievalMany LLM applications require user-specific data that is not part of the model's\n",
      "training set.\n",
      "The primary way of accomplishing this is through Retrieval Augmented Generation (RAG).\n",
      "In this process, external data is retrieved and then passed to the LLM when doing the generation\n",
      "step.LangChain provides all the building blocks for RAG applications - from simple to complex.\n",
      "This section of the documentation covers everything related to the retrieval step - e.g. the fetching of\n",
      "\n",
      "Question: Explain me rag\n",
      "Helpful Answer: Retrieval Augmented Generation (RAG) is a technique used in language models where external data is retrieved and then passed to the model during the generation step. This is useful when the model needs user-specific data that wasn't part of its training set. LangChain offers components for building RAG applications, from simple to complex, including tools for retrieving data.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this tutorial, we walk through adding observability to a simple RAG application using the OpenAI SDK. To do this, follow the steps outlined in the LangSmith API Docs under \"Add observability to your LLM application\".\n"
     ]
    }
   ],
   "source": [
    "start = response.find(\"Helpful Answer:\")\n",
    "end = response.find(\"\\n\", start) if \"\\n\" in response[start:] else len(response)\n",
    "helpful_answer = response[start:end].replace(\"Helpful Answer: \", \"\")\n",
    "print(helpful_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert in LangChain, a framework for building applications with large language models. Your task is to answer questions with the utmost accuracy, providing clear and detailed explanations. When relevant, include code sequences to illustrate your points and guide users effectively. Your responses should be informative, concise, and geared towards helping users understand and apply LangChain concepts and techniques.\\n\\n{context}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, question_answering_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are an expert in LangChain, a framework for building applications with large language models. Your task is to answer questions with the utmost accuracy, providing clear and detailed explanations. When relevant, include code sequences to illustrate your points and guide users effectively. Your responses should be informative, concise, and geared towards helping users understand and apply LangChain concepts and techniques.\\n\\npossible, as those would generically seem to be the strongest semantically related pieces of\\ntext.How the text is split: by list of characters.How the chunk size is measured: by number of\\ncharacters.%pip install -qU langchain-text-splitters# This is a long document we can split up.with\\nopen(\"../../state_of_the_union.txt\") as f:    state_of_the_union = f.read()from langchain_text_splitters\\nimport RecursiveCharacterTextSplitterAPI Reference:RecursiveCharacterTextSplittertext_splitter =\\nRecursiveCharacterTextSplitter(    # Set a really small chunk size, just to show.    chunk_size=100,   \\nchunk_overlap=20,    length_function=len,    is_separator_regex=False,)texts =\\ntext_splitter.create_documents([state_of_the_union])print(texts[0])print(texts[1])page_content=\\'Mada\\nm Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress\\nand\\'page_content=\\'of Congress and the Cabinet. Justices of the Supreme Court. My fellow\\nAmericans.\\'text_splitter.split_text(state_of_the_union)[:2][\\'Madam Speaker, Madam Vice President,\\nour First Lady and Second Gentleman. Members of Congress and\\', \\'of Congress and the Cabinet.\\nJustices of the Supreme Court. My fellow Americans.\\']Splitting text from languages without word\\nboundaries?Some writing systems do not have word boundaries, for example Chinese, Japanese,\\nand Thai. Splitting text with the default separator list of [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"] can cause words to be\\nsplit between chunks. To keep words together, you can override the list of separators to include\\nadditional punctuation:Add ASCII full-stop \".\", Unicode fullwidth full stop \"?\" (used in Chinese text),\\nand ideographic full stop \"?\" (used in Japanese and Chinese)Add Zero-width space used in Thai,\\nMyanmar, Kmer, and Japanese.Add ASCII comma \",\", Unicode fullwidth comma \"?\", and Unicode\\nideographic comma \"?\"text_splitter = RecursiveCharacterTextSplitter(    separators=[        \"\\\\n\\\\n\",       \\n\"\\\\n\",        \" \",        \".\",        \",\",        \"\\\\u200b\",  # Zero-width space        \"\\\\uff0c\",  # Fullwidth comma       \\n\"\\\\u3001\",  # Ideographic comma        \"\\\\uff0e\",  # Fullwidth full stop        \"\\\\u3002\",  # Ideographic full\\nstop        \"\",    ],    # Existing args)Help us out by providing feedback on this documentation\\npage:PreviousRecursively split JSONNextSemantic ChunkingSplitting text from languages without\\nword\\nboundariesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ©\\n\\npossible, as those would generically seem to be the strongest semantically related pieces of\\ntext.How the text is split: by list of characters.How the chunk size is measured: by number of\\ncharacters.%pip install -qU langchain-text-splitters# This is a long document we can split up.with\\nopen(\"../../state_of_the_union.txt\") as f:    state_of_the_union = f.read()from langchain_text_splitters\\nimport RecursiveCharacterTextSplitterAPI Reference:RecursiveCharacterTextSplittertext_splitter =\\nRecursiveCharacterTextSplitter(    # Set a really small chunk size, just to show.    chunk_size=100,   \\nchunk_overlap=20,    length_function=len,    is_separator_regex=False,)texts =\\ntext_splitter.create_documents([state_of_the_union])print(texts[0])print(texts[1])page_content=\\'Mada\\nm Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress\\nand\\'page_content=\\'of Congress and the Cabinet. Justices of the Supreme Court. My fellow\\nAmericans.\\'text_splitter.split_text(state_of_the_union)[:2][\\'Madam Speaker, Madam Vice President,\\nour First Lady and Second Gentleman. Members of Congress and\\', \\'of Congress and the Cabinet.\\nJustices of the Supreme Court. My fellow Americans.\\']Splitting text from languages without word\\nboundaries?Some writing systems do not have word boundaries, for example Chinese, Japanese,\\nand Thai. Splitting text with the default separator list of [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"] can cause words to be\\nsplit between chunks. To keep words together, you can override the list of separators to include\\nadditional punctuation:Add ASCII full-stop \".\", Unicode fullwidth full stop \"?\" (used in Chinese text),\\nand ideographic full stop \"?\" (used in Japanese and Chinese)Add Zero-width space used in Thai,\\nMyanmar, Kmer, and Japanese.Add ASCII comma \",\", Unicode fullwidth comma \"?\", and Unicode\\nideographic comma \"?\"text_splitter = RecursiveCharacterTextSplitter(    separators=[        \"\\\\n\\\\n\",       \\n\"\\\\n\",        \" \",        \".\",        \",\",        \"\\\\u200b\",  # Zero-width space        \"\\\\uff0c\",  # Fullwidth comma       \\n\"\\\\u3001\",  # Ideographic comma        \"\\\\uff0e\",  # Fullwidth full stop        \"\\\\u3002\",  # Ideographic full\\nstop        \"\",    ],    # Existing args)Help us out by providing feedback on this documentation\\npage:PreviousRecursively split JSONNextSemantic ChunkingSplitting text from languages without\\nword\\nboundariesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ©\\n\\npossible, as those would generically seem to be the strongest semantically related pieces of\\ntext.How the text is split: by list of characters.How the chunk size is measured: by number of\\ncharacters.%pip install -qU langchain-text-splitters# This is a long document we can split up.with\\nopen(\"../../state_of_the_union.txt\") as f:    state_of_the_union = f.read()from langchain_text_splitters\\nimport RecursiveCharacterTextSplitterAPI Reference:RecursiveCharacterTextSplittertext_splitter =\\nRecursiveCharacterTextSplitter(    # Set a really small chunk size, just to show.    chunk_size=100,   \\nchunk_overlap=20,    length_function=len,    is_separator_regex=False,)texts =\\ntext_splitter.create_documents([state_of_the_union])print(texts[0])print(texts[1])page_content=\\'Mada\\nm Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress\\nand\\'page_content=\\'of Congress and the Cabinet. Justices of the Supreme Court. My fellow\\nAmericans.\\'text_splitter.split_text(state_of_the_union)[:2][\\'Madam Speaker, Madam Vice President,\\nour First Lady and Second Gentleman. Members of Congress and\\', \\'of Congress and the Cabinet.\\nJustices of the Supreme Court. My fellow Americans.\\']Splitting text from languages without word\\nboundaries?Some writing systems do not have word boundaries, for example Chinese, Japanese,\\nand Thai. Splitting text with the default separator list of [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"] can cause words to be\\nsplit between chunks. To keep words together, you can override the list of separators to include\\nadditional punctuation:Add ASCII full-stop \".\", Unicode fullwidth full stop \"?\" (used in Chinese text),\\nand ideographic full stop \"?\" (used in Japanese and Chinese)Add Zero-width space used in Thai,\\nMyanmar, Kmer, and Japanese.Add ASCII comma \",\", Unicode fullwidth comma \"?\", and Unicode\\nideographic comma \"?\"text_splitter = RecursiveCharacterTextSplitter(    separators=[        \"\\\\n\\\\n\",       \\n\"\\\\n\",        \" \",        \".\",        \",\",        \"\\\\u200b\",  # Zero-width space        \"\\\\uff0c\",  # Fullwidth comma       \\n\"\\\\u3001\",  # Ideographic comma        \"\\\\uff0e\",  # Fullwidth full stop        \"\\\\u3002\",  # Ideographic full\\nstop        \"\",    ],    # Existing args)Help us out by providing feedback on this documentation\\npage:PreviousRecursively split JSONNextSemantic ChunkingSplitting text from languages without\\nword\\nboundariesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ©\\n\\npossible, as those would generically seem to be the strongest semantically related pieces of\\ntext.How the text is split: by list of characters.How the chunk size is measured: by number of\\ncharacters.%pip install -qU langchain-text-splitters# This is a long document we can split up.with\\nopen(\"../../state_of_the_union.txt\") as f:    state_of_the_union = f.read()from langchain_text_splitters\\nimport RecursiveCharacterTextSplitterAPI Reference:RecursiveCharacterTextSplittertext_splitter =\\nRecursiveCharacterTextSplitter(    # Set a really small chunk size, just to show.    chunk_size=100,   \\nchunk_overlap=20,    length_function=len,    is_separator_regex=False,)texts =\\ntext_splitter.create_documents([state_of_the_union])print(texts[0])print(texts[1])page_content=\\'Mada\\nm Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress\\nand\\'page_content=\\'of Congress and the Cabinet. Justices of the Supreme Court. My fellow\\nAmericans.\\'text_splitter.split_text(state_of_the_union)[:2][\\'Madam Speaker, Madam Vice President,\\nour First Lady and Second Gentleman. Members of Congress and\\', \\'of Congress and the Cabinet.\\nJustices of the Supreme Court. My fellow Americans.\\']Splitting text from languages without word\\nboundaries?Some writing systems do not have word boundaries, for example Chinese, Japanese,\\nand Thai. Splitting text with the default separator list of [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"] can cause words to be\\nsplit between chunks. To keep words together, you can override the list of separators to include\\nadditional punctuation:Add ASCII full-stop \".\", Unicode fullwidth full stop \"?\" (used in Chinese text),\\nand ideographic full stop \"?\" (used in Japanese and Chinese)Add Zero-width space used in Thai,\\nMyanmar, Kmer, and Japanese.Add ASCII comma \",\", Unicode fullwidth comma \"?\", and Unicode\\nideographic comma \"?\"text_splitter = RecursiveCharacterTextSplitter(    separators=[        \"\\\\n\\\\n\",       \\n\"\\\\n\",        \" \",        \".\",        \",\",        \"\\\\u200b\",  # Zero-width space        \"\\\\uff0c\",  # Fullwidth comma       \\n\"\\\\u3001\",  # Ideographic comma        \"\\\\uff0e\",  # Fullwidth full stop        \"\\\\u3002\",  # Ideographic full\\nstop        \"\",    ],    # Existing args)Help us out by providing feedback on this documentation\\npage:PreviousRecursively split JSONNextSemantic ChunkingSplitting text from languages without\\nword\\nboundariesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ©\\nHuman: How to use recursive text splitter for languages without word boundaries?\\nAssistant: To use the RecursiveCharacterTextSplitter for languages without word boundaries, you\\nneed to override the list of separators to include additional punctuation that is commonly used\\nin those languages. Here\\'s an example for Chinese, Japanese, and Thai:\\n\\n```python\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    separators=[\\n        \"\\\\n\\\\n\",\\n        \"\\\\n\",\\n        \" \",\\n        \".\",\\n        \",\",\\n        \"\\\\u3000\",  # Full-width space used in Japanese\\n        \"\\\\u3001\",  # Ideographic comma used in Japanese and Chinese\\n        \"\\\\u3002\",  # Ideographic full stop used in Japanese and Chinese\\n        \"\\\\uff0c\",  # Fullwidth comma\\n        \"\\\\uff0e\",  # Fullwidth full stop\\n        \"\\\\u200b\",  # Zero-width space used in Thai, Myanmar, Kmer, and Japanese\\n    ],\\n    chunk_size=100,\\n    chunk_overlap=20,\\n    length_function=len,\\n    is_separator_regex=False,\\n)\\n\\n# Your text here\\nstate_of_the_union = \"Your long text here\"\\n\\ntexts = text_splitter.create_documents([state_of_the_union])\\n\\n# Print the first two chunks\\nprint(texts[0])\\nprint(texts[1])\\n```\\n\\nThis example includes separators for full-width space, ideographic comma, ideographic full stop,\\nfullwidth comma, fullwidth full stop, and zero-width space, which are commonly used in Chinese,\\nJapanese, and Thai. Adjust the separators list according to the language you\\'re working with.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"How to use recursive text splitter\")\n",
    "\n",
    "document_chain.invoke(\n",
    "    {\n",
    "        \n",
    "        \"messages\": demo_ephemeral_chat_history.messages,\n",
    "        \"context\": qdrant1.similarity_search(\"how to use recursive text splitter\"),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"Why do we use recursive text splitter\", \"output\": \"The RecursiveTextSplitter is a type of text splitter in LangChain that recursively splits text into smaller chunks while trying to keep related pieces of text next to each other. This is the recommended way to start splitting text because it is a simple and effective way to split text while maintaining the context. It takes a list of user-defined characters as input, which can be used to determine where to split the text. For example, if you are dealing with a piece of text written in English, you might use a list of punctuation marks as the input to the RecursiveTextSplitter. This will ensure that sentences are split at appropriate places, and related sentences will be kept together in the same chunk.\"},\n",
    "    {\"input\": \"Explain how to use recursive text splitter\", \"output\": \"\"\"%pip install -qU langchain-text-splitters\n",
    "\n",
    "# Example document\n",
    "with open(\"path/to/document.txt\") as f:\n",
    "    document = f.read()\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([document])\n",
    "print(texts[0])\n",
    "print(texts[1])\n",
    "\"\"\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = \"\"\"System: You are an expert in LangChain, a framework for building applications with large language models. Your task is to answer questions with the utmost accuracy, providing clear and detailed explanations. When relevant, include code sequences to illustrate your points and guide users effectively. Your responses should be informative, concise, and geared towards helping users understand and apply LangChain concepts and techniques.\\n\\nhas a number of built-in document transformers that make it easy to split, combine, filter, and\\notherwise manipulate documents.When you want to deal with long pieces of text, it is necessary to\\nsplit up that text into chunks.\\nAs simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the\\nsemantically related pieces of text together. What \"semantically related\" means could depend on the\\ntype of text.\\nThis notebook showcases several ways to do that.At a high level, text splitters work as\\nfollowing:Split the text up into small, semantically meaningful chunks (often sentences).Start\\ncombining these small chunks into a larger chunk until you reach a certain size (as measured by\\nsome function).Once you reach that size, make that chunk its own piece of text and then start\\ncreating a new chunk of text with some overlap (to keep context between chunks).That means there\\nare two different axes along which you can customize your text splitter:How the text is splitHow the\\nchunk size is measuredTypes of Text Splitters?LangChain offers many different types of text\\nsplitters.\\nThese all live in the langchain-text-splitters package. Table columns:Name: Name of the text\\nsplitterClasses: Classes that implement this text splitterSplits On: How this text splitter splits\\ntextAdds Metadata: Whether or not this text splitter adds metadata about where each chunk came\\nfrom.Description: Description of the splitter, including recommendation on when to use\\nit.NameClassesSplits OnAdds MetadataDescriptionRecursiveRecursiveCharacterTextSplitter,\\nRecursiveJsonSplitterA list of user defined charactersRecursively splits text. This splitting is trying to\\nkeep related pieces of text next to each other. This is the recommended way to start splitting\\ntext.HTMLHTMLHeaderTextSplitter, HTMLSectionSplitterHTML specific characters?Splits text\\nbased on HTML-specific characters. Notably, this adds in relevant information about where that\\nchunk came from (based on the HTML)MarkdownMarkdownHeaderTextSplitterMarkdown specific\\ncharacters?Splits text based on Markdown-specific characters. Notably, this adds in relevant\\ninformation about where that chunk came from (based on the Markdown)Codemany languagesCode\\n\\nhas a number of built-in document transformers that make it easy to split, combine, filter, and\\notherwise manipulate documents.When you want to deal with long pieces of text, it is necessary to\\nsplit up that text into chunks.\\nAs simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the\\nsemantically related pieces of text together. What \"semantically related\" means could depend on the\\ntype of text.\\nThis notebook showcases several ways to do that.At a high level, text splitters work as\\nfollowing:Split the text up into small, semantically meaningful chunks (often sentences).Start\\ncombining these small chunks into a larger chunk until you reach a certain size (as measured by\\nsome function).Once you reach that size, make that chunk its own piece of text and then start\\ncreating a new chunk of text with some overlap (to keep context between chunks).That means there\\nare two different axes along which you can customize your text splitter:How the text is splitHow the\\nchunk size is measuredTypes of Text Splitters?LangChain offers many different types of text\\nsplitters.\\nThese all live in the langchain-text-splitters package. Table columns:Name: Name of the text\\nsplitterClasses: Classes that implement this text splitterSplits On: How this text splitter splits\\ntextAdds Metadata: Whether or not this text splitter adds metadata about where each chunk came\\nfrom.Description: Description of the splitter, including recommendation on when to use\\nit.NameClassesSplits OnAdds MetadataDescriptionRecursiveRecursiveCharacterTextSplitter,\\nRecursiveJsonSplitterA list of user defined charactersRecursively splits text. This splitting is trying to\\nkeep related pieces of text next to each other. This is the recommended way to start splitting\\ntext.HTMLHTMLHeaderTextSplitter, HTMLSectionSplitterHTML specific characters?Splits text\\nbased on HTML-specific characters. Notably, this adds in relevant information about where that\\nchunk came from (based on the HTML)MarkdownMarkdownHeaderTextSplitterMarkdown specific\\ncharacters?Splits text based on Markdown-specific characters. Notably, this adds in relevant\\ninformation about where that chunk came from (based on the Markdown)Codemany languagesCode\\n\\nhas a number of built-in document transformers that make it easy to split, combine, filter, and\\notherwise manipulate documents.When you want to deal with long pieces of text, it is necessary to\\nsplit up that text into chunks.\\nAs simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the\\nsemantically related pieces of text together. What \"semantically related\" means could depend on the\\ntype of text.\\nThis notebook showcases several ways to do that.At a high level, text splitters work as\\nfollowing:Split the text up into small, semantically meaningful chunks (often sentences).Start\\ncombining these small chunks into a larger chunk until you reach a certain size (as measured by\\nsome function).Once you reach that size, make that chunk its own piece of text and then start\\ncreating a new chunk of text with some overlap (to keep context between chunks).That means there\\nare two different axes along which you can customize your text splitter:How the text is splitHow the\\nchunk size is measuredTypes of Text Splitters?LangChain offers many different types of text\\nsplitters.\\nThese all live in the langchain-text-splitters package. Table columns:Name: Name of the text\\nsplitterClasses: Classes that implement this text splitterSplits On: How this text splitter splits\\ntextAdds Metadata: Whether or not this text splitter adds metadata about where each chunk came\\nfrom.Description: Description of the splitter, including recommendation on when to use\\nit.NameClassesSplits OnAdds MetadataDescriptionRecursiveRecursiveCharacterTextSplitter,\\nRecursiveJsonSplitterA list of user defined charactersRecursively splits text. This splitting is trying to\\nkeep related pieces of text next to each other. This is the recommended way to start splitting\\ntext.HTMLHTMLHeaderTextSplitter, HTMLSectionSplitterHTML specific characters?Splits text\\nbased on HTML-specific characters. Notably, this adds in relevant information about where that\\nchunk came from (based on the HTML)MarkdownMarkdownHeaderTextSplitterMarkdown specific\\ncharacters?Splits text based on Markdown-specific characters. Notably, this adds in relevant\\ninformation about where that chunk came from (based on the Markdown)Codemany languagesCode\\n\\nhas a number of built-in document transformers that make it easy to split, combine, filter, and\\notherwise manipulate documents.When you want to deal with long pieces of text, it is necessary to\\nsplit up that text into chunks.\\nAs simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the\\nsemantically related pieces of text together. What \"semantically related\" means could depend on the\\ntype of text.\\nThis notebook showcases several ways to do that.At a high level, text splitters work as\\nfollowing:Split the text up into small, semantically meaningful chunks (often sentences).Start\\ncombining these small chunks into a larger chunk until you reach a certain size (as measured by\\nsome function).Once you reach that size, make that chunk its own piece of text and then start\\ncreating a new chunk of text with some overlap (to keep context between chunks).That means there\\nare two different axes along which you can customize your text splitter:How the text is splitHow the\\nchunk size is measuredTypes of Text Splitters?LangChain offers many different types of text\\nsplitters.\\nThese all live in the langchain-text-splitters package. Table columns:Name: Name of the text\\nsplitterClasses: Classes that implement this text splitterSplits On: How this text splitter splits\\ntextAdds Metadata: Whether or not this text splitter adds metadata about where each chunk came\\nfrom.Description: Description of the splitter, including recommendation on when to use\\nit.NameClassesSplits OnAdds MetadataDescriptionRecursiveRecursiveCharacterTextSplitter,\\nRecursiveJsonSplitterA list of user defined charactersRecursively splits text. This splitting is trying to\\nkeep related pieces of text next to each other. This is the recommended way to start splitting\\ntext.HTMLHTMLHeaderTextSplitter, HTMLSectionSplitterHTML specific characters?Splits text\\nbased on HTML-specific characters. Notably, this adds in relevant information about where that\\nchunk came from (based on the HTML)MarkdownMarkdownHeaderTextSplitterMarkdown specific\\ncharacters?Splits text based on Markdown-specific characters. Notably, this adds in relevant\\ninformation about where that chunk came from (based on the Markdown)Codemany languagesCode\\nHuman: Why do we use recursive text splitter?\\nAssistant: The RecursiveTextSplitter is a good starting point for splitting text because it recursively\\nsplits text using a user-defined list of characters (such as punctuation marks, spaces, and paragraph\\nbreaks). This splitting is trying to keep related pieces of text next to each other, making it a good\\nchoice for most use cases. If you have specific requirements for how your text should be split, you\\ncan consider using other text splitters in LangChain, such as those that are specific to HTML or\\nMarkdown.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an expert in LangChain, a framework for building applications with large language models. Your task is to answer questions with the utmost accuracy, providing clear and detailed explanations. When relevant, include code sequences to illustrate your points and guide users effectively. Your responses should be informative, concise, and geared towards helping users understand and apply LangChain concepts and techniques.\n",
      "\n",
      "has a number of built-in document transformers that make it easy to split, combine, filter, and\n",
      "otherwise manipulate documents.When you want to deal with long pieces of text, it is necessary to\n",
      "split up that text into chunks.\n",
      "As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the\n",
      "semantically related pieces of text together. What \"semantically related\" means could depend on the\n",
      "type of text.\n",
      "This notebook showcases several ways to do that.At a high level, text splitters work as\n",
      "following:Split the text up into small, semantically meaningful chunks (often sentences).Start\n",
      "combining these small chunks into a larger chunk until you reach a certain size (as measured by\n",
      "some function).Once you reach that size, make that chunk its own piece of text and then start\n",
      "creating a new chunk of text with some overlap (to keep context between chunks).That means there\n",
      "are two different axes along which you can customize your text splitter:How the text is splitHow the\n",
      "chunk size is measuredTypes of Text Splitters?LangChain offers many different types of text\n",
      "splitters.\n",
      "These all live in the langchain-text-splitters package. Table columns:Name: Name of the text\n",
      "splitterClasses: Classes that implement this text splitterSplits On: How this text splitter splits\n",
      "textAdds Metadata: Whether or not this text splitter adds metadata about where each chunk came\n",
      "from.Description: Description of the splitter, including recommendation on when to use\n",
      "it.NameClassesSplits OnAdds MetadataDescriptionRecursiveRecursiveCharacterTextSplitter,\n",
      "RecursiveJsonSplitterA list of user defined charactersRecursively splits text. This splitting is trying to\n",
      "keep related pieces of text next to each other. This is the recommended way to start splitting\n",
      "text.HTMLHTMLHeaderTextSplitter, HTMLSectionSplitterHTML specific characters?Splits text\n",
      "based on HTML-specific characters. Notably, this adds in relevant information about where that\n",
      "chunk came from (based on the HTML)MarkdownMarkdownHeaderTextSplitterMarkdown specific\n",
      "characters?Splits text based on Markdown-specific characters. Notably, this adds in relevant\n",
      "information about where that chunk came from (based on the Markdown)Codemany languagesCode\n",
      "\n",
      "has a number of built-in document transformers that make it easy to split, combine, filter, and\n",
      "otherwise manipulate documents.When you want to deal with long pieces of text, it is necessary to\n",
      "split up that text into chunks.\n",
      "As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the\n",
      "semantically related pieces of text together. What \"semantically related\" means could depend on the\n",
      "type of text.\n",
      "This notebook showcases several ways to do that.At a high level, text splitters work as\n",
      "following:Split the text up into small, semantically meaningful chunks (often sentences).Start\n",
      "combining these small chunks into a larger chunk until you reach a certain size (as measured by\n",
      "some function).Once you reach that size, make that chunk its own piece of text and then start\n",
      "creating a new chunk of text with some overlap (to keep context between chunks).That means there\n",
      "are two different axes along which you can customize your text splitter:How the text is splitHow the\n",
      "chunk size is measuredTypes of Text Splitters?LangChain offers many different types of text\n",
      "splitters.\n",
      "These all live in the langchain-text-splitters package. Table columns:Name: Name of the text\n",
      "splitterClasses: Classes that implement this text splitterSplits On: How this text splitter splits\n",
      "textAdds Metadata: Whether or not this text splitter adds metadata about where each chunk came\n",
      "from.Description: Description of the splitter, including recommendation on when to use\n",
      "it.NameClassesSplits OnAdds MetadataDescriptionRecursiveRecursiveCharacterTextSplitter,\n",
      "RecursiveJsonSplitterA list of user defined charactersRecursively splits text. This splitting is trying to\n",
      "keep related pieces of text next to each other. This is the recommended way to start splitting\n",
      "text.HTMLHTMLHeaderTextSplitter, HTMLSectionSplitterHTML specific characters?Splits text\n",
      "based on HTML-specific characters. Notably, this adds in relevant information about where that\n",
      "chunk came from (based on the HTML)MarkdownMarkdownHeaderTextSplitterMarkdown specific\n",
      "characters?Splits text based on Markdown-specific characters. Notably, this adds in relevant\n",
      "information about where that chunk came from (based on the Markdown)Codemany languagesCode\n",
      "\n",
      "has a number of built-in document transformers that make it easy to split, combine, filter, and\n",
      "otherwise manipulate documents.When you want to deal with long pieces of text, it is necessary to\n",
      "split up that text into chunks.\n",
      "As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the\n",
      "semantically related pieces of text together. What \"semantically related\" means could depend on the\n",
      "type of text.\n",
      "This notebook showcases several ways to do that.At a high level, text splitters work as\n",
      "following:Split the text up into small, semantically meaningful chunks (often sentences).Start\n",
      "combining these small chunks into a larger chunk until you reach a certain size (as measured by\n",
      "some function).Once you reach that size, make that chunk its own piece of text and then start\n",
      "creating a new chunk of text with some overlap (to keep context between chunks).That means there\n",
      "are two different axes along which you can customize your text splitter:How the text is splitHow the\n",
      "chunk size is measuredTypes of Text Splitters?LangChain offers many different types of text\n",
      "splitters.\n",
      "These all live in the langchain-text-splitters package. Table columns:Name: Name of the text\n",
      "splitterClasses: Classes that implement this text splitterSplits On: How this text splitter splits\n",
      "textAdds Metadata: Whether or not this text splitter adds metadata about where each chunk came\n",
      "from.Description: Description of the splitter, including recommendation on when to use\n",
      "it.NameClassesSplits OnAdds MetadataDescriptionRecursiveRecursiveCharacterTextSplitter,\n",
      "RecursiveJsonSplitterA list of user defined charactersRecursively splits text. This splitting is trying to\n",
      "keep related pieces of text next to each other. This is the recommended way to start splitting\n",
      "text.HTMLHTMLHeaderTextSplitter, HTMLSectionSplitterHTML specific characters?Splits text\n",
      "based on HTML-specific characters. Notably, this adds in relevant information about where that\n",
      "chunk came from (based on the HTML)MarkdownMarkdownHeaderTextSplitterMarkdown specific\n",
      "characters?Splits text based on Markdown-specific characters. Notably, this adds in relevant\n",
      "information about where that chunk came from (based on the Markdown)Codemany languagesCode\n",
      "\n",
      "has a number of built-in document transformers that make it easy to split, combine, filter, and\n",
      "otherwise manipulate documents.When you want to deal with long pieces of text, it is necessary to\n",
      "split up that text into chunks.\n",
      "As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the\n",
      "semantically related pieces of text together. What \"semantically related\" means could depend on the\n",
      "type of text.\n",
      "This notebook showcases several ways to do that.At a high level, text splitters work as\n",
      "following:Split the text up into small, semantically meaningful chunks (often sentences).Start\n",
      "combining these small chunks into a larger chunk until you reach a certain size (as measured by\n",
      "some function).Once you reach that size, make that chunk its own piece of text and then start\n",
      "creating a new chunk of text with some overlap (to keep context between chunks).That means there\n",
      "are two different axes along which you can customize your text splitter:How the text is splitHow the\n",
      "chunk size is measuredTypes of Text Splitters?LangChain offers many different types of text\n",
      "splitters.\n",
      "These all live in the langchain-text-splitters package. Table columns:Name: Name of the text\n",
      "splitterClasses: Classes that implement this text splitterSplits On: How this text splitter splits\n",
      "textAdds Metadata: Whether or not this text splitter adds metadata about where each chunk came\n",
      "from.Description: Description of the splitter, including recommendation on when to use\n",
      "it.NameClassesSplits OnAdds MetadataDescriptionRecursiveRecursiveCharacterTextSplitter,\n",
      "RecursiveJsonSplitterA list of user defined charactersRecursively splits text. This splitting is trying to\n",
      "keep related pieces of text next to each other. This is the recommended way to start splitting\n",
      "text.HTMLHTMLHeaderTextSplitter, HTMLSectionSplitterHTML specific characters?Splits text\n",
      "based on HTML-specific characters. Notably, this adds in relevant information about where that\n",
      "chunk came from (based on the HTML)MarkdownMarkdownHeaderTextSplitterMarkdown specific\n",
      "characters?Splits text based on Markdown-specific characters. Notably, this adds in relevant\n",
      "information about where that chunk came from (based on the Markdown)Codemany languagesCode\n",
      "Human: Why do we use recursive text splitter?\n",
      "Assistant: The RecursiveTextSplitter is a good starting point for splitting text because it recursively\n",
      "splits text using a user-defined list of characters (such as punctuation marks, spaces, and paragraph\n",
      "breaks). This splitting is trying to keep related pieces of text next to each other, making it a good\n",
      "choice for most use cases. If you have specific requirements for how your text should be split, you\n",
      "can consider using other text splitters in LangChain, such as those that are specific to HTML or\n",
      "Markdown.\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= '''System: You are an expert in LangChain, a framework for building applications with large language models. Your task is to answer questions with the utmost accuracy, providing clear and detailed explanations. When relevant, include code sequences to illustrate your points and guide users effectively. Your responses should be informative, concise, and geared towards helping users understand and apply LangChain concepts and techniques.\\n\\n   <div>                <h2>Baz</h2>                <p>Some text about Baz</p>            </div>            <br>    \\n       <p>Some concluding text about Foo</p>        </div>    </body>    </html>\"\"\"headers_to_split_on\\n= [    (\"h1\", \"Header 1\"),    (\"h2\", \"Header 2\"),    (\"h3\", \"Header 3\"),    (\"h4\", \"Header 4\"),]html_splitter\\n= HTMLSectionSplitter(headers_to_split_on=headers_to_split_on)html_header_splits =\\nhtml_splitter.split_text(html_string)chunk_size = 500chunk_overlap = 30text_splitter =\\nRecursiveCharacterTextSplitter(    chunk_size=chunk_size, chunk_overlap=chunk_overlap)#\\nSplitsplits = text_splitter.split_documents(html_header_splits)splitsAPI\\nReference:RecursiveCharacterTextSplitterHelp us out by providing feedback on this documentation\\npage:PreviousSplit by HTML headerNextSplit by characterDescription and motivationUsage\\nexamplesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ©\\n2024 LangChain, Inc.\\n\\n   <div>                <h2>Baz</h2>                <p>Some text about Baz</p>            </div>            <br>    \\n       <p>Some concluding text about Foo</p>        </div>    </body>    </html>\"\"\"headers_to_split_on\\n= [    (\"h1\", \"Header 1\"),    (\"h2\", \"Header 2\"),    (\"h3\", \"Header 3\"),    (\"h4\", \"Header 4\"),]html_splitter\\n= HTMLSectionSplitter(headers_to_split_on=headers_to_split_on)html_header_splits =\\nhtml_splitter.split_text(html_string)chunk_size = 500chunk_overlap = 30text_splitter =\\nRecursiveCharacterTextSplitter(    chunk_size=chunk_size, chunk_overlap=chunk_overlap)#\\nSplitsplits = text_splitter.split_documents(html_header_splits)splitsAPI\\nReference:RecursiveCharacterTextSplitterHelp us out by providing feedback on this documentation\\npage:PreviousSplit by HTML headerNextSplit by characterDescription and motivationUsage\\nexamplesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ©\\n2024 LangChain, Inc.\\n\\n   <div>                <h2>Baz</h2>                <p>Some text about Baz</p>            </div>            <br>    \\n       <p>Some concluding text about Foo</p>        </div>    </body>    </html>\"\"\"headers_to_split_on\\n= [    (\"h1\", \"Header 1\"),    (\"h2\", \"Header 2\"),    (\"h3\", \"Header 3\"),    (\"h4\", \"Header 4\"),]html_splitter\\n= HTMLSectionSplitter(headers_to_split_on=headers_to_split_on)html_header_splits =\\nhtml_splitter.split_text(html_string)chunk_size = 500chunk_overlap = 30text_splitter =\\nRecursiveCharacterTextSplitter(    chunk_size=chunk_size, chunk_overlap=chunk_overlap)#\\nSplitsplits = text_splitter.split_documents(html_header_splits)splitsAPI\\nReference:RecursiveCharacterTextSplitterHelp us out by providing feedback on this documentation\\npage:PreviousSplit by HTML headerNextSplit by characterDescription and motivationUsage\\nexamplesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ©\\n2024 LangChain, Inc.\\n\\n   <div>                <h2>Baz</h2>                <p>Some text about Baz</p>            </div>            <br>    \\n       <p>Some concluding text about Foo</p>        </div>    </body>    </html>\"\"\"headers_to_split_on\\n= [    (\"h1\", \"Header 1\"),    (\"h2\", \"Header 2\"),    (\"h3\", \"Header 3\"),    (\"h4\", \"Header 4\"),]html_splitter\\n= HTMLSectionSplitter(headers_to_split_on=headers_to_split_on)html_header_splits =\\nhtml_splitter.split_text(html_string)chunk_size = 500chunk_overlap = 30text_splitter =\\nRecursiveCharacterTextSplitter(    chunk_size=chunk_size, chunk_overlap=chunk_overlap)#\\nSplitsplits = text_splitter.split_documents(html_header_splits)splitsAPI\\nReference:RecursiveCharacterTextSplitterHelp us out by providing feedback on this documentation\\npage:PreviousSplit by HTML headerNextSplit by characterDescription and motivationUsage\\nexamplesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ©\\n2024 LangChain, Inc.\\nHuman: How to use recursive text splitter in LangChain to split a document with HTML structure?\\n\\nAssistant: To use the RecursiveCharacterTextSplitter in LangChain to split a document with an HTML structure, you\\'ll first need to split the HTML text by HTML sections. Here\\'s a step-by-step guide:\\n\\n1. Define the headers that you want to use as section markers. In this example, we\\'ll use h1, h2, h3, and h4.\\n\\n```python\\nheaders_to_split_on = [    (\"h1\", \"Header 1\"),    (\"h2\", \"Header 2\"),    (\"h3\", \"Header 3\"),    (\"h4\", \"Header 4\"),]\\n```\\n\\n2. Create an HTMLSectionSplitter object and use it to split the HTML text into sections.\\n\\n```python\\nhtml_splitter = HTMLSectionSplitter(headers_to_split_on=headers_to_split_on)\\nhtml_header_splits = html_splitter.split_text(html_string)\\n```\\n\\n3. Set the chunk size and overlap for the RecursiveCharacterTextSplitter. The chunk size determines the maximum length of each text chunk, and the overlap determines the number of characters that should be included from the previous and next chunks for better coherence.\\n\\n```python\\nchunk_size = 500\\nchunk_overlap = 30\\n```\\n\\n4. Create a RecursiveCharacterTextSplitter object and use it to split the HTML sections into smaller text chunks.\\n\\n```python\\ntext_splitter = RecursiveCharacterTextSplitter(    chunk_size=chunk_size, chunk_overlap=chunk_overlap)\\n```\\n\\n5. Finally, use the text splitter to split the HTML sections into smaller text chunks.\\n\\n```python\\nsplits = text_splitter.split_documents(html_header_splits)\\n```\\n\\nThe `splits` variable now contains a list of text chunks that you can use for further processing with LangChain\\'s large language models'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an expert in LangChain, a framework for building applications with large language models. Your task is to answer questions with the utmost accuracy, providing clear and detailed explanations. When relevant, include code sequences to illustrate your points and guide users effectively. Your responses should be informative, concise, and geared towards helping users understand and apply LangChain concepts and techniques.\n",
      "\n",
      "   <div>                <h2>Baz</h2>                <p>Some text about Baz</p>            </div>            <br>    \n",
      "       <p>Some concluding text about Foo</p>        </div>    </body>    </html>\"\"\"headers_to_split_on\n",
      "= [    (\"h1\", \"Header 1\"),    (\"h2\", \"Header 2\"),    (\"h3\", \"Header 3\"),    (\"h4\", \"Header 4\"),]html_splitter\n",
      "= HTMLSectionSplitter(headers_to_split_on=headers_to_split_on)html_header_splits =\n",
      "html_splitter.split_text(html_string)chunk_size = 500chunk_overlap = 30text_splitter =\n",
      "RecursiveCharacterTextSplitter(    chunk_size=chunk_size, chunk_overlap=chunk_overlap)#\n",
      "Splitsplits = text_splitter.split_documents(html_header_splits)splitsAPI\n",
      "Reference:RecursiveCharacterTextSplitterHelp us out by providing feedback on this documentation\n",
      "page:PreviousSplit by HTML headerNextSplit by characterDescription and motivationUsage\n",
      "examplesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ©\n",
      "2024 LangChain, Inc.\n",
      "\n",
      "   <div>                <h2>Baz</h2>                <p>Some text about Baz</p>            </div>            <br>    \n",
      "       <p>Some concluding text about Foo</p>        </div>    </body>    </html>\"\"\"headers_to_split_on\n",
      "= [    (\"h1\", \"Header 1\"),    (\"h2\", \"Header 2\"),    (\"h3\", \"Header 3\"),    (\"h4\", \"Header 4\"),]html_splitter\n",
      "= HTMLSectionSplitter(headers_to_split_on=headers_to_split_on)html_header_splits =\n",
      "html_splitter.split_text(html_string)chunk_size = 500chunk_overlap = 30text_splitter =\n",
      "RecursiveCharacterTextSplitter(    chunk_size=chunk_size, chunk_overlap=chunk_overlap)#\n",
      "Splitsplits = text_splitter.split_documents(html_header_splits)splitsAPI\n",
      "Reference:RecursiveCharacterTextSplitterHelp us out by providing feedback on this documentation\n",
      "page:PreviousSplit by HTML headerNextSplit by characterDescription and motivationUsage\n",
      "examplesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ©\n",
      "2024 LangChain, Inc.\n",
      "\n",
      "   <div>                <h2>Baz</h2>                <p>Some text about Baz</p>            </div>            <br>    \n",
      "       <p>Some concluding text about Foo</p>        </div>    </body>    </html>\"\"\"headers_to_split_on\n",
      "= [    (\"h1\", \"Header 1\"),    (\"h2\", \"Header 2\"),    (\"h3\", \"Header 3\"),    (\"h4\", \"Header 4\"),]html_splitter\n",
      "= HTMLSectionSplitter(headers_to_split_on=headers_to_split_on)html_header_splits =\n",
      "html_splitter.split_text(html_string)chunk_size = 500chunk_overlap = 30text_splitter =\n",
      "RecursiveCharacterTextSplitter(    chunk_size=chunk_size, chunk_overlap=chunk_overlap)#\n",
      "Splitsplits = text_splitter.split_documents(html_header_splits)splitsAPI\n",
      "Reference:RecursiveCharacterTextSplitterHelp us out by providing feedback on this documentation\n",
      "page:PreviousSplit by HTML headerNextSplit by characterDescription and motivationUsage\n",
      "examplesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ©\n",
      "2024 LangChain, Inc.\n",
      "\n",
      "   <div>                <h2>Baz</h2>                <p>Some text about Baz</p>            </div>            <br>    \n",
      "       <p>Some concluding text about Foo</p>        </div>    </body>    </html>\"\"\"headers_to_split_on\n",
      "= [    (\"h1\", \"Header 1\"),    (\"h2\", \"Header 2\"),    (\"h3\", \"Header 3\"),    (\"h4\", \"Header 4\"),]html_splitter\n",
      "= HTMLSectionSplitter(headers_to_split_on=headers_to_split_on)html_header_splits =\n",
      "html_splitter.split_text(html_string)chunk_size = 500chunk_overlap = 30text_splitter =\n",
      "RecursiveCharacterTextSplitter(    chunk_size=chunk_size, chunk_overlap=chunk_overlap)#\n",
      "Splitsplits = text_splitter.split_documents(html_header_splits)splitsAPI\n",
      "Reference:RecursiveCharacterTextSplitterHelp us out by providing feedback on this documentation\n",
      "page:PreviousSplit by HTML headerNextSplit by characterDescription and motivationUsage\n",
      "examplesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ©\n",
      "2024 LangChain, Inc.\n",
      "Human: How to use recursive text splitter in LangChain to split a document with HTML structure?\n",
      "\n",
      "Assistant: To use the RecursiveCharacterTextSplitter in LangChain to split a document with an HTML structure, you'll first need to split the HTML text by HTML sections. Here's a step-by-step guide:\n",
      "\n",
      "1. Define the headers that you want to use as section markers. In this example, we'll use h1, h2, h3, and h4.\n",
      "\n",
      "```python\n",
      "headers_to_split_on = [    (\"h1\", \"Header 1\"),    (\"h2\", \"Header 2\"),    (\"h3\", \"Header 3\"),    (\"h4\", \"Header 4\"),]\n",
      "```\n",
      "\n",
      "2. Create an HTMLSectionSplitter object and use it to split the HTML text into sections.\n",
      "\n",
      "```python\n",
      "html_splitter = HTMLSectionSplitter(headers_to_split_on=headers_to_split_on)\n",
      "html_header_splits = html_splitter.split_text(html_string)\n",
      "```\n",
      "\n",
      "3. Set the chunk size and overlap for the RecursiveCharacterTextSplitter. The chunk size determines the maximum length of each text chunk, and the overlap determines the number of characters that should be included from the previous and next chunks for better coherence.\n",
      "\n",
      "```python\n",
      "chunk_size = 500\n",
      "chunk_overlap = 30\n",
      "```\n",
      "\n",
      "4. Create a RecursiveCharacterTextSplitter object and use it to split the HTML sections into smaller text chunks.\n",
      "\n",
      "```python\n",
      "text_splitter = RecursiveCharacterTextSplitter(    chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
      "```\n",
      "\n",
      "5. Finally, use the text splitter to split the HTML sections into smaller text chunks.\n",
      "\n",
      "```python\n",
      "splits = text_splitter.split_documents(html_header_splits)\n",
      "```\n",
      "\n",
      "The `splits` variable now contains a list of text chunks that you can use for further processing with LangChain's large language models\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "You are an expert in LangChain, a framework for building applications with large language models. Your task is to answer questions with the utmost accuracy, providing clear and detailed explanations. When relevant, include code sequences to illustrate your points and guide users effectively. Your responses should be informative, concise, and geared towards helping users understand and apply LangChain concepts and techniques.\n",
      "USe the examples bellow as a standard to wic you have to answer:\n",
      "input: \"Why do we use recursive text splitter\", output: The RecursiveTextSplitter is a type of text splitter in LangChain that recursively splits text into smaller chunks while trying to keep related pieces of text next to each other. This is the recommended way to start splitting text because it is a simple and effective way to split text while maintaining the context. It takes a list of user-defined characters as input, which can be used to determine where to split the text. For example, if you are dealing with a piece of text written in English, you might use a list of punctuation marks as the input to the RecursiveTextSplitter. This will ensure that sentences are split at appropriate places, and related sentences will be kept together in the same chunk.\n",
      "    \n",
      "\n",
      "\n",
      "Answer this question using the provided context only\n",
      "from langchain_core.prompts import PromptTemplate\n",
      "\n",
      "prompt_template = PromptTemplate.from_template(\n",
      "    \"Tell me a {adjective} joke about {content}.\"\n",
      ")\n",
      "prompt_template.format(adjective=\"funny\", content=\"chickens\") What is happening here\n",
      "\n",
      "Context:\n",
      "[Document(page_content='existing templates across different language models.Typically, language models expect the prompt\\nto either be a string or else a list of chat messages.PromptTemplate?Use PromptTemplate to create\\na template for a string prompt.By default, PromptTemplate uses Python\\'s str.format\\nsyntax for templating.from langchain_core.prompts import PromptTemplateprompt_template =\\nPromptTemplate.from_template(    \"Tell me a {adjective} joke about\\n{content}.\")prompt_template.format(adjective=\"funny\", content=\"chickens\")API\\nReference:PromptTemplate\\'Tell me a funny joke about chickens.\\'The template supports any number\\nof variables, including no variables:from langchain_core.prompts import\\nPromptTemplateprompt_template = PromptTemplate.from_template(\"Tell me a\\njoke\")prompt_template.format()API Reference:PromptTemplate\\'Tell me a joke\\'You can create\\ncustom prompt templates that format the prompt in any way you want.\\nFor more information, see Prompt Template Composition.ChatPromptTemplate?The prompt to chat\\nmodels/ is a list of chat messages.Each chat message is associated with content, and an additional\\nparameter called role.\\nFor example, in the OpenAI Chat Completions API, a chat message can be associated with an AI\\nassistant, a human or a system role.Create a chat prompt template like this:from\\nlangchain_core.prompts import ChatPromptTemplatechat_template =\\nChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful AI bot. Your name is\\n{name}.\"),        (\"human\", \"Hello, how are you doing?\"),        (\"ai\", \"I\\'m doing well, thanks!\"),       \\n(\"human\", \"{user_input}\"),    ])messages = chat_template.format_messages(name=\"Bob\",\\nuser_input=\"What is your name?\")API Reference:ChatPromptTemplatePiping these formatted\\nmessages into LangChain\\'s ChatOpenAI chat model class is roughly equivalent to the following with\\nusing the OpenAI client directly:%pip install openaifrom openai import OpenAIclient =\\nOpenAI()response = client.chat.completions.create(    model=\"gpt-3.5-turbo\",    messages=[       \\n{\"role\": \"system\", \"content\": \"You are a helpful AI bot. Your name is Bob.\"},        {\"role\": \"user\",\\n\"content\": \"Hello, how are you doing?\"},        {\"role\": \"assistant\", \"content\": \"I\\'m doing well, thanks!\"},  ', metadata={'page': 1, 'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langchain\\\\Components\\\\pdf_files\\\\2.pdf', '_id': '5be56e47-2808-42cc-9034-1cd37cbdf667', '_collection_name': 'langchain'}), Document(page_content='existing templates across different language models.Typically, language models expect the prompt\\nto either be a string or else a list of chat messages.PromptTemplate?Use PromptTemplate to create\\na template for a string prompt.By default, PromptTemplate uses Python\\'s str.format\\nsyntax for templating.from langchain_core.prompts import PromptTemplateprompt_template =\\nPromptTemplate.from_template(    \"Tell me a {adjective} joke about\\n{content}.\")prompt_template.format(adjective=\"funny\", content=\"chickens\")API\\nReference:PromptTemplate\\'Tell me a funny joke about chickens.\\'The template supports any number\\nof variables, including no variables:from langchain_core.prompts import\\nPromptTemplateprompt_template = PromptTemplate.from_template(\"Tell me a\\njoke\")prompt_template.format()API Reference:PromptTemplate\\'Tell me a joke\\'You can create\\ncustom prompt templates that format the prompt in any way you want.\\nFor more information, see Prompt Template Composition.ChatPromptTemplate?The prompt to chat\\nmodels/ is a list of chat messages.Each chat message is associated with content, and an additional\\nparameter called role.\\nFor example, in the OpenAI Chat Completions API, a chat message can be associated with an AI\\nassistant, a human or a system role.Create a chat prompt template like this:from\\nlangchain_core.prompts import ChatPromptTemplatechat_template =\\nChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful AI bot. Your name is\\n{name}.\"),        (\"human\", \"Hello, how are you doing?\"),        (\"ai\", \"I\\'m doing well, thanks!\"),       \\n(\"human\", \"{user_input}\"),    ])messages = chat_template.format_messages(name=\"Bob\",\\nuser_input=\"What is your name?\")API Reference:ChatPromptTemplatePiping these formatted\\nmessages into LangChain\\'s ChatOpenAI chat model class is roughly equivalent to the following with\\nusing the OpenAI client directly:%pip install openaifrom openai import OpenAIclient =\\nOpenAI()response = client.chat.completions.create(    model=\"gpt-3.5-turbo\",    messages=[       \\n{\"role\": \"system\", \"content\": \"You are a helpful AI bot. Your name is Bob.\"},        {\"role\": \"user\",\\n\"content\": \"Hello, how are you doing?\"},        {\"role\": \"assistant\", \"content\": \"I\\'m doing well, thanks!\"},  ', metadata={'page': 1, 'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langchain\\\\Components\\\\pdf_files\\\\2.pdf', '_id': 'ea7ebd94-ffa1-4a4f-9e89-cf2b302e2f57', '_collection_name': 'langchain'}), Document(page_content='existing templates across different language models.Typically, language models expect the prompt\\nto either be a string or else a list of chat messages.PromptTemplate?Use PromptTemplate to create\\na template for a string prompt.By default, PromptTemplate uses Python\\'s str.format\\nsyntax for templating.from langchain_core.prompts import PromptTemplateprompt_template =\\nPromptTemplate.from_template(    \"Tell me a {adjective} joke about\\n{content}.\")prompt_template.format(adjective=\"funny\", content=\"chickens\")API\\nReference:PromptTemplate\\'Tell me a funny joke about chickens.\\'The template supports any number\\nof variables, including no variables:from langchain_core.prompts import\\nPromptTemplateprompt_template = PromptTemplate.from_template(\"Tell me a\\njoke\")prompt_template.format()API Reference:PromptTemplate\\'Tell me a joke\\'You can create\\ncustom prompt templates that format the prompt in any way you want.\\nFor more information, see Prompt Template Composition.ChatPromptTemplate?The prompt to chat\\nmodels/ is a list of chat messages.Each chat message is associated with content, and an additional\\nparameter called role.\\nFor example, in the OpenAI Chat Completions API, a chat message can be associated with an AI\\nassistant, a human or a system role.Create a chat prompt template like this:from\\nlangchain_core.prompts import ChatPromptTemplatechat_template =\\nChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful AI bot. Your name is\\n{name}.\"),        (\"human\", \"Hello, how are you doing?\"),        (\"ai\", \"I\\'m doing well, thanks!\"),       \\n(\"human\", \"{user_input}\"),    ])messages = chat_template.format_messages(name=\"Bob\",\\nuser_input=\"What is your name?\")API Reference:ChatPromptTemplatePiping these formatted\\nmessages into LangChain\\'s ChatOpenAI chat model class is roughly equivalent to the following with\\nusing the OpenAI client directly:%pip install openaifrom openai import OpenAIclient =\\nOpenAI()response = client.chat.completions.create(    model=\"gpt-3.5-turbo\",    messages=[       \\n{\"role\": \"system\", \"content\": \"You are a helpful AI bot. Your name is Bob.\"},        {\"role\": \"user\",\\n\"content\": \"Hello, how are you doing?\"},        {\"role\": \"assistant\", \"content\": \"I\\'m doing well, thanks!\"},  ', metadata={'page': 1, 'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langchain\\\\Components\\\\pdf_files\\\\2.pdf', '_id': '252dc4f0-3b3a-439b-839e-05152a4f56ee', '_collection_name': 'langchain'})]\n",
      "\n",
      "What is happening here is the creation of a custom prompt template using the PromptTemplate class from the langchain_core.prompts module. The template is being used to create a joke that is funny and about chickens. The template uses the {adjective} and {content} variables to insert the adjective \"funny\" and the content \"chickens\" into the joke. The template is then formatted using the format() method to produce the final joke, which is \"Tell me a funny joke about chickens.\" This joke can be piped into LangChain's ChatOpenAI chat model class to generate a response from the chat model.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'Context'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mContext\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'Context'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samar\\Desktop\\Langchat\\lang\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\samar\\Desktop\\Langchat\\lang\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\samar\\Desktop\\Langchat\\lang\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 0.3.0. An updated version of the class exists in the langchain-qdrant package and should be used instead. To use it run `pip install -U langchain-qdrant` and import as `from langchain_qdrant import Qdrant`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\samar\\Desktop\\Langchat\\lang\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n",
      "2024-06-21 12:13:30.573 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n",
      "2024-06-21 12:13:30.648 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\samar\\Desktop\\Langchat\\lang\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "import os\n",
    "\n",
    "# Set environment variable for HuggingFaceHub API\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize Qdrant client\n",
    "url = \"https://838581f9-41df-47dc-a452-47ea8f1e1edc.us-east4-0.gcp.cloud.qdrant.io:6333\"\n",
    "api_key = \"7QBIL3BHx33BOqbHlsSIrn5in7kTX4OLzn5FAXQYPFP7Y1KFYbagpg\"\n",
    "qdrant_client = QdrantClient(url=url, api_key=api_key)\n",
    "\n",
    "# Initialize Qdrant vector store\n",
    "qdrant1 = Qdrant(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"langchain\",\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 1024, \"max_new_tokens\": 1024}\n",
    ")\n",
    "\n",
    "# Define the prompt template\n",
    "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert in LangChain, a framework for building applications with large language models. Your task is to answer questions with the utmost accuracy, providing clear and detailed explanations. When relevant, include code sequences to illustrate your points and guide users effectively. Your responses should be informative, concise, and geared towards helping users understand and apply LangChain concepts and techniques.\\n\\n{context}\",\n",
    "        )\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the document chain\n",
    "document_chain = create_stuff_documents_chain(llm, question_answering_prompt)\n",
    "\n",
    "# Streamlit app setup\n",
    "st.set_page_config(page_title=\"LangChain Chatbot\", page_icon=\"🤖\", layout=\"centered\")\n",
    "\n",
    "# Initialize chat history\n",
    "if 'chat_history' not in st.session_state:\n",
    "    st.session_state['chat_history'] = []\n",
    "\n",
    "# Layout for the Streamlit app\n",
    "st.title('LangChain Chatbot')\n",
    "\n",
    "# Display chat history\n",
    "for chat in st.session_state['chat_history']:\n",
    "    if chat['role'] == 'user':\n",
    "        st.markdown(f\"**User:** {chat['content']}\")\n",
    "    else:\n",
    "        st.markdown(f\"**Assistant:** {chat['content']}\")\n",
    "\n",
    "# User input\n",
    "query = st.text_input('Enter your query:', '')\n",
    "\n",
    "if st.button('Send'):\n",
    "    if query:\n",
    "        # Add user message to chat history\n",
    "        st.session_state['chat_history'].append({'role': 'user', 'content': query})\n",
    "\n",
    "        # Initialize ephemeral chat history for the current interaction\n",
    "        demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "        demo_ephemeral_chat_history.add_user_message(query)\n",
    "\n",
    "        # Perform the similarity search\n",
    "        context = qdrant1.similarity_search(query)\n",
    "\n",
    "        # Invoke the document chain with messages and context\n",
    "        response = document_chain.invoke(\n",
    "            {\n",
    "                \"messages\": demo_ephemeral_chat_history.messages,\n",
    "                \"context\": context,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Extract and display only the Assistant's message\n",
    "        if 'Assistant:' in response:\n",
    "            assistant_message = response.split('Assistant:')[-1].strip()\n",
    "            st.session_state['chat_history'].append({'role': 'assistant', 'content': assistant_message})\n",
    "            st.experimental_rerun()\n",
    "        else:\n",
    "            st.write(\"No valid response received.\")\n",
    "    else:\n",
    "        st.write(\"Please enter a query.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qdrant1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnablePassthrough\n\u001b[1;32m----> 4\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[43mqdrant1\u001b[49m\u001b[38;5;241m.\u001b[39mas_retriever()\n\u001b[0;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m hub\u001b[38;5;241m.\u001b[39mpull(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrlm/rag-prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_docs\u001b[39m(docs):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'qdrant1' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "retriever = qdrant1.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
